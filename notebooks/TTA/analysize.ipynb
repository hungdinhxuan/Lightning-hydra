{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadec76",
   "metadata": {},
   "source": [
    "# Group by attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e55c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      utt     spoof     score     label  \\\n",
      "0  ASVspoof2021_DF_eval/DF_E_2814598.flac -2.331280  2.107358  bonafide   \n",
      "1  ASVspoof2021_DF_eval/DF_E_4998650.flac -2.310411  2.088900  bonafide   \n",
      "2  ASVspoof2021_DF_eval/DF_E_4192130.flac -2.393897  2.160999  bonafide   \n",
      "3  ASVspoof2021_DF_eval/DF_E_2592924.flac -2.411914  2.170061  bonafide   \n",
      "4  ASVspoof2021_DF_eval/DF_E_3631097.flac -2.352637  2.124310  bonafide   \n",
      "\n",
      "  attack_model attack_type  \n",
      "0         none        none  \n",
      "1         none        none  \n",
      "2         none        none  \n",
      "3         none        none  \n",
      "4         none        none  \n"
     ]
    }
   ],
   "source": [
    "# load result file\n",
    "import pandas as pd\n",
    "\n",
    "result_file = '/nvme1/hungdx/Lightning-hydra/logs/results/TTA_benchmark_test_det_show_lts/ToP_LA19/ADV_2025_cnsl_xlsr_vib_paper_ToP_LA19.txt'\n",
    "df = pd.read_csv(result_file, sep=' ', header=None)\n",
    "df.columns = ['utt', 'spoof', 'score']\n",
    "\n",
    "\n",
    "# meta datafile\n",
    "metadata_file = '/nvme1/hungdx/Lightning-hydra/data/TTA_benchmark_2025/ADV_2025/ADV/protocol.txt'\n",
    "metadata = pd.read_csv(metadata_file, sep=' ', header=None)\n",
    "metadata.columns = ['utt', 'label', 'attack_model', 'attack_type']\n",
    "\n",
    "\n",
    "# merge df and metadata\n",
    "df = pd.merge(df, metadata, on='utt', how='left')\n",
    "\n",
    "# print the first 5 rows\n",
    "print(df.head())\n",
    "# print the last 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e76a9b",
   "metadata": {},
   "source": [
    "# Performance group by attack_type & attack_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb6d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER by Attack Type:\n",
      "BIM: 2.43%\n",
      "FGSM: 14.85%\n",
      "PGD: 14.65%\n",
      "\n",
      "EER by Attack Model:\n",
      "aasistssl: 13.65%\n",
      "conformerssl: 10.06%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "def calculate_group_eer(df, group_column='attack_type'):\n",
    "    \"\"\"Calculate EER for each group in the dataset.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Overall EER calculation\n",
    "    bona_scores = df[df['label'] == 'bonafide']['score'].values\n",
    "    spoof_scores = df[df['label'] == 'spoof']['score'].values\n",
    "    \n",
    "    if len(bona_scores) > 0 and len(spoof_scores) > 0:\n",
    "        overall_eer, overall_threshold = em.compute_eer(bona_scores, spoof_scores)\n",
    "        results['overall'] = {\n",
    "            'eer': overall_eer * 100,\n",
    "            'threshold': overall_threshold,\n",
    "            'samples': len(df)\n",
    "        }\n",
    "    \n",
    "    # Group-specific EER calculation\n",
    "    group_results = {}\n",
    "    all_bona_scores = df[df['label'] == 'bonafide']['score'].values\n",
    "    \n",
    "    for group_name, group_df in df.groupby(group_column):\n",
    "        if group_name in ['none', '-']:  # Skip bonafide group\n",
    "            continue\n",
    "            \n",
    "        # Get spoof samples for this attack\n",
    "        group_spoof_scores = group_df[group_df['label'] == 'spoof']['score'].values\n",
    "        \n",
    "        if len(group_spoof_scores) > 0 and len(all_bona_scores) > 0:\n",
    "            # Calculate EER for this specific attack vs all bonafide\n",
    "            group_eer, group_threshold = em.compute_eer(all_bona_scores, group_spoof_scores)\n",
    "            \n",
    "            group_results[group_name] = {\n",
    "                'eer': group_eer * 100,\n",
    "                'threshold': group_threshold,\n",
    "                'samples': len(group_df),\n",
    "                'spoof_samples': len(group_spoof_scores)\n",
    "            }\n",
    "    \n",
    "    results['groups'] = group_results\n",
    "    return results\n",
    "\n",
    "# Use with your existing df\n",
    "results_by_type = calculate_group_eer(df, 'attack_type')\n",
    "results_by_model = calculate_group_eer(df, 'attack_model')\n",
    "\n",
    "# Print results\n",
    "print(\"EER by Attack Type:\")\n",
    "for group, metrics in results_by_type['groups'].items():\n",
    "    print(f\"{group}: {metrics['eer']:.2f}%\")\n",
    "\n",
    "print(\"\\nEER by Attack Model:\")\n",
    "for group, metrics in results_by_model['groups'].items():\n",
    "    print(f\"{group}: {metrics['eer']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338661db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
