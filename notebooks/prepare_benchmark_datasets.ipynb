{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950aef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c773e41",
   "metadata": {},
   "source": [
    "# ASVspoof2019_LA_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab6b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3d29e",
   "metadata": {},
   "source": [
    "# ASVspoof2021_LA_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e06cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_metadata = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/notebooks/asvspoof-challenge-2021/eval-package/keys/LA/CM/trial_metadata.txt\", sep=\" \", header=None)\n",
    "# only keep column 1th and 5th\n",
    "\n",
    "trial_metadata = trial_metadata[[1, 5]]\n",
    "\n",
    "# adding column subset with value is eval to trial_metadata\n",
    "\n",
    "trial_metadata.columns = [\"utt\", \"label\"]\n",
    "\n",
    "trial_metadata[\"subset\"] = \"eval\"\n",
    "\n",
    "# Add flac extension to utt\n",
    "trial_metadata[\"utt\"] = trial_metadata[\"utt\"].apply(lambda x: os.path.splitext(x)[0] + \".flac\")\n",
    "\n",
    "\n",
    "# change column order to [utt, subset, label]\n",
    "trial_metadata = trial_metadata[[\"utt\", \"subset\", \"label\"]]\n",
    "\n",
    "# export to protocol.txt\n",
    "\n",
    "trial_metadata.to_csv(\"asvspoof2021_la_eval_protocol.txt\", sep=\" \", header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbe0ff",
   "metadata": {},
   "source": [
    "# ASVspoof2021_DF_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e783f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_metadata = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/notebooks/asvspoof-challenge-2021/eval-package/keys/DF/CM/trial_metadata.txt\", sep=\" \", header=None)\n",
    "# only keep column 1th and 5th\n",
    "\n",
    "trial_metadata = trial_metadata[[1, 5]]\n",
    "\n",
    "# adding column subset with value is eval to trial_metadata\n",
    "\n",
    "trial_metadata.columns = [\"utt\", \"label\"]\n",
    "\n",
    "trial_metadata[\"subset\"] = \"eval\"\n",
    "\n",
    "trial_metadata[\"utt\"] = trial_metadata[\"utt\"].apply(lambda x: os.path.splitext(x)[0] + \".flac\")\n",
    "\n",
    "# change column order to [utt, subset, label]\n",
    "trial_metadata = trial_metadata[[\"utt\", \"subset\", \"label\"]]\n",
    "\n",
    "# export to protocol.txt\n",
    "\n",
    "trial_metadata.to_csv(\"asvspoof2021_df_eval_protocol.txt\", sep=\" \", header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65ed8c",
   "metadata": {},
   "source": [
    "# ASVSpoof2024-Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5904e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ce1bc",
   "metadata": {},
   "source": [
    "# ASVSpoof2024-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de1062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_metadata = pd.read_csv(\"/data/ASVspoof5/ASVspoof5.eval.track_1.tsv\", sep=\" \", header=None)\n",
    "\n",
    "# only keep column 1th and 8th\n",
    "\n",
    "trial_metadata = trial_metadata[[1, 8]]\n",
    "\n",
    "# adding column subset with value is eval to trial_metadata\n",
    "\n",
    "trial_metadata.columns = [\"utt\", \"label\"]\n",
    "\n",
    "trial_metadata[\"subset\"] = \"eval\"\n",
    "\n",
    "trial_metadata[\"utt\"] = trial_metadata[\"utt\"].apply(lambda x: os.path.splitext(x)[0] + \".flac\")\n",
    "\n",
    "# change column order to [utt, subset, label]\n",
    "trial_metadata = trial_metadata[[\"utt\", \"subset\", \"label\"]]\n",
    "\n",
    "# export to protocol.txt\n",
    "\n",
    "trial_metadata.to_csv(\"asvspoof2024_eval_protocol.txt\", sep=\" \", header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36de325",
   "metadata": {},
   "source": [
    "# Fake or real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "798110b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_metadata = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/fake_or_real_prot.txt\", sep=\" \", header=None)\n",
    "\n",
    "# update colums\n",
    "trial_metadata.columns = [\"utt\", \"subset\", \"label\"]\n",
    "trial_metadata['subset'] = 'eval'\n",
    "\n",
    "# export to protocol.txt\n",
    "trial_metadata.to_csv(\"fake_or_real_protocol.txt\", sep=\" \", header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556096cf",
   "metadata": {},
   "source": [
    "# Codec fale Yuankun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed46a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol file created at: /nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/protocol.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List of all codec dataset files\n",
    "codec_files = {\n",
    "    'A1': '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/label/A1.txt',\n",
    "    'A2': '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/label/A2.txt',\n",
    "    'A3': '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/label/A3.txt',\n",
    "    'C1': '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/label/C1.txt',\n",
    "    'C2': '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/label/C2.txt',\n",
    "    'C3': '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/label/C3.txt',\n",
    "    'C4': '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/label/C4.txt',\n",
    "    'C5': '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/label/C5.txt',\n",
    "    'C6': '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/label/C6.txt',\n",
    "    'C7': '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/label/C7.txt'\n",
    "}\n",
    "\n",
    "# Output protocol file path\n",
    "output_file = '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/Codecfake_Yuankun/protocol.txt'\n",
    "\n",
    "# Initialize an empty list to store protocol lines\n",
    "protocol_lines = []\n",
    "\n",
    "# Process each codec file\n",
    "for codec, file_path in codec_files.items():\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(file_path, sep=\" \", header=None)\n",
    "    \n",
    "    # Assuming columns: 0 (filename), 1 (label), 2 (subset identifier)\n",
    "    for _, row in df.iterrows():\n",
    "        filename = row[0]  # e.g., L1_p275_117.wav\n",
    "        label = 'bonafide' if row[1] == 'real' else 'spoof'  # Convert real->bonafide, fake->spoof\n",
    "        subset = 'eval'  # Fixed subset as per requirement\n",
    "        \n",
    "        # Create protocol line: <codec_folder>/<filename> <subset> <label>\n",
    "        protocol_line = f\"{codec}/{filename} {subset} {label}\"\n",
    "        protocol_lines.append(protocol_line)\n",
    "\n",
    "# Write to protocol file\n",
    "with open(output_file, 'w') as f:\n",
    "    for line in protocol_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"Protocol file created at: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224be335",
   "metadata": {},
   "source": [
    "# SONAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fe51538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol file created at: /nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/SONAR_dataset/protocol.txt\n"
     ]
    }
   ],
   "source": [
    "# List of folders\n",
    "folders = [\n",
    "    'AudioGen',\n",
    "    'FlashSpeech',\n",
    "    'NaturalSpeech3',\n",
    "    'OpenAI',\n",
    "    'PromptTTS2',\n",
    "    'SeedTTS',\n",
    "    'VALLE',\n",
    "    'VoiceBox',\n",
    "    'xTTS',\n",
    "    'real_samples'\n",
    "]\n",
    "# Base directory containing the folders (adjust this path as needed)\n",
    "base_dir = '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/SONAR_dataset'\n",
    "\n",
    "# Output protocol file path\n",
    "output_file = '/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/SONAR_dataset/protocol.txt'\n",
    "\n",
    "# Initialize an empty list to store protocol lines\n",
    "protocol_lines = []\n",
    "\n",
    "# Process each folder\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Warning: Folder {folder_path} does not exist. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Determine label based on folder\n",
    "    label = 'bonafide' if folder == 'real_samples' else 'spoof'\n",
    "    subset = 'eval'  # Fixed subset as per requirement\n",
    "    \n",
    "    # Iterate through files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Ensure it's a file (not a directory)\n",
    "        if os.path.isfile(os.path.join(folder_path, file_name)):\n",
    "            # Create protocol line: <folder_name>/<file_name> <subset> <label>\n",
    "            protocol_line = f\"{folder}/{file_name} {subset} {label}\"\n",
    "            protocol_lines.append(protocol_line)\n",
    "\n",
    "# Write to protocol file\n",
    "with open(output_file, 'w') as f:\n",
    "    for line in protocol_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"Protocol file created at: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1631cb",
   "metadata": {},
   "source": [
    "# DFADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcefbe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol file created at: DFADD_protocol.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "ROOT_FOLDER = \"/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/DFADD\"\n",
    "fake_dataset_list = [\n",
    "    \"DATASET_GradTTS\",\n",
    "    \"DATASET_MatchaTTS\",\n",
    "    \"DATASET_NaturalSpeech2\",\n",
    "    \"DATASET_PflowTTS\",\n",
    "    \"DATASET_StyleTTS2\",\n",
    "]\n",
    "bonafide_dataset = \"/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/DFADD/home/isjwdu/Work/Dataset/DFADD/DATASET_VCTK_BONAFIDE/test\"\n",
    "output_file = \"DFADD_protocol.txt\"\n",
    "\n",
    "# Initialize list to store protocol lines\n",
    "protocol_lines = []\n",
    "\n",
    "# Process fake datasets\n",
    "for dataset in fake_dataset_list:\n",
    "    # Construct path to test.txt\n",
    "    test_file_path = os.path.join(ROOT_FOLDER, dataset, \"test.txt\")\n",
    "    \n",
    "    # Check if test.txt exists\n",
    "    if not os.path.isfile(test_file_path):\n",
    "        print(f\"Warning: {test_file_path} does not exist. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Read test.txt (assuming space-separated, columns: speaker_id, utt_id, _, _, label)\n",
    "    df = pd.read_csv(test_file_path, sep=\" \", header=None)\n",
    "    \n",
    "    # Process each row\n",
    "    for _, row in df.iterrows():\n",
    "        utt_id = row[1]  # e.g., p227_093\n",
    "        # Skip bonafide entries\n",
    "        current_label = row[4]\n",
    "        if current_label == \"bonafide\":\n",
    "            continue\n",
    "        \n",
    "        label = 'spoof'  # All entries in fake datasets are spoof\n",
    "        subset = 'eval'  # Fixed subset as per requirement\n",
    "        \n",
    "        # Construct relative path: <dataset>/<utt_id>.wav\n",
    "        relative_path = f\"{dataset}/test/{utt_id}.flac\"\n",
    "        \n",
    "        # Create protocol line\n",
    "        protocol_line = f\"{relative_path} {subset} {label}\"\n",
    "        protocol_lines.append(protocol_line)\n",
    "\n",
    "# Process bonafide dataset\n",
    "if os.path.isdir(bonafide_dataset):\n",
    "    # Get relative path for bonafide dataset\n",
    "    bonafide_folder = os.path.relpath(bonafide_dataset, ROOT_FOLDER)\n",
    "    \n",
    "    # Iterate through .wav files in bonafide_dataset\n",
    "    for file_name in os.listdir(bonafide_dataset):\n",
    "        if file_name.endswith('.wav'):\n",
    "            label = 'bonafide'  # All files in bonafide_dataset are bonafide\n",
    "            subset = 'eval'  # Fixed subset\n",
    "            \n",
    "            # Construct relative path: <bonafide_folder>/<file_name>\n",
    "            relative_path = f\"{bonafide_folder}/{file_name}\"\n",
    "            \n",
    "            # Create protocol line\n",
    "            protocol_line = f\"{relative_path} {subset} {label}\"\n",
    "            protocol_lines.append(protocol_line)\n",
    "else:\n",
    "    print(f\"Warning: Bonafide dataset directory {bonafide_dataset} does not exist.\")\n",
    "\n",
    "# Write to protocol file\n",
    "with open(output_file, 'w') as f:\n",
    "    for line in protocol_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"Protocol file created at: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be337ec",
   "metadata": {},
   "source": [
    "# LibriSeVoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc82e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol file created at: LibriSeVoc_protocol.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "list_of_folders = os.listdir(\"/nvme1/hungdx/Datasets/LibriSeVoc\")\n",
    "list_of_folders\n",
    "\n",
    "\n",
    "base_dir = \"/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/LibriSeVoc\"  # Adjust this path as needed\n",
    "output_file = \"LibriSeVoc_protocol.txt\"\n",
    "\n",
    "# Initialize list to store protocol lines\n",
    "protocol_lines = []\n",
    "\n",
    "# Process each folder\n",
    "for folder in list_of_folders:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Warning: Folder {folder_path} does not exist. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Determine label based on folder\n",
    "    label = 'bonafide' if folder == 'gt' else 'spoof'\n",
    "    subset = 'eval'  # Fixed subset as per requirement\n",
    "    \n",
    "    # Iterate through .wav files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Construct relative path: <folder>/<file_name>\n",
    "            relative_path = f\"{folder}/{file_name}\"\n",
    "            \n",
    "            # Create protocol line: <relative_path> <subset> <label>\n",
    "            protocol_line = f\"{relative_path} {subset} {label}\"\n",
    "            protocol_lines.append(protocol_line)\n",
    "\n",
    "# Write to protocol file\n",
    "with open(output_file, 'w') as f:\n",
    "    for line in protocol_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"Protocol file created at: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854914f",
   "metadata": {},
   "source": [
    "# ADD_2022 track 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f87a519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df = pd.read_csv(\"/nvme2/hungdx/Datasets/track1_label.txt\", sep=\" \", header=None)\n",
    "df.columns = [\"utt\", \"label\"]\n",
    "\n",
    "# change label to bonafide and spoof\n",
    "df['label'] = df['label'].replace({'genuine': 'bonafide', 'fake': 'spoof'})\n",
    "\n",
    "df['subset'] = 'eval'\n",
    "\n",
    "# change order of columns\n",
    "df = df[['utt', 'subset', 'label']]\n",
    "\n",
    "# convert relative path to absolute path\n",
    "# df['utt'] = df['utt'].apply(lambda x: os.path.join(\"wav/\", x))\n",
    "# df\n",
    "# export to protocol.txt\n",
    "df.to_csv(\"ADD2022_track1_protocol.txt\", sep=\" \", header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5ef6c",
   "metadata": {},
   "source": [
    "# ADD_2022 track 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a02753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df = pd.read_csv(\"/nvme2/hungdx/Datasets/track3_R2_label.txt\", sep=\" \", header=None)\n",
    "df.columns = [\"utt\", \"label\", \"unknown\"]\n",
    "\n",
    "# change label to bonafide and spoof\n",
    "df['label'] = df['label'].replace({'genuine': 'bonafide', 'fake': 'spoof'})\n",
    "\n",
    "df['subset'] = 'eval'\n",
    "\n",
    "# change order of columns\n",
    "df = df[['utt', 'subset', 'label']]\n",
    "\n",
    "# convert relative path to absolute path\n",
    "# df['utt'] = df['utt'].apply(lambda x: os.path.join(\"wav/\", x))\n",
    "# df\n",
    "# export to protocol.txt\n",
    "df.to_csv(\"ADD2022_track3_protocol.txt\", sep=\" \", header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360f046",
   "metadata": {},
   "source": [
    "# ADD 2023 R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b2bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/ADD2023_R1/label.txt\", sep=\" \", header=None)\n",
    "df.columns = [\"utt\", \"label\"]\n",
    "\n",
    "# change label to bonafide and spoof\n",
    "df['label'] = df['label'].replace({'genuine': 'bonafide', 'fake': 'spoof'})\n",
    "\n",
    "df['subset'] = 'eval'\n",
    "\n",
    "# change order of columns\n",
    "df = df[['utt', 'subset', 'label']]\n",
    "\n",
    "# convert relative path to absolute path\n",
    "df['utt'] = df['utt'].apply(lambda x: os.path.join(\"wav/\", x))\n",
    "df\n",
    "# export to protocol.txt\n",
    "df.to_csv(\"ADD2023_R1_protocol.txt\", sep=\" \", header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbf72a",
   "metadata": {},
   "source": [
    "# ADD 2023 R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b6bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena/ADD2023_R2/label.txt\", sep=\" \", header=None)\n",
    "df.columns = [\"utt\", \"label\"]\n",
    "\n",
    "# change label to bonafide and spoof\n",
    "df['label'] = df['label'].replace({'genuine': 'bonafide', 'fake': 'spoof'})\n",
    "\n",
    "df['subset'] = 'eval'\n",
    "\n",
    "# change order of columns\n",
    "df = df[['utt', 'subset', 'label']]\n",
    "\n",
    "# convert relative path to absolute path\n",
    "df['utt'] = df['utt'].apply(lambda x: os.path.join(\"wav/\", x))\n",
    "df\n",
    "# export to protocol.txt\n",
    "df.to_csv(\"ADD2023_R2_protocol.txt\", sep=\" \", header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377acf5b",
   "metadata": {},
   "source": [
    "# Verify all folders in benchrmark have protocol.txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c08acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BENCHMARK_ROOT = \"/nvme1/hungdx/Lightning-hydra/data/huggingface_benchrmark_Speech-DF-Arena\"\n",
    "list_of_folders = os.listdir(BENCHMARK_ROOT)\n",
    "\n",
    "# only keep folders\n",
    " \n",
    "list_of_folders =  [folder for folder in list_of_folders if os.path.isdir(os.path.join(BENCHMARK_ROOT, folder))]\n",
    "\n",
    "for folder in list_of_folders:\n",
    "    # check if folder have protocol.txt\n",
    "    if not os.path.isfile(os.path.join(BENCHMARK_ROOT, folder, \"protocol.txt\")):\n",
    "        print(f\"{folder} do not have protocol.txt\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232655c",
   "metadata": {},
   "source": [
    "# MLAAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "484dac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for folder names with spaces in /nvme1/hungdx/Lightning-hydra/data/MLAAD/MLAAD_v6/fake...\n",
      "No directories with spaces found\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import shutil\n",
    "\n",
    "\n",
    "def rename_folders_with_spaces(directory):\n",
    "    \"\"\"\n",
    "    Recursively find folders with spaces in their names and rename them by replacing spaces with underscores.\n",
    "    Returns a dictionary mapping old paths to new paths for later reference.\n",
    "    \"\"\"\n",
    "    path_mapping = {}\n",
    "    directory = Path(directory)\n",
    "    \n",
    "    print(f\"Checking for folder names with spaces in {directory}...\")\n",
    "    \n",
    "    # First pass: collect all directories that need renaming\n",
    "    dirs_to_rename = []\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):\n",
    "        for dir_name in dirs:\n",
    "            if \" \" in dir_name:\n",
    "                old_path = os.path.join(root, dir_name)\n",
    "                new_name = dir_name.replace(\" \", \"_\")\n",
    "                new_path = os.path.join(root, new_name)\n",
    "                dirs_to_rename.append((old_path, new_path))\n",
    "    \n",
    "    # Second pass: rename directories\n",
    "    for old_path, new_path in dirs_to_rename:\n",
    "        try:\n",
    "            print(f\"Renaming: {old_path} -> {new_path}\")\n",
    "            # Check if target already exists\n",
    "            if os.path.exists(new_path):\n",
    "                print(f\"Warning: Target path {new_path} already exists. Skipping...\")\n",
    "                continue\n",
    "                \n",
    "            os.rename(old_path, new_path)\n",
    "            path_mapping[old_path] = new_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error renaming {old_path}: {e}\")\n",
    "    \n",
    "    return path_mapping\n",
    "spoof_dir = \"/nvme1/hungdx/Lightning-hydra/data/MLAAD/MLAAD_v6/fake\"\n",
    "path_mapping = rename_folders_with_spaces(spoof_dir)\n",
    "if path_mapping:\n",
    "    print(f\"Renamed {len(path_mapping)} directories with spaces in their names\")\n",
    "else:\n",
    "    print(\"No directories with spaces found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a606525c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bonafide files from /nvme1/hungdx/Lightning-hydra/data/MLAAD/M-AILABS...\n",
      "Warning: de_DE/by_book/female/ramona_deininger/tom_sawyer/wavs/._tom_sawyer_09_f000196.wav contain ._ \n",
      "Warning: en_US/by_book/male/elliot_miller/silent_bullet/wavs/._silent_bullet_11_f000200.wav contain ._ \n",
      "Warning: en_US/by_book/female/judy_bieber/ozma_of_oz/wavs/._ozma_of_oz_10_f000095.wav contain ._ \n",
      "Found 243059 bonafide files\n",
      "Processing spoof files from /nvme1/hungdx/Lightning-hydra/data/MLAAD/MLAAD_v6/fake...\n",
      "Found 175000 spoof files\n",
      "Protocol file written to /nvme1/hungdx/Lightning-hydra/data/MLAAD/protocol.txt\n",
      "Metadata file written to /nvme1/hungdx/Lightning-hydra/data/MLAAD/meta.csv\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def process_bonafide_files(bonafide_dir):\n",
    "    \"\"\"Process M-AILABS (bonafide) dataset and extract paths and metadata\"\"\"\n",
    "    bonafide_paths = []\n",
    "    bonafide_metadata = []\n",
    "    \n",
    "    bonafide_dir = Path(bonafide_dir)\n",
    "    \n",
    "    # Walk through the directory structure\n",
    "    for language_dir in bonafide_dir.iterdir():\n",
    "        if not language_dir.is_dir() or language_dir.name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        language_code = language_dir.name  # e.g., de_DE, pl_PL\n",
    "        \n",
    "        by_book_dir = language_dir / \"by_book\"\n",
    "        if not by_book_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        # Process female and male directories\n",
    "        for gender_dir in by_book_dir.iterdir():\n",
    "            if not gender_dir.is_dir() or gender_dir.name not in ['female', 'male']:\n",
    "                continue\n",
    "                \n",
    "            # Process speaker directories\n",
    "            for speaker_dir in gender_dir.iterdir():\n",
    "                if not speaker_dir.is_dir():\n",
    "                    continue\n",
    "                    \n",
    "                # Process book directories\n",
    "                for book_dir in speaker_dir.iterdir():\n",
    "                    if not book_dir.is_dir():\n",
    "                        continue\n",
    "                        \n",
    "                    # Find the metadata.csv file\n",
    "                    metadata_csv = book_dir / \"metadata.csv\"\n",
    "                    wavs_dir = book_dir / \"wavs\"\n",
    "                    \n",
    "                    if not metadata_csv.exists() or not wavs_dir.exists():\n",
    "                        continue\n",
    "                        \n",
    "                    # Read metadata\n",
    "                    metadata_dict = {}\n",
    "                    try:\n",
    "                        with open(metadata_csv, 'r', encoding='utf-8') as f:\n",
    "                            for line in f:\n",
    "                                parts = line.strip().split('|')\n",
    "                                if len(parts) >= 2:\n",
    "                                    file_id = parts[0]\n",
    "                                    transcript = parts[1]\n",
    "                                    metadata_dict[file_id] = transcript\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading metadata from {metadata_csv}: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Process WAV files\n",
    "                    for wav_file in wavs_dir.glob('*.wav'):\n",
    "                        relative_path = wav_file.relative_to(bonafide_dir)\n",
    "                        file_id = wav_file.stem\n",
    "                        \n",
    "                        if \"._\" in str(relative_path):\n",
    "                            print(f\"Warning: {relative_path} contain ._ \")\n",
    "                            continue\n",
    "                        \n",
    "                        relative_path = \"M-AILABS/\" + str(relative_path)\n",
    "                        \n",
    "                        # Check if relative_path not contain \" copy\"\n",
    "                        if \" copy\" in relative_path:\n",
    "                            continue\n",
    "                        \n",
    "                        # Add path to protocol list\n",
    "                        bonafide_paths.append(relative_path)\n",
    "                        \n",
    "                        # Extract transcript from metadata if available\n",
    "                        transcript = metadata_dict.get(file_id, \"-\")\n",
    "                        \n",
    "                        # Create metadata entry\n",
    "                        metadata_entry = {\n",
    "                            'path': str(relative_path),\n",
    "                            'original_file': \"-\",\n",
    "                            'language': language_code,\n",
    "                            'is_original_language': \"-\",\n",
    "                            'duration': \"-\",\n",
    "                            'training_data': \"-\",\n",
    "                            'model_name': \"-\",\n",
    "                            'architecture': \"-\",\n",
    "                            'transcript': transcript,\n",
    "                            'label': \"bonafide\"\n",
    "                        }\n",
    "                        bonafide_metadata.append(metadata_entry)\n",
    "    \n",
    "    return bonafide_paths, bonafide_metadata\n",
    "\n",
    "def process_spoof_files(spoof_dir):\n",
    "    \"\"\"Process MLAAD_v6 (spoof) dataset and extract paths and metadata\"\"\"\n",
    "    spoof_paths = []\n",
    "    spoof_metadata = []\n",
    "    \n",
    "    spoof_dir = Path(spoof_dir)\n",
    "    \n",
    "    # Walk through the directory structure\n",
    "    for language_dir in spoof_dir.iterdir():\n",
    "        if not language_dir.is_dir() or language_dir.name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        # Process model directories\n",
    "        for model_dir in language_dir.iterdir():\n",
    "            if not model_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            # Find the meta.csv file\n",
    "            meta_csv = model_dir / \"meta.csv\"\n",
    "            if not meta_csv.exists():\n",
    "                continue\n",
    "                \n",
    "            # Read metadata from CSV\n",
    "            try:\n",
    "                df = pd.read_csv(meta_csv, quoting=csv.QUOTE_NONE, delimiter='|')\n",
    "                \n",
    "                # Process each row in the metadata\n",
    "                for _, row in df.iterrows():\n",
    "                    wav_file = row['path']\n",
    "                    # if not wav_file.exists():\n",
    "                    #     print(f\"Warning: Audio file {wav_file} mentioned in metadata does not exist\")\n",
    "                    #     continue\n",
    "                    wav_file = wav_file.split(\"./\")[1]\n",
    "                    if \"Resemble.ai (April 12th, 2025)\" in wav_file:\n",
    "                        # replace \"Resemble.ai (April 12th, 2025)\" to \"Resemble.ai_12_April_2025\"\n",
    "                        wav_file = wav_file.replace(\"Resemble.ai (April 12th, 2025)\", \"Resemble.ai_12_April_2025\")\n",
    "                    if \"Nari Dia-1.6B\" in wav_file:\n",
    "                        # replace \"Nari Dia-1.6B\" to \"Nari_Dia-1.6B\"\n",
    "                        wav_file = wav_file.replace(\"Nari Dia-1.6B\", \"Nari_Dia-1.6B\")\n",
    "                    relative_path = \"MLAAD_v6/\" + wav_file\n",
    "                    #relative_path = wav_file.relative_to(spoof_dir)\n",
    "                    \n",
    "                    # Add path to protocol list\n",
    "                    spoof_paths.append(str(relative_path))\n",
    "                    \n",
    "                    # Create metadata entry with all original fields plus label\n",
    "                    metadata_entry = {**row.to_dict(), 'label': 'spoof'}\n",
    "                    metadata_entry['path'] = str(relative_path)  # Make sure path is relative\n",
    "                    spoof_metadata.append(metadata_entry)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing metadata from {meta_csv}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return spoof_paths, spoof_metadata\n",
    "\n",
    "def write_protocol_file(bonafide_paths, spoof_paths, output_path):\n",
    "    \"\"\"Write protocol file with <path> <subset> <label> format\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        # Write bonafide entries\n",
    "        for path in bonafide_paths:\n",
    "            f.write(f\"{path} eval bonafide\\n\")\n",
    "        \n",
    "        # Write spoof entries\n",
    "        for path in spoof_paths:\n",
    "            f.write(f\"{path} eval spoof\\n\")\n",
    "    \n",
    "    print(f\"Protocol file written to {output_path}\")\n",
    "\n",
    "def write_metadata_file(bonafide_metadata, spoof_metadata, output_path):\n",
    "    \"\"\"Write consolidated metadata file\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Combine metadata and ensure all entries have the same columns\n",
    "    all_metadata = bonafide_metadata + spoof_metadata\n",
    "    \n",
    "    # Get all column names\n",
    "    columns = set()\n",
    "    for entry in all_metadata:\n",
    "        columns.update(entry.keys())\n",
    "    \n",
    "    # Sort columns to ensure consistent order, with 'path' first and 'label' last\n",
    "    columns = sorted(columns)\n",
    "    if 'path' in columns:\n",
    "        columns.remove('path')\n",
    "        columns = ['path'] + columns\n",
    "    if 'label' in columns:\n",
    "        columns.remove('label')\n",
    "        columns.append('label')\n",
    "    \n",
    "    # Write CSV\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=columns, delimiter='|')\n",
    "        writer.writeheader()\n",
    "        for entry in all_metadata:\n",
    "            # Ensure all columns are present\n",
    "            for col in columns:\n",
    "                if col not in entry:\n",
    "                    entry[col] = \"-\"\n",
    "            writer.writerow(entry)\n",
    "    \n",
    "    print(f\"Metadata file written to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    bonafide_dir = \"/nvme1/hungdx/Lightning-hydra/data/MLAAD/M-AILABS\"\n",
    "    spoof_dir = \"/nvme1/hungdx/Lightning-hydra/data/MLAAD/MLAAD_v6/fake\"\n",
    "    output_dir = \"/nvme1/hungdx/Lightning-hydra/data/MLAAD\"\n",
    "    \n",
    "    print(f\"Processing bonafide files from {bonafide_dir}...\")\n",
    "    bonafide_paths, bonafide_metadata = process_bonafide_files(bonafide_dir)\n",
    "    print(f\"Found {len(bonafide_paths)} bonafide files\")\n",
    "    \n",
    "    print(f\"Processing spoof files from {spoof_dir}...\")\n",
    "    spoof_paths, spoof_metadata = process_spoof_files(spoof_dir)\n",
    "    print(f\"Found {len(spoof_paths)} spoof files\")\n",
    "    \n",
    "    # Write protocol file\n",
    "    protocol_path = os.path.join(output_dir, \"protocol.txt\")\n",
    "    write_protocol_file(bonafide_paths, spoof_paths, protocol_path)\n",
    "    \n",
    "    # Write metadata file\n",
    "    metadata_path = os.path.join(output_dir, \"meta.csv\")\n",
    "    write_metadata_file(bonafide_metadata, spoof_metadata, metadata_path)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffff189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chechk valid protocol.txt\n",
    "import os\n",
    "\n",
    "protocol_path = \"/nvme1/hungdx/Lightning-hydra/data/MLAAD/protocol.txt\"\n",
    "\n",
    "for line in open(protocol_path, \"r\"):\n",
    "    try:\n",
    "        path, subset, label = line.strip().split()\n",
    "    except:\n",
    "        print(line)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c9ebf",
   "metadata": {},
   "source": [
    "# AIHUB_FreeCommunication_may"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41739c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing to remove column and make it to correct form: path subset eval\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "protocol_file = \"/nvme1/hungdx/Lightning-hydra/data/cnsl_benchmark/AIHUB_FreeCommunication_may/AIHUB_FreeCommunication_may_protocol.txt\"\n",
    "\n",
    "# read protocol file\n",
    "df = pd.read_csv(protocol_file, sep=\" \", header=None)\n",
    "df.columns = [\"path\", \"spk\", \"sth\", \"label\"]\n",
    "\n",
    "# remove column spk and sth\n",
    "df = df.drop(columns=[\"spk\", \"sth\"])\n",
    "\n",
    "# add new column subset\n",
    "df[\"subset\"] = \"eval\"\n",
    "\n",
    "# re-arrange column\n",
    "df = df[[\"path\", \"subset\", \"label\"]]\n",
    "\n",
    "# save to csv\n",
    "df.to_csv(\"/nvme1/hungdx/Lightning-hydra/data/cnsl_benchmark/AIHUB_FreeCommunication_may/protocol.txt\", index=False, header=False, sep=\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb335340",
   "metadata": {},
   "source": [
    "# AIHUB_FreeCommunication June2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load bonafide data\n",
    "bonafide_df = pd.read_csv('/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHUB_FreeCommunication/may/may_bona.csv')\n",
    "\n",
    "print(bonafide_df.head())\n",
    "# update bonafide_df['path_audio'] to be like spoof_df['path_audio']\n",
    "bonafide_df['path_audio'] = bonafide_df['path_audio'].apply(lambda x: f\"Bonafide/{x}\")\n",
    "\n",
    "bonafide_df['label'] = 'bonafide'\n",
    "#bonafide_df['attack_type'] = '-'\n",
    "\n",
    "# Check if there are common columns for merging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec2614",
   "metadata": {},
   "source": [
    "# AIHUB_FreeCommunication June2 Spoof Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c40346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spoof directory: /nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHUB_FreeCommunication/june/june_week2/Spoof\n",
      "Processing attack: a04\n",
      "  Found 100 speakers in a04\n",
      "Processing attack: a08\n",
      "  Found 100 speakers in a08\n",
      "Processing attack: a10\n",
      "  Found 100 speakers in a10\n",
      "Processing attack: a01\n",
      "  Found 100 speakers in a01\n",
      "Processing attack: a03\n",
      "  Found 100 speakers in a03\n",
      "Processing attack: a06\n",
      "  Found 100 speakers in a06\n",
      "Processing attack: a07\n",
      "  Found 100 speakers in a07\n",
      "Processing attack: a09\n",
      "  Found 100 speakers in a09\n",
      "Processing attack: a11\n",
      "  Found 100 speakers in a11\n",
      "Processing attack: a12\n",
      "  Found 100 speakers in a12\n",
      "Processing attack: a13\n",
      "  Found 100 speakers in a13\n",
      "\n",
      "Total spoof files found: 54923\n",
      "Number of unique attack types: 11\n",
      "\n",
      "First 10 rows of the DataFrame:\n",
      "            path subset  label\n",
      "0   a04/10/1.wav   eval  spoof\n",
      "1  a04/10/10.wav   eval  spoof\n",
      "2  a04/10/11.wav   eval  spoof\n",
      "3  a04/10/12.wav   eval  spoof\n",
      "4  a04/10/13.wav   eval  spoof\n",
      "5  a04/10/14.wav   eval  spoof\n",
      "6  a04/10/15.wav   eval  spoof\n",
      "7  a04/10/16.wav   eval  spoof\n",
      "8  a04/10/17.wav   eval  spoof\n",
      "9  a04/10/18.wav   eval  spoof\n",
      "\n",
      "Attack type distribution:\n",
      "path\n",
      "a04    5000\n",
      "a08    5000\n",
      "a10    5000\n",
      "a01    5000\n",
      "a03    5000\n",
      "a06    5000\n",
      "a09    5000\n",
      "a12    5000\n",
      "a11    5000\n",
      "a13    5000\n",
      "a07    4923\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def collect_spoof_files(spoof_base_dir):\n",
    "    \"\"\"\n",
    "    Traverse spoof directory structure and collect all WAV files\n",
    "    Structure: spoof_base_dir/attack_id/speaker_id/audio_files.wav\n",
    "    \"\"\"\n",
    "    spoof_data = []\n",
    "    spoof_base_path = Path(spoof_base_dir)\n",
    "    \n",
    "    print(f\"Processing spoof directory: {spoof_base_dir}\")\n",
    "    \n",
    "    # Iterate through attack folders (a01, a03, a04, etc.)\n",
    "    for attack_folder in spoof_base_path.iterdir():\n",
    "        if not attack_folder.is_dir():\n",
    "            continue\n",
    "            \n",
    "        attack_id = attack_folder.name\n",
    "        print(f\"Processing attack: {attack_id}\")\n",
    "        \n",
    "        # Iterate through speaker folders within each attack\n",
    "        speaker_count = 0\n",
    "        for speaker_folder in attack_folder.iterdir():\n",
    "            if not speaker_folder.is_dir():\n",
    "                continue\n",
    "                \n",
    "            speaker_id = speaker_folder.name\n",
    "            speaker_count += 1\n",
    "            \n",
    "            # Find all WAV files in this speaker folder\n",
    "            wav_files = list(speaker_folder.glob(\"*.wav\"))\n",
    "            \n",
    "            for wav_file in wav_files:\n",
    "                # Create relative path from the spoof base directory\n",
    "                relative_path = wav_file.relative_to(spoof_base_path)\n",
    "                \n",
    "                # Add to our data collection\n",
    "                spoof_data.append({\n",
    "                    'path': str(relative_path),\n",
    "                    'subset': 'eval', \n",
    "                    'label': 'spoof'\n",
    "                })\n",
    "        \n",
    "        print(f\"  Found {speaker_count} speakers in {attack_id}\")\n",
    "    \n",
    "    return spoof_data\n",
    "\n",
    "# Set your spoof directory path\n",
    "spoof_directory = \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHUB_FreeCommunication/june/june_week2/Spoof\"\n",
    "\n",
    "# Collect all spoof files\n",
    "spoof_files_data = collect_spoof_files(spoof_directory)\n",
    "\n",
    "# Create DataFrame\n",
    "spoof_df = pd.DataFrame(spoof_files_data)\n",
    "\n",
    "print(f\"\\nTotal spoof files found: {len(spoof_df)}\")\n",
    "print(f\"Number of unique attack types: {spoof_df['path'].str.split('/').str[0].nunique()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 10 rows of the DataFrame:\")\n",
    "print(spoof_df.head(10))\n",
    "\n",
    "# Display some statistics\n",
    "print(f\"\\nAttack type distribution:\")\n",
    "attack_counts = spoof_df['path'].str.split('/').str[0].value_counts()\n",
    "print(attack_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8654c56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spoof protocol file saved to: /nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHUB_FreeCommunication/june/june_week2/aihub_june2_spoof_protocol.txt\n",
      "\n",
      "DataFrame shape: (54923, 3)\n",
      "Columns: ['path', 'subset', 'label']\n",
      "\n",
      "Random sample of 5 rows:\n",
      "                  path subset  label\n",
      "4302   a04/1332/11.wav   eval  spoof\n",
      "6365   a08/1095/23.wav   eval  spoof\n",
      "46686  a12/1126/21.wav   eval  spoof\n",
      "5498    a08/1033/8.wav   eval  spoof\n",
      "35466  a09/1039/49.wav   eval  spoof\n",
      "\n",
      "File extensions found:\n",
      "path\n",
      "wav    54923\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Optional: Save the DataFrame to CSV\n",
    "output_file = \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHUB_FreeCommunication/june/june_week2/aihub_june2_spoof_protocol.txt\"\n",
    "spoof_df.to_csv(output_file, sep=' ', header=False, index=False)\n",
    "print(f\"Spoof protocol file saved to: {output_file}\")\n",
    "\n",
    "# Display more detailed information\n",
    "print(f\"\\nDataFrame shape: {spoof_df.shape}\")\n",
    "print(f\"Columns: {list(spoof_df.columns)}\")\n",
    "\n",
    "# Sample some random rows\n",
    "print(f\"\\nRandom sample of 5 rows:\")\n",
    "print(spoof_df.sample(5) if len(spoof_df) > 5 else spoof_df)\n",
    "\n",
    "# Check if there are any files with different extensions\n",
    "print(f\"\\nFile extensions found:\")\n",
    "extensions = spoof_df['path'].str.split('.').str[-1].value_counts()\n",
    "print(extensions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ce6c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  speaker_norm                                         path_audio    age  \\\n",
      "0         1060  Training/data/TS_random_01/random/2021-12-15/1...  adult   \n",
      "1         1060  Training/data/TS_random_01/random/2021-12-15/1...  adult   \n",
      "2         1060  Training/data/TS_random_01/random/2021-12-15/1...  adult   \n",
      "3         1060  Training/data/TS_random_01/random/2021-12-15/1...  adult   \n",
      "4         1060  Training/data/TS_random_01/random/2021-12-15/1...  adult   \n",
      "\n",
      "  gender  recrdTime recrdEnv recrdDevice subset  \\\n",
      "0      M       1.96    Clean  SmartPhone  Train   \n",
      "1      M       3.71    Clean  SmartPhone  Train   \n",
      "2      M       5.89    Clean  SmartPhone  Train   \n",
      "3      M       4.86    Clean  SmartPhone  Train   \n",
      "4      M       5.00    Clean  SmartPhone  Train   \n",
      "\n",
      "                                          path_label  \n",
      "0  Training/label/TL_random_01/random/2021-12-15/...  \n",
      "1  Training/label/TL_random_01/random/2021-12-15/...  \n",
      "2  Training/label/TL_random_01/random/2021-12-15/...  \n",
      "3  Training/label/TL_random_01/random/2021-12-15/...  \n",
      "4  Training/label/TL_random_01/random/2021-12-15/...  \n",
      "Spoof columns: ['path_audio', 'subset', 'label', 'attack_type']\n",
      "Bonafide columns: ['speaker_norm', 'path_audio', 'age', 'gender', 'recrdTime', 'recrdEnv', 'recrdDevice', 'subset', 'path_label', 'label']\n",
      "Common columns: {'path_audio', 'subset', 'label'}\n",
      "Files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spoof_df = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHUB_FreeCommunication/june/june_week2/aihub_june2_spoof_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "spoof_df.columns = [\"path_audio\", \"subset\", \"label\"]\n",
    "# if spoof_df['path_audio'] startwith ax whee x can start from 1-14 then add a column attack_type = a01, a02, ...\n",
    "# it's better if using regex\n",
    "spoof_df['attack_type'] = spoof_df['path_audio'].str.extract(r'a(\\d{2})')\n",
    "\n",
    "spoof_df['attack_type'] = spoof_df['attack_type'].apply(lambda x: f\"a{x}\")\n",
    "\n",
    "spoof_df['path_audio'] = spoof_df['path_audio'].apply(lambda x: f\"Spoof/{x}\")\n",
    "\n",
    "bonafide_df = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHUB_FreeCommunication/may/may_bona.csv\")\n",
    "\n",
    "print(bonafide_df.head())\n",
    "# update bonafide_df['path_audio'] to be like spoof_df['path_audio']\n",
    "bonafide_df['path_audio'] = bonafide_df['path_audio'].apply(lambda x: f\"Bonafide/{x}\")\n",
    "\n",
    "\n",
    "bonafide_df['label'] = 'bonafide'\n",
    "#bonafide_df['attack_type'] = '-'\n",
    "\n",
    "# Check if there are common columns for merging\n",
    "print(\"Spoof columns:\", spoof_df.columns.tolist())\n",
    "print(\"Bonafide columns:\", bonafide_df.columns.tolist())\n",
    "print(\"Common columns:\", set(spoof_df.columns) & set(bonafide_df.columns))\n",
    "\n",
    "# If you need to merge on path_audio, use outer join to keep all records\n",
    "merged_df = spoof_df.merge(bonafide_df, on='path_audio', how='outer', suffixes=('_spoof', '_bonafide'))\n",
    "\n",
    "# Handle label conflicts if any\n",
    "if 'label_spoof' in merged_df.columns and 'label_bonafide' in merged_df.columns:\n",
    "    merged_df['label'] = merged_df['label_spoof'].fillna(merged_df['label_bonafide'])\n",
    "    merged_df = merged_df.drop(['label_spoof', 'label_bonafide'], axis=1)\n",
    "\n",
    "# Continue with the rest of the processing...\n",
    "merged_df['subset'] = 'eval'\n",
    "\n",
    "# fill missing value of attack_type with \"-\"\n",
    "merged_df['attack_type'] = merged_df['attack_type'].fillna('-')\n",
    "\n",
    "# Save files\n",
    "merged_df.to_csv('/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHUB_FreeCommunication/june/june_week2/june_week2_merged_df.csv', index=False)\n",
    "\n",
    "merged_df = merged_df[['path_audio', 'subset', 'label']]\n",
    "merged_df.to_csv('/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHUB_FreeCommunication/june/june_week2/protocol.txt', header=False, sep=' ', index=False)\n",
    "\n",
    "print(\"Files saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
