{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b7f510",
   "metadata": {},
   "source": [
    "# Deepen ASVSpoof & MLAAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b5120",
   "metadata": {},
   "source": [
    "## ASVSpoof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f425791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Loaded metadata with shape: (7200, 11)\n",
      "Columns: ['path', 'subset', 'label', 'hash', 'path_audio', 'carrier', 'carrier_fine', 'modification', 'desc', 'target', 'partition']\n",
      "\n",
      "Processing asv19_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g4_june27.txt...\n",
      "Processed results shape: (3600, 15)\n",
      "Required columns present: True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model: asv19_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g4_june27.txt\n",
      "\n",
      "Total Samples: 3600\n",
      "Overall Accuracy: 68.78%\n",
      "Overall EER: 21.11%\n",
      "\n",
      "Metrics by group:\n",
      "  ADD_BACKGROUND_MUSIC:\n",
      "    Accuracy: 65.50%\n",
      "    EER: 26.00%\n",
      "    Threshold: -4.4287\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  ADD_BACKGROUND_NOISE:\n",
      "    Accuracy: 78.00%\n",
      "    EER: 27.00%\n",
      "    Threshold: -4.5923\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  AMPLITUDE_MODULATION:\n",
      "    Accuracy: 56.50%\n",
      "    EER: 11.00%\n",
      "    Threshold: -5.1046\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  AUTOTUNE:\n",
      "    Accuracy: 53.00%\n",
      "    EER: 14.00%\n",
      "    Threshold: -4.9811\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  BASELINE:\n",
      "    Accuracy: 85.00%\n",
      "    EER: 19.00%\n",
      "    Threshold: -4.9520\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  BIT_DEPTH:\n",
      "    Accuracy: 61.50%\n",
      "    EER: 17.00%\n",
      "    Threshold: -4.8738\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  ECHO:\n",
      "    Accuracy: 51.00%\n",
      "    EER: 18.00%\n",
      "    Threshold: -5.0397\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  EQUALIZATION:\n",
      "    Accuracy: 87.00%\n",
      "    EER: 14.00%\n",
      "    Threshold: -4.9940\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  FREQ_MINUS:\n",
      "    Accuracy: 70.00%\n",
      "    EER: 20.00%\n",
      "    Threshold: -4.9197\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  FREQ_PLUS:\n",
      "    Accuracy: 68.50%\n",
      "    EER: 21.00%\n",
      "    Threshold: -4.9393\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  GAUSSIAN_NOISE:\n",
      "    Accuracy: 66.00%\n",
      "    EER: 48.00%\n",
      "    Threshold: -2.1997\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  HIGH_PASS_FILTER:\n",
      "    Accuracy: 85.00%\n",
      "    EER: 40.00%\n",
      "    Threshold: -3.0001\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  LOW_PASS_FILTER:\n",
      "    Accuracy: 73.50%\n",
      "    EER: 12.00%\n",
      "    Threshold: -5.1017\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  MP3_COMPRESSION:\n",
      "    Accuracy: 67.00%\n",
      "    EER: 11.00%\n",
      "    Threshold: -5.0737\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  PITCH_SHIFT:\n",
      "    Accuracy: 53.50%\n",
      "    EER: 22.00%\n",
      "    Threshold: -4.7457\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  REVERB:\n",
      "    Accuracy: 86.50%\n",
      "    EER: 12.00%\n",
      "    Threshold: -5.0116\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  SILENCE_INJECTION:\n",
      "    Accuracy: 79.00%\n",
      "    EER: 18.00%\n",
      "    Threshold: -4.9051\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  TIME_STRETCH:\n",
      "    Accuracy: 51.50%\n",
      "    EER: 18.00%\n",
      "    Threshold: -4.7635\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "\n",
      "Additional metrics:\n",
      "  F1 Score: 0.5554\n",
      "  Recall: 0.3900\n",
      "  Precision: 0.9643\n",
      "----------------------------------------------------------------------\n",
      "ADD_BACKGROUND_MUSIC,ADD_BACKGROUND_NOISE,AMPLITUDE_MODULATION,AUTOTUNE,BASELINE,BIT_DEPTH,ECHO,EQUALIZATION,FREQ_MINUS,FREQ_PLUS,GAUSSIAN_NOISE,HIGH_PASS_FILTER,LOW_PASS_FILTER,MP3_COMPRESSION,PITCH_SHIFT,REVERB,SILENCE_INJECTION,TIME_STRETCH,Pooled EER\n",
      "26.00,27.00,11.00,14.00,19.00,17.00,18.00,14.00,20.00,21.00,48.00,40.00,12.00,11.00,22.00,12.00,18.00,18.00,20.44\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Dict, Optional, List, Union, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "from pathlib import Path\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "# Constants\n",
    "METADATA_PATH = \"/nvme1/hungdx/Lightning-hydra/data/shortcutASV/asv19/protocol.txt\"\n",
    "\n",
    "META_CSV_PATH = \"/nvme1/hungdx/Lightning-hydra/data/shortcutASV/asv19/data.csv\"\n",
    "\n",
    "# old prediction file\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/Conformer_MDT_DEC2024_correct/asv19_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_lora_infer_Conformer_MDT_DEC2024_correct.txt\"\n",
    "\n",
    "# new g1-june26\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_june26/asv19_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_june26.txt\"\n",
    "\n",
    "# new g2-june26\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g2_june26/asv19_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g2_june26.txt\"\n",
    "\n",
    "# new g1-june27\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_june27/asv19_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_june27.txt\"\n",
    "\n",
    "# new g2-june27\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g2_june27/asv19_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g2_june27.txt\"\n",
    "\n",
    "# new g3-june27\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g3_june27/asv19_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g3_june27.txt\"\n",
    "\n",
    "# new g4-june27\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g4_june27/asv19_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g4_june27.txt\"\n",
    "\n",
    "# new g1-june29\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_june29/asv19_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_june29.txt\"\n",
    "\n",
    "# new g5-june29\n",
    "\n",
    "\n",
    "class MetricsCalculator:\n",
    "    @staticmethod\n",
    "    def calculate_metrics(df: pd.DataFrame, group_column: Optional[str] = None) -> Dict[str, Union[float, Dict[str, Union[float, int]]]]:\n",
    "        \"\"\"Calculate various metrics for the given DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing predictions and ground truth\n",
    "            group_column: Optional column name to group results by\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing calculated metrics and sample counts\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'overall': accuracy_score(df[\"label\"], df[\"pred\"]) * 100,\n",
    "            'f1': f1_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'recall': recall_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'precision': precision_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'total_samples': len(df)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall EER\n",
    "        bona_cm = df[df['label'] == 'bonafide']['score'].values\n",
    "        spoof_cm = df[df['label'] == 'spoof']['score'].values\n",
    "        eer_cm, _ = em.compute_eer(bona_cm, spoof_cm)\n",
    "        results['overall_eer'] = eer_cm * 100\n",
    "        \n",
    "        if group_column and group_column in df.columns:\n",
    "            group_metrics = {}\n",
    "            \n",
    "            # Get all bona fide samples\n",
    "            all_bona = df[df['label'] == 'bonafide']\n",
    "            \n",
    "            # Get all spoof samples\n",
    "            all_spoof = df[df['label'] == 'spoof']\n",
    "            \n",
    "            # Process each attack type\n",
    "            for group, group_df in df.groupby(group_column):\n",
    "                if group == '-':  # For bona fide samples\n",
    "                    # Use all spoof samples and all bona fide samples\n",
    "                    group_bona = all_bona['score'].values\n",
    "                    group_spoof = all_spoof['score'].values\n",
    "                else:  # For spoof attack types\n",
    "                    # Get spoof samples for this attack type\n",
    "                    group_spoof = group_df[group_df['label'] == 'spoof']['score'].values\n",
    "                    # Randomly select equal number of bona fide samples\n",
    "                    n_spoof = len(group_spoof)\n",
    "                    group_bona = np.random.choice(all_bona['score'].values, size=n_spoof, replace=False)\n",
    "                \n",
    "                # Calculate EER for this group\n",
    "                group_eer, group_threshold = em.compute_eer(group_bona, group_spoof)\n",
    "                \n",
    "                group_metrics[group] = {\n",
    "                    'accuracy': accuracy_score(group_df[\"label\"], group_df[\"pred\"]) * 100,\n",
    "                    'eer': group_eer * 100,\n",
    "                    'threshold': group_threshold,\n",
    "                    'samples': len(group_df),\n",
    "                    'bona_samples': len(group_bona),\n",
    "                    'spoof_samples': len(group_spoof)\n",
    "                }\n",
    "            results['groups'] = group_metrics\n",
    "        \n",
    "        return results\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    \"\"\"Load and process metadata files with proper column handling.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing merged metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load protocol file\n",
    "        metadata = pd.read_csv(METADATA_PATH, sep=\" \", header=None)\n",
    "        metadata.columns = [\"path\", \"subset\", \"label\"]\n",
    "        \n",
    "        # Load CSV metadata\n",
    "        meta_csv = pd.read_csv(META_CSV_PATH, sep=\"|\")\n",
    "        # rename columns from file_path to path_audio\n",
    "        \n",
    "        meta_csv = meta_csv.rename(columns={'file_path': 'path_audio'})\n",
    "\n",
    "        \n",
    "        # Handle column conflicts before merge by dropping conflicting columns from meta_csv\n",
    "        conflicting_cols = ['subset', 'label', 'path']\n",
    "        for col in conflicting_cols:\n",
    "            if col in meta_csv.columns:\n",
    "                print(f\"Dropping conflicting column '{col}' from CSV metadata\")\n",
    "                meta_csv = meta_csv.drop(columns=[col])\n",
    "        \n",
    "        # Check if 'path_audio' column exists\n",
    "        if 'path_audio' not in meta_csv.columns:\n",
    "            # Try to find similar column names\n",
    "            audio_cols = [col for col in meta_csv.columns if 'path' in col.lower() or 'audio' in col.lower()]\n",
    "            if audio_cols:\n",
    "                meta_csv['path_audio'] = meta_csv[audio_cols[0]]\n",
    "            else:\n",
    "                raise ValueError(\"No suitable audio path column found in CSV metadata\")\n",
    "        \n",
    "        # Perform merge\n",
    "        merged_metadata = metadata.merge(meta_csv, left_on='path', right_on='path_audio', how='left')\n",
    "        \n",
    "        # Clean up any remaining suffixed columns (shouldn't happen now, but just in case)\n",
    "        if 'label' not in merged_metadata.columns:\n",
    "            if 'label_x' in merged_metadata.columns:\n",
    "                merged_metadata['label'] = merged_metadata['label_x']\n",
    "                merged_metadata = merged_metadata.drop(columns=['label_x'])\n",
    "            if 'label_y' in merged_metadata.columns:\n",
    "                merged_metadata = merged_metadata.drop(columns=['label_y'])\n",
    "        \n",
    "        if 'subset' not in merged_metadata.columns:\n",
    "            if 'subset_x' in merged_metadata.columns:\n",
    "                merged_metadata['subset'] = merged_metadata['subset_x']\n",
    "                merged_metadata = merged_metadata.drop(columns=['subset_x'])\n",
    "            if 'subset_y' in merged_metadata.columns:\n",
    "                merged_metadata = merged_metadata.drop(columns=['subset_y'])\n",
    "        \n",
    "        # Fix path column if it got suffixed\n",
    "        if 'path' not in merged_metadata.columns and 'path_x' in merged_metadata.columns:\n",
    "            merged_metadata['path'] = merged_metadata['path_x']\n",
    "            merged_metadata = merged_metadata.drop(columns=['path_x'])\n",
    "        if 'path_y' in merged_metadata.columns:\n",
    "            merged_metadata = merged_metadata.drop(columns=['path_y'])\n",
    "        \n",
    "        return merged_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load metadata: {str(e)}\")\n",
    "\n",
    "def process_prediction_file(score_file: str, metadata_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process a single prediction file and return results DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        score_file: Path to the prediction file\n",
    "        metadata_df: DataFrame containing metadata\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing processed predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "        pred_df.columns = [\"path\", \"spoof\", \"score\"]\n",
    "        pred_df = pred_df.drop_duplicates(subset=['path'])\n",
    "        \n",
    "        \n",
    "        # Remove all utterances starting with \"Spoof/a09\" to \"Spoof/a13\"\n",
    "        # This is to exclude specific utterances from the evaluation\n",
    "        # Adjust the condition based on your dataset structure\n",
    "        #pred_df = pred_df[~pred_df[\"path\"].str.startswith((\"Spoof/a09\", \"Spoof/a10\", \"Spoof/a11\", \"Spoof/a12\", \"Spoof/a13\"))]\n",
    "        \n",
    "        #### \n",
    "        \n",
    "        # Perform merge\n",
    "        merged_df = pred_df.merge(metadata_df, left_on='path', right_on='path_audio', how='left')\n",
    "        \n",
    "        # Handle column naming issues after merge\n",
    "        if 'label' not in merged_df.columns:\n",
    "            if 'label_x' in merged_df.columns:\n",
    "                merged_df['label'] = merged_df['label_x']\n",
    "                merged_df = merged_df.drop(columns=['label_x'])\n",
    "            elif 'label_y' in merged_df.columns:\n",
    "                merged_df['label'] = merged_df['label_y']\n",
    "                merged_df = merged_df.drop(columns=['label_y'])\n",
    "            else:\n",
    "                raise ValueError(\"No label column found after merge\")\n",
    "        \n",
    "        # Clean up any remaining suffixed columns\n",
    "        cols_to_drop = []\n",
    "        for col in ['label_x', 'label_y', 'path_x', 'path_y']:\n",
    "            if col in merged_df.columns and col.replace('_x', '').replace('_y', '') in merged_df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "        if cols_to_drop:\n",
    "            merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "        \n",
    "        # Create predictions\n",
    "        merged_df['pred'] = merged_df.apply(\n",
    "            lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to process prediction file {score_file}: {str(e)}\")\n",
    "\n",
    "def print_results(model_results: Dict, model_name: str, original_results: Optional[Dict] = None) -> None:\n",
    "    \"\"\"Print evaluation results in a formatted way.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary containing model metrics\n",
    "        model_name: Name of the model\n",
    "        original_results: Optional dictionary containing original model results for comparison\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    print(f\"\\nTotal Samples: {model_results['total_samples']}\")\n",
    "    print(f\"Overall Accuracy: {model_results['overall']:.2f}%\")\n",
    "    print(f\"Overall EER: {model_results['overall_eer']:.2f}%\")\n",
    "    \n",
    "    if 'groups' in model_results:\n",
    "        print(\"\\nMetrics by group:\")\n",
    "        for group, metrics in model_results['groups'].items():\n",
    "            print(f\"  {group}:\")\n",
    "            print(f\"    Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "            print(f\"    EER: {metrics['eer']:.2f}%\")\n",
    "            print(f\"    Threshold: {metrics['threshold']:.4f}\")\n",
    "            print(f\"    Total Samples: {metrics['samples']}\")\n",
    "            print(f\"    Bona fide Samples: {metrics['bona_samples']}\")\n",
    "            print(f\"    Spoof Samples: {metrics['spoof_samples']}\")\n",
    "    \n",
    "    print(\"\\nAdditional metrics:\")\n",
    "    print(f\"  F1 Score: {model_results['f1']:.4f}\")\n",
    "    print(f\"  Recall: {model_results['recall']:.4f}\")\n",
    "    print(f\"  Precision: {model_results['precision']:.4f}\")\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "def print_eer_csv(model_results: Dict, model_name: str):\n",
    "    if 'groups' not in model_results:\n",
    "        print(\"No group EERs to print.\")\n",
    "        return\n",
    "\n",
    "    # Collect attack types and EERs\n",
    "    attack_types = sorted([k for k in model_results['groups'].keys() if k != '-'])\n",
    "    eers = [model_results['groups'][atk]['eer'] for atk in attack_types]\n",
    "\n",
    "    # Calculate pooled EER (mean of all attack type EERs)\n",
    "    pooled_eer = np.mean(eers) if eers else 0.0\n",
    "\n",
    "    # Print header\n",
    "    print(','.join(attack_types + ['Pooled EER']))\n",
    "    # Print values\n",
    "    print(','.join([f\"{eer:.2f}\" for eer in eers] + [f\"{pooled_eer:.2f}\"]))\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to run the evaluation pipeline.\"\"\"\n",
    "    try:\n",
    "        print(\"Loading metadata...\")\n",
    "        metadata_df = load_metadata()\n",
    "        print(f\"Loaded metadata with shape: {metadata_df.shape}\")\n",
    "        print(f\"Columns: {metadata_df.columns.tolist()}\")\n",
    "        \n",
    "        prediction_files = [PREDICTION_FILE]\n",
    "        prediction_files = sorted(prediction_files)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for score_file in prediction_files:\n",
    "            model_name = Path(score_file).name\n",
    "            print(f\"\\nProcessing {model_name}...\")\n",
    "            \n",
    "            results_df = process_prediction_file(score_file, metadata_df)\n",
    "            print(f\"Processed results shape: {results_df.shape}\")\n",
    "            print(f\"Required columns present: {all(col in results_df.columns for col in ['label', 'pred', 'score'])}\")\n",
    "            \n",
    "            metrics = MetricsCalculator.calculate_metrics(results_df, group_column='modification')\n",
    "            all_results[model_name] = metrics\n",
    "        \n",
    "        for model_name, metrics in all_results.items():\n",
    "            print_results(metrics, model_name)\n",
    "            print_eer_csv(metrics, model_name)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ecbc3",
   "metadata": {},
   "source": [
    "# MLAAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1ca5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Loaded metadata with shape: (7200, 11)\n",
      "Columns: ['path', 'subset', 'label', 'hash', 'path_audio', 'carrier', 'carrier_fine', 'modification', 'desc', 'target', 'partition']\n",
      "\n",
      "Processing mlaad_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g4_june27.txt...\n",
      "Processed results shape: (3600, 15)\n",
      "Required columns present: True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model: mlaad_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g4_june27.txt\n",
      "\n",
      "Total Samples: 3600\n",
      "Overall Accuracy: 78.17%\n",
      "Overall EER: 20.44%\n",
      "\n",
      "Metrics by group:\n",
      "  ADD_BACKGROUND_MUSIC:\n",
      "    Accuracy: 65.50%\n",
      "    EER: 29.00%\n",
      "    Threshold: -1.4083\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  ADD_BACKGROUND_NOISE:\n",
      "    Accuracy: 79.50%\n",
      "    EER: 35.00%\n",
      "    Threshold: -2.7769\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  AMPLITUDE_MODULATION:\n",
      "    Accuracy: 88.50%\n",
      "    EER: 15.00%\n",
      "    Threshold: -4.6509\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  AUTOTUNE:\n",
      "    Accuracy: 74.50%\n",
      "    EER: 7.00%\n",
      "    Threshold: -4.9625\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  BASELINE:\n",
      "    Accuracy: 98.00%\n",
      "    EER: 14.00%\n",
      "    Threshold: -4.5043\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  BIT_DEPTH:\n",
      "    Accuracy: 82.50%\n",
      "    EER: 13.00%\n",
      "    Threshold: -4.7795\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  ECHO:\n",
      "    Accuracy: 53.00%\n",
      "    EER: 10.00%\n",
      "    Threshold: -5.0457\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  EQUALIZATION:\n",
      "    Accuracy: 96.50%\n",
      "    EER: 13.00%\n",
      "    Threshold: -4.1938\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  FREQ_MINUS:\n",
      "    Accuracy: 83.50%\n",
      "    EER: 21.00%\n",
      "    Threshold: -3.5643\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  FREQ_PLUS:\n",
      "    Accuracy: 76.00%\n",
      "    EER: 21.00%\n",
      "    Threshold: -3.6025\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  GAUSSIAN_NOISE:\n",
      "    Accuracy: 54.50%\n",
      "    EER: 39.00%\n",
      "    Threshold: -1.0068\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  HIGH_PASS_FILTER:\n",
      "    Accuracy: 67.50%\n",
      "    EER: 32.00%\n",
      "    Threshold: -1.6436\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  LOW_PASS_FILTER:\n",
      "    Accuracy: 92.50%\n",
      "    EER: 17.00%\n",
      "    Threshold: -3.8302\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  MP3_COMPRESSION:\n",
      "    Accuracy: 91.00%\n",
      "    EER: 25.00%\n",
      "    Threshold: -4.1692\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  PITCH_SHIFT:\n",
      "    Accuracy: 56.00%\n",
      "    EER: 18.00%\n",
      "    Threshold: -4.2049\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  REVERB:\n",
      "    Accuracy: 97.50%\n",
      "    EER: 15.00%\n",
      "    Threshold: -4.5520\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  SILENCE_INJECTION:\n",
      "    Accuracy: 98.50%\n",
      "    EER: 18.00%\n",
      "    Threshold: -4.6352\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "  TIME_STRETCH:\n",
      "    Accuracy: 52.00%\n",
      "    EER: 14.00%\n",
      "    Threshold: -4.6560\n",
      "    Total Samples: 200\n",
      "    Bona fide Samples: 100\n",
      "    Spoof Samples: 100\n",
      "\n",
      "Additional metrics:\n",
      "  F1 Score: 0.7421\n",
      "  Recall: 0.6283\n",
      "  Precision: 0.9062\n",
      "----------------------------------------------------------------------\n",
      "ADD_BACKGROUND_MUSIC,ADD_BACKGROUND_NOISE,AMPLITUDE_MODULATION,AUTOTUNE,BASELINE,BIT_DEPTH,ECHO,EQUALIZATION,FREQ_MINUS,FREQ_PLUS,GAUSSIAN_NOISE,HIGH_PASS_FILTER,LOW_PASS_FILTER,MP3_COMPRESSION,PITCH_SHIFT,REVERB,SILENCE_INJECTION,TIME_STRETCH,Pooled EER\n",
      "29.00,35.00,15.00,7.00,14.00,13.00,10.00,13.00,21.00,21.00,39.00,32.00,17.00,25.00,18.00,15.00,18.00,14.00,19.78\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Dict, Optional, List, Union, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "from pathlib import Path\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "# Constants\n",
    "METADATA_PATH = \"/nvme1/hungdx/Lightning-hydra/data/shortcutASV/mlaad/protocol.txt\"\n",
    "\n",
    "META_CSV_PATH = \"/nvme1/hungdx/Lightning-hydra/data/shortcutASV/mlaad/data.csv\"\n",
    "\n",
    "# old prediction file\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/Conformer_MDT_DEC2024_correct/mlaad_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_lora_infer_Conformer_MDT_DEC2024_correct.txt\"\n",
    "\n",
    "# new g1-june26\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_june26/mlaad_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_june26.txt\"\n",
    "\n",
    "# new g2-june26\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g2_june26/mlaad_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g2_june26.txt\"\n",
    "\n",
    "# new g1-june27\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_june27/mlaad_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_june27.txt\"\n",
    "\n",
    "# new g2-june27\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g2_june27/mlaad_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g2_june27.txt\"\n",
    "\n",
    "# new g3-june27\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g3_june27/mlaad_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g3_june27.txt\"\n",
    "\n",
    "# new g4-june27\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g4_june27/mlaad_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g4_june27.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MetricsCalculator:\n",
    "    @staticmethod\n",
    "    def calculate_metrics(df: pd.DataFrame, group_column: Optional[str] = None) -> Dict[str, Union[float, Dict[str, Union[float, int]]]]:\n",
    "        \"\"\"Calculate various metrics for the given DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing predictions and ground truth\n",
    "            group_column: Optional column name to group results by\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing calculated metrics and sample counts\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'overall': accuracy_score(df[\"label\"], df[\"pred\"]) * 100,\n",
    "            'f1': f1_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'recall': recall_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'precision': precision_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'total_samples': len(df)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall EER\n",
    "        bona_cm = df[df['label'] == 'bonafide']['score'].values\n",
    "        spoof_cm = df[df['label'] == 'spoof']['score'].values\n",
    "        eer_cm, _ = em.compute_eer(bona_cm, spoof_cm)\n",
    "        results['overall_eer'] = eer_cm * 100\n",
    "        \n",
    "        if group_column and group_column in df.columns:\n",
    "            group_metrics = {}\n",
    "            \n",
    "            # Get all bona fide samples\n",
    "            all_bona = df[df['label'] == 'bonafide']\n",
    "            \n",
    "            # Get all spoof samples\n",
    "            all_spoof = df[df['label'] == 'spoof']\n",
    "            \n",
    "            # Process each attack type\n",
    "            for group, group_df in df.groupby(group_column):\n",
    "                if group == '-':  # For bona fide samples\n",
    "                    # Use all spoof samples and all bona fide samples\n",
    "                    group_bona = all_bona['score'].values\n",
    "                    group_spoof = all_spoof['score'].values\n",
    "                else:  # For spoof attack types\n",
    "                    # Get spoof samples for this attack type\n",
    "                    group_spoof = group_df[group_df['label'] == 'spoof']['score'].values\n",
    "                    # Randomly select equal number of bona fide samples\n",
    "                    n_spoof = len(group_spoof)\n",
    "                    group_bona = np.random.choice(all_bona['score'].values, size=n_spoof, replace=False)\n",
    "                \n",
    "                # Calculate EER for this group\n",
    "                group_eer, group_threshold = em.compute_eer(group_bona, group_spoof)\n",
    "                \n",
    "                group_metrics[group] = {\n",
    "                    'accuracy': accuracy_score(group_df[\"label\"], group_df[\"pred\"]) * 100,\n",
    "                    'eer': group_eer * 100,\n",
    "                    'threshold': group_threshold,\n",
    "                    'samples': len(group_df),\n",
    "                    'bona_samples': len(group_bona),\n",
    "                    'spoof_samples': len(group_spoof)\n",
    "                }\n",
    "            results['groups'] = group_metrics\n",
    "        \n",
    "        return results\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    \"\"\"Load and process metadata files with proper column handling.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing merged metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load protocol file\n",
    "        metadata = pd.read_csv(METADATA_PATH, sep=\" \", header=None)\n",
    "        metadata.columns = [\"path\", \"subset\", \"label\"]\n",
    "        \n",
    "        # Load CSV metadata\n",
    "        meta_csv = pd.read_csv(META_CSV_PATH, sep=\"|\")\n",
    "        # rename columns from file_path to path_audio\n",
    "        \n",
    "        meta_csv = meta_csv.rename(columns={'file_path': 'path_audio'})\n",
    "\n",
    "        \n",
    "        # Handle column conflicts before merge by dropping conflicting columns from meta_csv\n",
    "        conflicting_cols = ['subset', 'label', 'path']\n",
    "        for col in conflicting_cols:\n",
    "            if col in meta_csv.columns:\n",
    "                print(f\"Dropping conflicting column '{col}' from CSV metadata\")\n",
    "                meta_csv = meta_csv.drop(columns=[col])\n",
    "        \n",
    "        # Check if 'path_audio' column exists\n",
    "        if 'path_audio' not in meta_csv.columns:\n",
    "            # Try to find similar column names\n",
    "            audio_cols = [col for col in meta_csv.columns if 'path' in col.lower() or 'audio' in col.lower()]\n",
    "            if audio_cols:\n",
    "                meta_csv['path_audio'] = meta_csv[audio_cols[0]]\n",
    "            else:\n",
    "                raise ValueError(\"No suitable audio path column found in CSV metadata\")\n",
    "        \n",
    "        # Perform merge\n",
    "        merged_metadata = metadata.merge(meta_csv, left_on='path', right_on='path_audio', how='left')\n",
    "        \n",
    "        # Clean up any remaining suffixed columns (shouldn't happen now, but just in case)\n",
    "        if 'label' not in merged_metadata.columns:\n",
    "            if 'label_x' in merged_metadata.columns:\n",
    "                merged_metadata['label'] = merged_metadata['label_x']\n",
    "                merged_metadata = merged_metadata.drop(columns=['label_x'])\n",
    "            if 'label_y' in merged_metadata.columns:\n",
    "                merged_metadata = merged_metadata.drop(columns=['label_y'])\n",
    "        \n",
    "        if 'subset' not in merged_metadata.columns:\n",
    "            if 'subset_x' in merged_metadata.columns:\n",
    "                merged_metadata['subset'] = merged_metadata['subset_x']\n",
    "                merged_metadata = merged_metadata.drop(columns=['subset_x'])\n",
    "            if 'subset_y' in merged_metadata.columns:\n",
    "                merged_metadata = merged_metadata.drop(columns=['subset_y'])\n",
    "        \n",
    "        # Fix path column if it got suffixed\n",
    "        if 'path' not in merged_metadata.columns and 'path_x' in merged_metadata.columns:\n",
    "            merged_metadata['path'] = merged_metadata['path_x']\n",
    "            merged_metadata = merged_metadata.drop(columns=['path_x'])\n",
    "        if 'path_y' in merged_metadata.columns:\n",
    "            merged_metadata = merged_metadata.drop(columns=['path_y'])\n",
    "        \n",
    "        return merged_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load metadata: {str(e)}\")\n",
    "\n",
    "def process_prediction_file(score_file: str, metadata_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process a single prediction file and return results DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        score_file: Path to the prediction file\n",
    "        metadata_df: DataFrame containing metadata\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing processed predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "        pred_df.columns = [\"path\", \"spoof\", \"score\"]\n",
    "        pred_df = pred_df.drop_duplicates(subset=['path'])\n",
    "        \n",
    "        \n",
    "        # Remove all utterances starting with \"Spoof/a09\" to \"Spoof/a13\"\n",
    "        # This is to exclude specific utterances from the evaluation\n",
    "        # Adjust the condition based on your dataset structure\n",
    "        #pred_df = pred_df[~pred_df[\"path\"].str.startswith((\"Spoof/a09\", \"Spoof/a10\", \"Spoof/a11\", \"Spoof/a12\", \"Spoof/a13\"))]\n",
    "        \n",
    "        #### \n",
    "        \n",
    "        # Perform merge\n",
    "        merged_df = pred_df.merge(metadata_df, left_on='path', right_on='path_audio', how='left')\n",
    "        \n",
    "        # Handle column naming issues after merge\n",
    "        if 'label' not in merged_df.columns:\n",
    "            if 'label_x' in merged_df.columns:\n",
    "                merged_df['label'] = merged_df['label_x']\n",
    "                merged_df = merged_df.drop(columns=['label_x'])\n",
    "            elif 'label_y' in merged_df.columns:\n",
    "                merged_df['label'] = merged_df['label_y']\n",
    "                merged_df = merged_df.drop(columns=['label_y'])\n",
    "            else:\n",
    "                raise ValueError(\"No label column found after merge\")\n",
    "        \n",
    "        # Clean up any remaining suffixed columns\n",
    "        cols_to_drop = []\n",
    "        for col in ['label_x', 'label_y', 'path_x', 'path_y']:\n",
    "            if col in merged_df.columns and col.replace('_x', '').replace('_y', '') in merged_df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "        if cols_to_drop:\n",
    "            merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "        \n",
    "        # Create predictions\n",
    "        merged_df['pred'] = merged_df.apply(\n",
    "            lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to process prediction file {score_file}: {str(e)}\")\n",
    "\n",
    "def print_results(model_results: Dict, model_name: str, original_results: Optional[Dict] = None) -> None:\n",
    "    \"\"\"Print evaluation results in a formatted way.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary containing model metrics\n",
    "        model_name: Name of the model\n",
    "        original_results: Optional dictionary containing original model results for comparison\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    print(f\"\\nTotal Samples: {model_results['total_samples']}\")\n",
    "    print(f\"Overall Accuracy: {model_results['overall']:.2f}%\")\n",
    "    print(f\"Overall EER: {model_results['overall_eer']:.2f}%\")\n",
    "    \n",
    "    if 'groups' in model_results:\n",
    "        print(\"\\nMetrics by group:\")\n",
    "        for group, metrics in model_results['groups'].items():\n",
    "            print(f\"  {group}:\")\n",
    "            print(f\"    Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "            print(f\"    EER: {metrics['eer']:.2f}%\")\n",
    "            print(f\"    Threshold: {metrics['threshold']:.4f}\")\n",
    "            print(f\"    Total Samples: {metrics['samples']}\")\n",
    "            print(f\"    Bona fide Samples: {metrics['bona_samples']}\")\n",
    "            print(f\"    Spoof Samples: {metrics['spoof_samples']}\")\n",
    "    \n",
    "    print(\"\\nAdditional metrics:\")\n",
    "    print(f\"  F1 Score: {model_results['f1']:.4f}\")\n",
    "    print(f\"  Recall: {model_results['recall']:.4f}\")\n",
    "    print(f\"  Precision: {model_results['precision']:.4f}\")\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "def print_eer_csv(model_results: Dict, model_name: str):\n",
    "    if 'groups' not in model_results:\n",
    "        print(\"No group EERs to print.\")\n",
    "        return\n",
    "\n",
    "    # Collect attack types and EERs\n",
    "    attack_types = sorted([k for k in model_results['groups'].keys() if k != '-'])\n",
    "    eers = [model_results['groups'][atk]['eer'] for atk in attack_types]\n",
    "\n",
    "    # Calculate pooled EER (mean of all attack type EERs)\n",
    "    pooled_eer = np.mean(eers) if eers else 0.0\n",
    "\n",
    "    # Print header\n",
    "    print(','.join(attack_types + ['Pooled EER']))\n",
    "    # Print values\n",
    "    print(','.join([f\"{eer:.2f}\" for eer in eers] + [f\"{pooled_eer:.2f}\"]))\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to run the evaluation pipeline.\"\"\"\n",
    "    try:\n",
    "        print(\"Loading metadata...\")\n",
    "        metadata_df = load_metadata()\n",
    "        print(f\"Loaded metadata with shape: {metadata_df.shape}\")\n",
    "        print(f\"Columns: {metadata_df.columns.tolist()}\")\n",
    "        \n",
    "        prediction_files = [PREDICTION_FILE]\n",
    "        prediction_files = sorted(prediction_files)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for score_file in prediction_files:\n",
    "            model_name = Path(score_file).name\n",
    "            print(f\"\\nProcessing {model_name}...\")\n",
    "            \n",
    "            results_df = process_prediction_file(score_file, metadata_df)\n",
    "            print(f\"Processed results shape: {results_df.shape}\")\n",
    "            print(f\"Required columns present: {all(col in results_df.columns for col in ['label', 'pred', 'score'])}\")\n",
    "            \n",
    "            metrics = MetricsCalculator.calculate_metrics(results_df, group_column='modification')\n",
    "            all_results[model_name] = metrics\n",
    "        \n",
    "        for model_name, metrics in all_results.items():\n",
    "            print_results(metrics, model_name)\n",
    "            print_eer_csv(metrics, model_name)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b6f9e",
   "metadata": {},
   "source": [
    "# Merged sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c34bddac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Loaded metadata with shape: (7200, 11)\n",
      "Columns: ['path', 'subset', 'label', 'hash', 'path_audio', 'carrier', 'carrier_fine', 'modification', 'desc', 'target', 'partition']\n",
      "\n",
      "Processing merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_lora_infer_Conformer_MDT_DEC2024_correct.txt...\n",
      "Processed results shape: (7200, 15)\n",
      "Required columns present: True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model: merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_lora_infer_Conformer_MDT_DEC2024_correct.txt\n",
      "\n",
      "Total Samples: 7200\n",
      "Overall Accuracy: 74.33%\n",
      "Overall EER: 21.78%\n",
      "\n",
      "Metrics by group:\n",
      "  ADD_BACKGROUND_MUSIC:\n",
      "    Accuracy: 73.00%\n",
      "    EER: 34.50%\n",
      "    Threshold: -1.5457\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  ADD_BACKGROUND_NOISE:\n",
      "    Accuracy: 80.50%\n",
      "    EER: 26.50%\n",
      "    Threshold: -1.9593\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  AMPLITUDE_MODULATION:\n",
      "    Accuracy: 74.50%\n",
      "    EER: 13.00%\n",
      "    Threshold: -4.4909\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  AUTOTUNE:\n",
      "    Accuracy: 61.75%\n",
      "    EER: 7.00%\n",
      "    Threshold: -4.8355\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  BASELINE:\n",
      "    Accuracy: 89.75%\n",
      "    EER: 10.00%\n",
      "    Threshold: -4.4504\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  BIT_DEPTH:\n",
      "    Accuracy: 75.25%\n",
      "    EER: 10.50%\n",
      "    Threshold: -4.5040\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  ECHO:\n",
      "    Accuracy: 51.00%\n",
      "    EER: 8.50%\n",
      "    Threshold: -4.7648\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  EQUALIZATION:\n",
      "    Accuracy: 89.75%\n",
      "    EER: 12.50%\n",
      "    Threshold: -4.5163\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  FREQ_MINUS:\n",
      "    Accuracy: 78.50%\n",
      "    EER: 21.50%\n",
      "    Threshold: -3.0357\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  FREQ_PLUS:\n",
      "    Accuracy: 80.00%\n",
      "    EER: 24.50%\n",
      "    Threshold: -3.8410\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  GAUSSIAN_NOISE:\n",
      "    Accuracy: 63.75%\n",
      "    EER: 47.00%\n",
      "    Threshold: -0.2989\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  HIGH_PASS_FILTER:\n",
      "    Accuracy: 74.50%\n",
      "    EER: 35.00%\n",
      "    Threshold: -1.9389\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  LOW_PASS_FILTER:\n",
      "    Accuracy: 84.25%\n",
      "    EER: 18.00%\n",
      "    Threshold: -4.1389\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  MP3_COMPRESSION:\n",
      "    Accuracy: 78.25%\n",
      "    EER: 17.00%\n",
      "    Threshold: -4.0687\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  PITCH_SHIFT:\n",
      "    Accuracy: 54.75%\n",
      "    EER: 20.00%\n",
      "    Threshold: -4.0090\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  REVERB:\n",
      "    Accuracy: 91.00%\n",
      "    EER: 10.50%\n",
      "    Threshold: -4.3883\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  SILENCE_INJECTION:\n",
      "    Accuracy: 86.00%\n",
      "    EER: 13.00%\n",
      "    Threshold: -4.5195\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "  TIME_STRETCH:\n",
      "    Accuracy: 51.50%\n",
      "    EER: 15.50%\n",
      "    Threshold: -4.2349\n",
      "    Total Samples: 400\n",
      "    Bona fide Samples: 200\n",
      "    Spoof Samples: 200\n",
      "\n",
      "Additional metrics:\n",
      "  F1 Score: 0.6856\n",
      "  Recall: 0.5597\n",
      "  Precision: 0.8845\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "EER CSV:\n",
      "ADD_BACKGROUND_MUSIC,ADD_BACKGROUND_NOISE,AMPLITUDE_MODULATION,AUTOTUNE,BASELINE,BIT_DEPTH,ECHO,EQUALIZATION,FREQ_MINUS,FREQ_PLUS,GAUSSIAN_NOISE,HIGH_PASS_FILTER,LOW_PASS_FILTER,MP3_COMPRESSION,PITCH_SHIFT,REVERB,SILENCE_INJECTION,TIME_STRETCH,Pooled EER\n",
      "34.50,26.50,13.00,7.00,10.00,10.50,8.50,12.50,21.50,24.50,47.00,35.00,18.00,17.00,20.00,10.50,13.00,15.50,19.14\n",
      "\n",
      "Accuracy CSV:\n",
      "ADD_BACKGROUND_MUSIC,ADD_BACKGROUND_NOISE,AMPLITUDE_MODULATION,AUTOTUNE,BASELINE,BIT_DEPTH,ECHO,EQUALIZATION,FREQ_MINUS,FREQ_PLUS,GAUSSIAN_NOISE,HIGH_PASS_FILTER,LOW_PASS_FILTER,MP3_COMPRESSION,PITCH_SHIFT,REVERB,SILENCE_INJECTION,TIME_STRETCH,Pooled Accuracy\n",
      "73.00,80.50,74.50,61.75,89.75,75.25,51.00,89.75,78.50,80.00,63.75,74.50,84.25,78.25,54.75,91.00,86.00,51.50,74.33\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Dict, Optional, List, Union, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "from pathlib import Path\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "# Constants\n",
    "METADATA_PATH = \"/nvme1/hungdx/Lightning-hydra/data/shortcutASV/merged_protocol.txt\"\n",
    "\n",
    "META_CSV_PATH = \"/nvme1/hungdx/Lightning-hydra/data/shortcutASV/merged_meta.csv\"\n",
    "\n",
    "# old prediction file\n",
    "PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/Conformer_MDT_DEC2024_correct/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_lora_infer_Conformer_MDT_DEC2024_correct.txt\"\n",
    "\n",
    "# Conformer_MDT_LA19\n",
    "#PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/Conformer_MDT_LA19/merged_scores_huggingface_benchmark_xlsr_conformertcm_mdt_lora_infer_Conformer_MDT_LA19.txt\"\n",
    "\n",
    "# AASIST_SSL_MDT_LA19\n",
    "#PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/AASIST_SSL_MDT_LA19/merged_scores_huggingface_benchmark_xlsr_aasist_mdt_paper_AASIST_SSL_MDT_LA19.txt\"\n",
    "\n",
    "# ToP_April\n",
    "#PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ToP_April/merged_scores_cnsl_xlsr_vib_large_corpus_ToP_April.txt\"\n",
    "\n",
    "\n",
    "\n",
    "# new g1-june27\n",
    "#PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_june27/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_june27.txt\"\n",
    "\n",
    "# new g2-june27\n",
    "#PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g2_june27/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g2_june27.txt\"\n",
    "\n",
    "# new g3-june27\n",
    "#PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g3_june27/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g3_june27.txt\"\n",
    "\n",
    "# new g4-june27\n",
    "#PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g4_june27/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g4_june27.txt\"\n",
    "\n",
    "# new g1-june29\n",
    "#PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_june29/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_june29.txt\"\n",
    "\n",
    "# new g5-june29\n",
    "#PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g5_june29/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g5_june29.txt\"\n",
    "\n",
    "# train-with-all-june30\n",
    "#PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_train-with-all_june30/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_train-with-all_june30.txt\"\n",
    "\n",
    "\n",
    "\n",
    "class MetricsCalculator:\n",
    "    @staticmethod\n",
    "    def calculate_metrics(df: pd.DataFrame, group_column: Optional[str] = None) -> Dict[str, Union[float, Dict[str, Union[float, int]]]]:\n",
    "        \"\"\"Calculate various metrics for the given DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing predictions and ground truth\n",
    "            group_column: Optional column name to group results by\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing calculated metrics and sample counts\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'overall': accuracy_score(df[\"label\"], df[\"pred\"]) * 100,\n",
    "            'f1': f1_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'recall': recall_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'precision': precision_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'total_samples': len(df)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall EER\n",
    "        bona_cm = df[df['label'] == 'bonafide']['score'].values\n",
    "        spoof_cm = df[df['label'] == 'spoof']['score'].values\n",
    "        eer_cm, _ = em.compute_eer(bona_cm, spoof_cm)\n",
    "        results['overall_eer'] = eer_cm * 100\n",
    "        \n",
    "        if group_column and group_column in df.columns:\n",
    "            group_metrics = {}\n",
    "            \n",
    "            # Get all bona fide samples\n",
    "            all_bona = df[df['label'] == 'bonafide']\n",
    "            \n",
    "            # Get all spoof samples\n",
    "            all_spoof = df[df['label'] == 'spoof']\n",
    "            \n",
    "            # Process each attack type\n",
    "            for group, group_df in df.groupby(group_column):\n",
    "                if group == '-':  # For bona fide samples\n",
    "                    # Use all spoof samples and all bona fide samples\n",
    "                    group_bona = all_bona['score'].values\n",
    "                    group_spoof = all_spoof['score'].values\n",
    "                else:  # For spoof attack types\n",
    "                    # Get spoof samples for this attack type\n",
    "                    group_spoof = group_df[group_df['label'] == 'spoof']['score'].values\n",
    "                    # Randomly select equal number of bona fide samples\n",
    "                    n_spoof = len(group_spoof)\n",
    "                    group_bona = np.random.choice(all_bona['score'].values, size=n_spoof, replace=False)\n",
    "                \n",
    "                # Calculate EER for this group\n",
    "                group_eer, group_threshold = em.compute_eer(group_bona, group_spoof)\n",
    "                \n",
    "                group_metrics[group] = {\n",
    "                    'accuracy': accuracy_score(group_df[\"label\"], group_df[\"pred\"]) * 100,\n",
    "                    'eer': group_eer * 100,\n",
    "                    'threshold': group_threshold,\n",
    "                    'samples': len(group_df),\n",
    "                    'bona_samples': len(group_bona),\n",
    "                    'spoof_samples': len(group_spoof)\n",
    "                }\n",
    "            results['groups'] = group_metrics\n",
    "        \n",
    "        return results\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    \"\"\"Load and process metadata files with proper column handling.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing merged metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load protocol file\n",
    "        metadata = pd.read_csv(METADATA_PATH, sep=\" \", header=None)\n",
    "        metadata.columns = [\"path\", \"subset\", \"label\"]\n",
    "        \n",
    "        # Load CSV metadata\n",
    "        meta_csv = pd.read_csv(META_CSV_PATH, sep=\"|\")\n",
    "        # rename columns from file_path to path_audio\n",
    "        \n",
    "        meta_csv = meta_csv.rename(columns={'file_path': 'path_audio'})\n",
    "\n",
    "        \n",
    "        # Handle column conflicts before merge by dropping conflicting columns from meta_csv\n",
    "        conflicting_cols = ['subset', 'label', 'path']\n",
    "        for col in conflicting_cols:\n",
    "            if col in meta_csv.columns:\n",
    "                print(f\"Dropping conflicting column '{col}' from CSV metadata\")\n",
    "                meta_csv = meta_csv.drop(columns=[col])\n",
    "        \n",
    "        # Check if 'path_audio' column exists\n",
    "        if 'path_audio' not in meta_csv.columns:\n",
    "            # Try to find similar column names\n",
    "            audio_cols = [col for col in meta_csv.columns if 'path' in col.lower() or 'audio' in col.lower()]\n",
    "            if audio_cols:\n",
    "                meta_csv['path_audio'] = meta_csv[audio_cols[0]]\n",
    "            else:\n",
    "                raise ValueError(\"No suitable audio path column found in CSV metadata\")\n",
    "        \n",
    "        # Perform merge\n",
    "        merged_metadata = metadata.merge(meta_csv, left_on='path', right_on='path_audio', how='left')\n",
    "        \n",
    "        # Clean up any remaining suffixed columns (shouldn't happen now, but just in case)\n",
    "        if 'label' not in merged_metadata.columns:\n",
    "            if 'label_x' in merged_metadata.columns:\n",
    "                merged_metadata['label'] = merged_metadata['label_x']\n",
    "                merged_metadata = merged_metadata.drop(columns=['label_x'])\n",
    "            if 'label_y' in merged_metadata.columns:\n",
    "                merged_metadata = merged_metadata.drop(columns=['label_y'])\n",
    "        \n",
    "        if 'subset' not in merged_metadata.columns:\n",
    "            if 'subset_x' in merged_metadata.columns:\n",
    "                merged_metadata['subset'] = merged_metadata['subset_x']\n",
    "                merged_metadata = merged_metadata.drop(columns=['subset_x'])\n",
    "            if 'subset_y' in merged_metadata.columns:\n",
    "                merged_metadata = merged_metadata.drop(columns=['subset_y'])\n",
    "        \n",
    "        # Fix path column if it got suffixed\n",
    "        if 'path' not in merged_metadata.columns and 'path_x' in merged_metadata.columns:\n",
    "            merged_metadata['path'] = merged_metadata['path_x']\n",
    "            merged_metadata = merged_metadata.drop(columns=['path_x'])\n",
    "        if 'path_y' in merged_metadata.columns:\n",
    "            merged_metadata = merged_metadata.drop(columns=['path_y'])\n",
    "        \n",
    "        return merged_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load metadata: {str(e)}\")\n",
    "\n",
    "def process_prediction_file(score_file: str, metadata_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process a single prediction file and return results DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        score_file: Path to the prediction file\n",
    "        metadata_df: DataFrame containing metadata\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing processed predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "        pred_df.columns = [\"path\", \"spoof\", \"score\"]\n",
    "        pred_df = pred_df.drop_duplicates(subset=['path'])\n",
    "        \n",
    "        \n",
    "        # Remove all utterances starting with \"Spoof/a09\" to \"Spoof/a13\"\n",
    "        # This is to exclude specific utterances from the evaluation\n",
    "        # Adjust the condition based on your dataset structure\n",
    "        #pred_df = pred_df[~pred_df[\"path\"].str.startswith((\"Spoof/a09\", \"Spoof/a10\", \"Spoof/a11\", \"Spoof/a12\", \"Spoof/a13\"))]\n",
    "        \n",
    "        #### \n",
    "        \n",
    "        # Perform merge\n",
    "        merged_df = pred_df.merge(metadata_df, left_on='path', right_on='path_audio', how='left')\n",
    "        \n",
    "        # Handle column naming issues after merge\n",
    "        if 'label' not in merged_df.columns:\n",
    "            if 'label_x' in merged_df.columns:\n",
    "                merged_df['label'] = merged_df['label_x']\n",
    "                merged_df = merged_df.drop(columns=['label_x'])\n",
    "            elif 'label_y' in merged_df.columns:\n",
    "                merged_df['label'] = merged_df['label_y']\n",
    "                merged_df = merged_df.drop(columns=['label_y'])\n",
    "            else:\n",
    "                raise ValueError(\"No label column found after merge\")\n",
    "        \n",
    "        # Clean up any remaining suffixed columns\n",
    "        cols_to_drop = []\n",
    "        for col in ['label_x', 'label_y', 'path_x', 'path_y']:\n",
    "            if col in merged_df.columns and col.replace('_x', '').replace('_y', '') in merged_df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "        if cols_to_drop:\n",
    "            merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "        \n",
    "        # Create predictions\n",
    "        merged_df['pred'] = merged_df.apply(\n",
    "            lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to process prediction file {score_file}: {str(e)}\")\n",
    "\n",
    "def print_results(model_results: Dict, model_name: str, original_results: Optional[Dict] = None) -> None:\n",
    "    \"\"\"Print evaluation results in a formatted way.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary containing model metrics\n",
    "        model_name: Name of the model\n",
    "        original_results: Optional dictionary containing original model results for comparison\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    print(f\"\\nTotal Samples: {model_results['total_samples']}\")\n",
    "    print(f\"Overall Accuracy: {model_results['overall']:.2f}%\")\n",
    "    print(f\"Overall EER: {model_results['overall_eer']:.2f}%\")\n",
    "    \n",
    "    if 'groups' in model_results:\n",
    "        print(\"\\nMetrics by group:\")\n",
    "        for group, metrics in model_results['groups'].items():\n",
    "            print(f\"  {group}:\")\n",
    "            print(f\"    Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "            print(f\"    EER: {metrics['eer']:.2f}%\")\n",
    "            print(f\"    Threshold: {metrics['threshold']:.4f}\")\n",
    "            print(f\"    Total Samples: {metrics['samples']}\")\n",
    "            print(f\"    Bona fide Samples: {metrics['bona_samples']}\")\n",
    "            print(f\"    Spoof Samples: {metrics['spoof_samples']}\")\n",
    "    \n",
    "    print(\"\\nAdditional metrics:\")\n",
    "    print(f\"  F1 Score: {model_results['f1']:.4f}\")\n",
    "    print(f\"  Recall: {model_results['recall']:.4f}\")\n",
    "    print(f\"  Precision: {model_results['precision']:.4f}\")\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "def print_eer_csv(model_results: Dict, model_name: str):\n",
    "    if 'groups' not in model_results:\n",
    "        print(\"No group EERs to print.\")\n",
    "        return\n",
    "\n",
    "    # Collect attack types and EERs\n",
    "    attack_types = sorted([k for k in model_results['groups'].keys() if k != '-'])\n",
    "    eers = [model_results['groups'][atk]['eer'] for atk in attack_types]\n",
    "\n",
    "    # Calculate pooled EER (mean of all attack type EERs)\n",
    "    pooled_eer = np.mean(eers) if eers else 0.0\n",
    "\n",
    "    # Print header\n",
    "    print(','.join(attack_types + ['Pooled EER']))\n",
    "    # Print values\n",
    "    print(','.join([f\"{eer:.2f}\" for eer in eers] + [f\"{pooled_eer:.2f}\"]))\n",
    "\n",
    "def print_accuracy_csv(model_results: Dict, model_name: str):\n",
    "    \"\"\"Print accuracy results in CSV format for each attack type.\"\"\"\n",
    "    if 'groups' not in model_results:\n",
    "        print(\"No group accuracies to print.\")\n",
    "        return\n",
    "\n",
    "    # Collect attack types and accuracies (excluding bonafide samples marked with '-')\n",
    "    attack_types = sorted([k for k in model_results['groups'].keys() if k != '-'])\n",
    "    accuracies = [model_results['groups'][atk]['accuracy'] for atk in attack_types]\n",
    "\n",
    "    # Calculate pooled accuracy (mean of all attack type accuracies)\n",
    "    pooled_accuracy = np.mean(accuracies) if accuracies else 0.0\n",
    "\n",
    "    # Print header\n",
    "    print(','.join(attack_types + ['Pooled Accuracy']))\n",
    "    # Print values\n",
    "    print(','.join([f\"{acc:.2f}\" for acc in accuracies] + [f\"{pooled_accuracy:.2f}\"]))\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to run the evaluation pipeline.\"\"\"\n",
    "    try:\n",
    "        print(\"Loading metadata...\")\n",
    "        metadata_df = load_metadata()\n",
    "        print(f\"Loaded metadata with shape: {metadata_df.shape}\")\n",
    "        print(f\"Columns: {metadata_df.columns.tolist()}\")\n",
    "        \n",
    "        prediction_files = [PREDICTION_FILE]\n",
    "        prediction_files = sorted(prediction_files)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for score_file in prediction_files:\n",
    "            model_name = Path(score_file).name\n",
    "            print(f\"\\nProcessing {model_name}...\")\n",
    "            \n",
    "            results_df = process_prediction_file(score_file, metadata_df)\n",
    "            print(f\"Processed results shape: {results_df.shape}\")\n",
    "            print(f\"Required columns present: {all(col in results_df.columns for col in ['label', 'pred', 'score'])}\")\n",
    "            \n",
    "            metrics = MetricsCalculator.calculate_metrics(results_df, group_column='modification')\n",
    "            all_results[model_name] = metrics\n",
    "        \n",
    "        for model_name, metrics in all_results.items():\n",
    "            print_results(metrics, model_name)\n",
    "            print(\"\\nEER CSV:\")\n",
    "            print_eer_csv(metrics, model_name)\n",
    "            print(\"\\nAccuracy CSV:\")\n",
    "            print_accuracy_csv(metrics, model_name)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499b49d",
   "metadata": {},
   "source": [
    "# Upgrade merged set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40f8149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER Calculation: Enabled\n",
      "Loading metadata...\n",
      "Protocol file loaded: 7200 entries\n",
      "Label distribution in protocol file:\n",
      "label\n",
      "spoof       3600\n",
      "bonafide    3600\n",
      "Name: count, dtype: int64\n",
      "Attack type distribution:\n",
      "attack_type\n",
      "BASELINE                400\n",
      "BIT_DEPTH               400\n",
      "MP3_COMPRESSION         400\n",
      "ADD_BACKGROUND_NOISE    400\n",
      "ADD_BACKGROUND_MUSIC    400\n",
      "EQUALIZATION            400\n",
      "FREQ_MINUS              400\n",
      "FREQ_PLUS               400\n",
      "GAUSSIAN_NOISE          400\n",
      "SILENCE_INJECTION       400\n",
      "HIGH_PASS_FILTER        400\n",
      "LOW_PASS_FILTER         400\n",
      "TIME_STRETCH            400\n",
      "PITCH_SHIFT             400\n",
      "ECHO                    400\n",
      "AMPLITUDE_MODULATION    400\n",
      "AUTOTUNE                400\n",
      "REVERB                  400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack type vs Label cross-tabulation:\n",
      "label                 bonafide  spoof   All\n",
      "attack_type                                \n",
      "ADD_BACKGROUND_MUSIC       200    200   400\n",
      "ADD_BACKGROUND_NOISE       200    200   400\n",
      "AMPLITUDE_MODULATION       200    200   400\n",
      "AUTOTUNE                   200    200   400\n",
      "BASELINE                   200    200   400\n",
      "BIT_DEPTH                  200    200   400\n",
      "ECHO                       200    200   400\n",
      "EQUALIZATION               200    200   400\n",
      "FREQ_MINUS                 200    200   400\n",
      "FREQ_PLUS                  200    200   400\n",
      "GAUSSIAN_NOISE             200    200   400\n",
      "HIGH_PASS_FILTER           200    200   400\n",
      "LOW_PASS_FILTER            200    200   400\n",
      "MP3_COMPRESSION            200    200   400\n",
      "PITCH_SHIFT                200    200   400\n",
      "REVERB                     200    200   400\n",
      "SILENCE_INJECTION          200    200   400\n",
      "TIME_STRETCH               200    200   400\n",
      "All                       3600   3600  7200\n",
      "CSV metadata file size: 3.2 MB\n",
      "CSV metadata loaded: 14400 entries\n",
      "CSV columns: ['hash', 'file_path', 'carrier', 'carrier_fine', 'modification', 'desc', 'target', 'partition']\n",
      "Merging protocol and CSV metadata...\n",
      "Merged metadata: 7200 entries\n",
      "Loaded metadata with shape: (7200, 12)\n",
      "Columns: ['path', 'subset', 'label', 'attack_type', 'hash', 'path_audio', 'carrier', 'carrier_fine', 'modification', 'desc', 'target', 'partition']\n",
      "\n",
      "Found 1 prediction files to process\n",
      "\n",
      "[1/1] Processing merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_infer_Conformer_MDT_DEC2024_correct.txt...\n",
      "Prediction file loaded: 7200 entries\n",
      "Merged prediction+metadata: 7200 entries\n",
      "Available columns: ['path_x', 'spoof', 'score', 'path_y', 'subset', 'label', 'attack_type', 'hash', 'path_audio', 'carrier', 'carrier_fine', 'modification', 'desc', 'target', 'partition']\n",
      "Final label distribution: {'spoof': 3600, 'bonafide': 3600}\n",
      "Attack type distribution: {'BASELINE': 400, 'BIT_DEPTH': 400, 'MP3_COMPRESSION': 400, 'ADD_BACKGROUND_NOISE': 400, 'ADD_BACKGROUND_MUSIC': 400, 'EQUALIZATION': 400, 'FREQ_MINUS': 400, 'FREQ_PLUS': 400, 'GAUSSIAN_NOISE': 400, 'SILENCE_INJECTION': 400, 'HIGH_PASS_FILTER': 400, 'LOW_PASS_FILTER': 400, 'TIME_STRETCH': 400, 'PITCH_SHIFT': 400, 'ECHO': 400, 'AMPLITUDE_MODULATION': 400, 'AUTOTUNE': 400, 'REVERB': 400}\n",
      "\n",
      "Attack type vs Label cross-tabulation:\n",
      "label                 bonafide  spoof   All\n",
      "attack_type                                \n",
      "ADD_BACKGROUND_MUSIC       200    200   400\n",
      "ADD_BACKGROUND_NOISE       200    200   400\n",
      "AMPLITUDE_MODULATION       200    200   400\n",
      "AUTOTUNE                   200    200   400\n",
      "BASELINE                   200    200   400\n",
      "BIT_DEPTH                  200    200   400\n",
      "ECHO                       200    200   400\n",
      "EQUALIZATION               200    200   400\n",
      "FREQ_MINUS                 200    200   400\n",
      "FREQ_PLUS                  200    200   400\n",
      "GAUSSIAN_NOISE             200    200   400\n",
      "HIGH_PASS_FILTER           200    200   400\n",
      "LOW_PASS_FILTER            200    200   400\n",
      "MP3_COMPRESSION            200    200   400\n",
      "PITCH_SHIFT                200    200   400\n",
      "REVERB                     200    200   400\n",
      "SILENCE_INJECTION          200    200   400\n",
      "TIME_STRETCH               200    200   400\n",
      "All                       3600   3600  7200\n",
      "Processed results shape: (7200, 16)\n",
      "Required columns present: True\n",
      "Unique labels found: ['bonafide', 'spoof']\n",
      "Available groups: ['ADD_BACKGROUND_MUSIC', 'ADD_BACKGROUND_NOISE', 'AMPLITUDE_MODULATION', 'AUTOTUNE', 'BASELINE', 'BIT_DEPTH', 'ECHO', 'EQUALIZATION', 'FREQ_MINUS', 'FREQ_PLUS', 'GAUSSIAN_NOISE', 'HIGH_PASS_FILTER', 'LOW_PASS_FILTER', 'MP3_COMPRESSION', 'PITCH_SHIFT', 'REVERB', 'SILENCE_INJECTION', 'TIME_STRETCH']\n",
      "\n",
      "Successfully processed 1 models\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model: merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_infer_Conformer_MDT_DEC2024_correct.txt\n",
      "\n",
      "Total Samples: 7200\n",
      "Overall Accuracy: 74.33%\n",
      "Overall EER: 21.78%\n",
      "\n",
      "BONAFIDE Accuracy by Attack Type:\n",
      "ADD_BACKGROUND_MUSIC,139,69.50\n",
      "ADD_BACKGROUND_NOISE,163,81.50\n",
      "AMPLITUDE_MODULATION,101,50.50\n",
      "AUTOTUNE,48,24.00\n",
      "BASELINE,162,81.00\n",
      "BIT_DEPTH,103,51.50\n",
      "ECHO,4,2.00\n",
      "EQUALIZATION,166,83.00\n",
      "FREQ_MINUS,133,66.50\n",
      "FREQ_PLUS,134,67.00\n",
      "GAUSSIAN_NOISE,137,68.50\n",
      "HIGH_PASS_FILTER,110,55.00\n",
      "LOW_PASS_FILTER,150,75.00\n",
      "MP3_COMPRESSION,124,62.00\n",
      "PITCH_SHIFT,19,9.50\n",
      "REVERB,168,84.00\n",
      "SILENCE_INJECTION,147,73.50\n",
      "TIME_STRETCH,7,3.50\n",
      "\n",
      "SPOOF Accuracy by Attack Type:\n",
      "ADD_BACKGROUND_MUSIC,153,76.50\n",
      "ADD_BACKGROUND_NOISE,159,79.50\n",
      "AMPLITUDE_MODULATION,197,98.50\n",
      "AUTOTUNE,199,99.50\n",
      "BASELINE,197,98.50\n",
      "BIT_DEPTH,198,99.00\n",
      "ECHO,200,100.00\n",
      "EQUALIZATION,193,96.50\n",
      "FREQ_MINUS,181,90.50\n",
      "FREQ_PLUS,186,93.00\n",
      "GAUSSIAN_NOISE,118,59.00\n",
      "HIGH_PASS_FILTER,188,94.00\n",
      "LOW_PASS_FILTER,187,93.50\n",
      "MP3_COMPRESSION,189,94.50\n",
      "PITCH_SHIFT,200,100.00\n",
      "REVERB,196,98.00\n",
      "SILENCE_INJECTION,197,98.50\n",
      "TIME_STRETCH,199,99.50\n",
      "\n",
      "Detailed Metrics by Attack Type:\n",
      "  ADD_BACKGROUND_MUSIC:\n",
      "    Bonafide: 139/200 (69.50%)\n",
      "    Spoof: 153/200 (76.50%)\n",
      "    Overall: 73.00%\n",
      "    EER: 27.50%\n",
      "    Threshold: -0.9512\n",
      "    Total Samples: 400\n",
      "  ADD_BACKGROUND_NOISE:\n",
      "    Bonafide: 163/200 (81.50%)\n",
      "    Spoof: 159/200 (79.50%)\n",
      "    Overall: 80.50%\n",
      "    EER: 18.50%\n",
      "    Threshold: -0.3856\n",
      "    Total Samples: 400\n",
      "  AMPLITUDE_MODULATION:\n",
      "    Bonafide: 101/200 (50.50%)\n",
      "    Spoof: 197/200 (98.50%)\n",
      "    Overall: 74.50%\n",
      "    EER: 12.50%\n",
      "    Threshold: -4.3012\n",
      "    Total Samples: 400\n",
      "  AUTOTUNE:\n",
      "    Bonafide: 48/200 (24.00%)\n",
      "    Spoof: 199/200 (99.50%)\n",
      "    Overall: 61.75%\n",
      "    EER: 26.50%\n",
      "    Threshold: -5.1223\n",
      "    Total Samples: 400\n",
      "  BASELINE:\n",
      "    Bonafide: 162/200 (81.00%)\n",
      "    Spoof: 197/200 (98.50%)\n",
      "    Overall: 89.75%\n",
      "    EER: 6.50%\n",
      "    Threshold: -3.5016\n",
      "    Total Samples: 400\n",
      "  BIT_DEPTH:\n",
      "    Bonafide: 103/200 (51.50%)\n",
      "    Spoof: 198/200 (99.00%)\n",
      "    Overall: 75.25%\n",
      "    EER: 8.50%\n",
      "    Threshold: -4.2732\n",
      "    Total Samples: 400\n",
      "  ECHO:\n",
      "    Bonafide: 4/200 (2.00%)\n",
      "    Spoof: 200/200 (100.00%)\n",
      "    Overall: 51.00%\n",
      "    EER: 48.50%\n",
      "    Threshold: -5.2153\n",
      "    Total Samples: 400\n",
      "  EQUALIZATION:\n",
      "    Bonafide: 166/200 (83.00%)\n",
      "    Spoof: 193/200 (96.50%)\n",
      "    Overall: 89.75%\n",
      "    EER: 5.50%\n",
      "    Threshold: -2.7111\n",
      "    Total Samples: 400\n",
      "  FREQ_MINUS:\n",
      "    Bonafide: 133/200 (66.50%)\n",
      "    Spoof: 181/200 (90.50%)\n",
      "    Overall: 78.50%\n",
      "    EER: 15.50%\n",
      "    Threshold: -2.3006\n",
      "    Total Samples: 400\n",
      "  FREQ_PLUS:\n",
      "    Bonafide: 134/200 (67.00%)\n",
      "    Spoof: 186/200 (93.00%)\n",
      "    Overall: 80.00%\n",
      "    EER: 14.00%\n",
      "    Threshold: -2.5006\n",
      "    Total Samples: 400\n",
      "  GAUSSIAN_NOISE:\n",
      "    Bonafide: 137/200 (68.50%)\n",
      "    Spoof: 118/200 (59.00%)\n",
      "    Overall: 63.75%\n",
      "    EER: 35.50%\n",
      "    Threshold: 0.2250\n",
      "    Total Samples: 400\n",
      "  HIGH_PASS_FILTER:\n",
      "    Bonafide: 110/200 (55.00%)\n",
      "    Spoof: 188/200 (94.00%)\n",
      "    Overall: 74.50%\n",
      "    EER: 21.00%\n",
      "    Threshold: -1.1093\n",
      "    Total Samples: 400\n",
      "  LOW_PASS_FILTER:\n",
      "    Bonafide: 150/200 (75.00%)\n",
      "    Spoof: 187/200 (93.50%)\n",
      "    Overall: 84.25%\n",
      "    EER: 12.50%\n",
      "    Threshold: -3.0265\n",
      "    Total Samples: 400\n",
      "  MP3_COMPRESSION:\n",
      "    Bonafide: 124/200 (62.00%)\n",
      "    Spoof: 189/200 (94.50%)\n",
      "    Overall: 78.25%\n",
      "    EER: 17.50%\n",
      "    Threshold: -4.0856\n",
      "    Total Samples: 400\n",
      "  PITCH_SHIFT:\n",
      "    Bonafide: 19/200 (9.50%)\n",
      "    Spoof: 200/200 (100.00%)\n",
      "    Overall: 54.75%\n",
      "    EER: 30.00%\n",
      "    Threshold: -4.4661\n",
      "    Total Samples: 400\n",
      "  REVERB:\n",
      "    Bonafide: 168/200 (84.00%)\n",
      "    Spoof: 196/200 (98.00%)\n",
      "    Overall: 91.00%\n",
      "    EER: 7.00%\n",
      "    Threshold: -3.5890\n",
      "    Total Samples: 400\n",
      "  SILENCE_INJECTION:\n",
      "    Bonafide: 147/200 (73.50%)\n",
      "    Spoof: 197/200 (98.50%)\n",
      "    Overall: 86.00%\n",
      "    EER: 6.50%\n",
      "    Threshold: -3.5397\n",
      "    Total Samples: 400\n",
      "  TIME_STRETCH:\n",
      "    Bonafide: 7/200 (3.50%)\n",
      "    Spoof: 199/200 (99.50%)\n",
      "    Overall: 51.50%\n",
      "    EER: 36.00%\n",
      "    Threshold: -4.6526\n",
      "    Total Samples: 400\n",
      "\n",
      "Additional overall metrics:\n",
      "  F1 Score: 0.6856\n",
      "  Recall: 0.5597\n",
      "  Precision: 0.8845\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "EER CSV:\n",
      "ADD_BACKGROUND_MUSIC,ADD_BACKGROUND_NOISE,AMPLITUDE_MODULATION,AUTOTUNE,BASELINE,BIT_DEPTH,ECHO,EQUALIZATION,FREQ_MINUS,FREQ_PLUS,GAUSSIAN_NOISE,HIGH_PASS_FILTER,LOW_PASS_FILTER,MP3_COMPRESSION,PITCH_SHIFT,REVERB,SILENCE_INJECTION,TIME_STRETCH,Pooled EER\n",
      "27.50,18.50,12.50,26.50,6.50,8.50,48.50,5.50,15.50,14.00,35.50,21.00,12.50,17.50,30.00,7.00,6.50,36.00,19.42\n",
      "\n",
      "Accuracy CSV:\n",
      "=== Bonafide Accuracy CSV ===\n",
      "ADD_BACKGROUND_MUSIC,ADD_BACKGROUND_NOISE,AMPLITUDE_MODULATION,AUTOTUNE,BASELINE,BIT_DEPTH,ECHO,EQUALIZATION,FREQ_MINUS,FREQ_PLUS,GAUSSIAN_NOISE,HIGH_PASS_FILTER,LOW_PASS_FILTER,MP3_COMPRESSION,PITCH_SHIFT,REVERB,SILENCE_INJECTION,TIME_STRETCH,Pooled Bonafide\n",
      "69.50,81.50,50.50,24.00,81.00,51.50,2.00,83.00,66.50,67.00,68.50,55.00,75.00,62.00,9.50,84.00,73.50,3.50,55.97\n",
      "\n",
      "=== Spoof Accuracy CSV ===\n",
      "ADD_BACKGROUND_MUSIC,ADD_BACKGROUND_NOISE,AMPLITUDE_MODULATION,AUTOTUNE,BASELINE,BIT_DEPTH,ECHO,EQUALIZATION,FREQ_MINUS,FREQ_PLUS,GAUSSIAN_NOISE,HIGH_PASS_FILTER,LOW_PASS_FILTER,MP3_COMPRESSION,PITCH_SHIFT,REVERB,SILENCE_INJECTION,TIME_STRETCH,Pooled Spoof\n",
      "76.50,79.50,98.50,99.50,98.50,99.00,100.00,96.50,90.50,93.00,59.00,94.00,93.50,94.50,100.00,98.00,98.50,99.50,92.69\n",
      "\n",
      "=== Overall Attack Type Accuracy CSV ===\n",
      "ADD_BACKGROUND_MUSIC,ADD_BACKGROUND_NOISE,AMPLITUDE_MODULATION,AUTOTUNE,BASELINE,BIT_DEPTH,ECHO,EQUALIZATION,FREQ_MINUS,FREQ_PLUS,GAUSSIAN_NOISE,HIGH_PASS_FILTER,LOW_PASS_FILTER,MP3_COMPRESSION,PITCH_SHIFT,REVERB,SILENCE_INJECTION,TIME_STRETCH,Pooled Overall\n",
      "73.00,80.50,74.50,61.75,89.75,75.25,51.00,89.75,78.50,80.00,63.75,74.50,84.25,78.25,54.75,91.00,86.00,51.50,74.33\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Dict, Optional, List, Union, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "from pathlib import Path\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "METADATA_PATH = \"/nvme1/hungdx/Lightning-hydra/data/shortcutASV/merged_protocol.txt\"\n",
    "META_CSV_PATH = \"/nvme1/hungdx/Lightning-hydra/data/shortcutASV/merged_meta.csv\"\n",
    "\n",
    "# Option to calculate EER (set to False to save time)\n",
    "CALCULATE_EER = True  # Set to False to skip EER calculations and save time\n",
    "\n",
    "# Multiple prediction files for simultaneous evaluation\n",
    "PREDICTION_FILES = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/Conformer_MDT_DEC2024_correct/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_infer_Conformer_MDT_DEC2024_correct.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/Conformer_MDT_LA19/merged_scores_huggingface_benchmark_xlsr_conformertcm_mdt_lora_infer_Conformer_MDT_LA19.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/AASIST_SSL_MDT_LA19/merged_scores_huggingface_benchmark_xlsr_aasist_mdt_paper_AASIST_SSL_MDT_LA19.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ToP_April/merged_scores_cnsl_xlsr_vib_large_corpus_ToP_April.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_june27/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_june27.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g2_june27/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g2_june27.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g3_june27/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g3_june27.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g4_june27/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g4_june27.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_june29/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_june29.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g5_june29/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g5_june29.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_train-with-all_june30/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_train-with-all_june30.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/Conformer_MDT_DEC2024_correct/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_infer_Conformer_MDT_DEC2024_correct.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/Conformer_MDT_DEC2024_correct_var/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_infer_var_Conformer_MDT_DEC2024_correct_var.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ToP_April/merged_scores_cnsl_xlsr_vib_large_corpus_ToP_April.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_v2_july2/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_v2_july2.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_v1_july2/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g1_v1_july2.txt\"\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g5_july2/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g5_july2.txt\",\n",
    "    # \"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g2_july2/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g2_july2.txt\"\n",
    "    #\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g6_july2/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_ConformerTCM_MDT_LoRA_exp_g6_july2.txt\"\n",
    "    #\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g1_july6/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_july4_ConformerTCM_MDT_LoRA_exp_g1_july6.txt\",\n",
    "    #\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoRA_exp_g2_july6/merged_scores_cnsl_lora_elevenlabs_xlsr_conformertcm_mdt_more_elevenlabs_july4_ConformerTCM_MDT_LoRA_exp_g2_july6.txt\",\n",
    "    #\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_LoKr_exp_g2_july6/merged_scores_cnsl_adaptations_ft_July6_xlsr_conformertcm_mdt_lokr_group2_ConformerTCM_MDT_LoKr_exp_g2_july6.txt\",\n",
    "    #\"/nvme1/hungdx/Lightning-hydra/logs/results/noisy_benchmark/ConformerTCM_MDT_VeRa_exp_g2_july6/merged_scores_cnsl_adaptations_ft_July6_xlsr_conformertcm_mdt_vera_group2_ConformerTCM_MDT_VeRa_exp_g2_july6.txt\"\n",
    "]\n",
    "\n",
    "# Filter to only include files that exist\n",
    "PREDICTION_FILES = [f for f in PREDICTION_FILES if os.path.exists(f)]\n",
    "\n",
    "\n",
    "class MetricsCalculator:\n",
    "    @staticmethod\n",
    "    def calculate_metrics(df: pd.DataFrame, group_column: Optional[str] = None, calculate_eer: bool = True) -> Dict[str, Union[float, Dict[str, Union[float, int]]]]:\n",
    "        \"\"\"Calculate various metrics for the given DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing predictions and ground truth\n",
    "            group_column: Optional column name to group results by\n",
    "            calculate_eer: Whether to calculate EER (can be time-consuming)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing calculated metrics and sample counts\n",
    "        \"\"\"\n",
    "        # Verify that we have the expected labels\n",
    "        unique_labels = df['label'].unique()\n",
    "        expected_labels = ['bonafide', 'spoof']\n",
    "        print(f\"Unique labels found: {sorted(unique_labels)}\")\n",
    "        \n",
    "        if not all(label in unique_labels for label in expected_labels if len(df[df['label'] == label]) > 0):\n",
    "            print(f\"Warning: Expected labels {expected_labels}, but found {unique_labels}\")\n",
    "        \n",
    "        results = {\n",
    "            'overall': accuracy_score(df[\"label\"], df[\"pred\"]) * 100,\n",
    "            'f1': f1_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'recall': recall_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'precision': precision_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'total_samples': len(df)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall EER only if requested\n",
    "        if calculate_eer:\n",
    "            bona_cm = df[df['label'] == 'bonafide']['score'].values\n",
    "            spoof_cm = df[df['label'] == 'spoof']['score'].values\n",
    "            if len(bona_cm) > 0 and len(spoof_cm) > 0:\n",
    "                eer_cm, _ = em.compute_eer(bona_cm, spoof_cm)\n",
    "                results['overall_eer'] = eer_cm * 100\n",
    "            else:\n",
    "                print(\"Warning: Missing bonafide or spoof samples for EER calculation\")\n",
    "                results['overall_eer'] = None\n",
    "        else:\n",
    "            results['overall_eer'] = None\n",
    "        \n",
    "        if group_column and group_column in df.columns:\n",
    "            group_metrics = {}\n",
    "            \n",
    "            print(f\"Available groups: {sorted(df[group_column].unique())}\")\n",
    "            \n",
    "            # Process each attack type/group\n",
    "            for group, group_df in df.groupby(group_column):\n",
    "                # Calculate separate accuracies for bonafide and spoof within this modification\n",
    "                \n",
    "                # Bonafide accuracy within this modification\n",
    "                bonafide_subset = group_df[group_df['label'] == 'bonafide']\n",
    "                bonafide_correct = len(bonafide_subset[bonafide_subset['pred'] == 'bonafide'])\n",
    "                bonafide_total = len(bonafide_subset)\n",
    "                bonafide_accuracy = (bonafide_correct / bonafide_total * 100) if bonafide_total > 0 else 0.0\n",
    "                \n",
    "                # Spoof accuracy within this modification\n",
    "                spoof_subset = group_df[group_df['label'] == 'spoof']\n",
    "                spoof_correct = len(spoof_subset[spoof_subset['pred'] == 'spoof'])\n",
    "                spoof_total = len(spoof_subset)\n",
    "                spoof_accuracy = (spoof_correct / spoof_total * 100) if spoof_total > 0 else 0.0\n",
    "                \n",
    "                # Overall accuracy for this modification (combining bonafide and spoof)\n",
    "                overall_accuracy = accuracy_score(group_df[\"label\"], group_df[\"pred\"]) * 100\n",
    "                \n",
    "                # Get score arrays for EER calculation\n",
    "                group_bona_scores = group_df[group_df['label'] == 'bonafide']['score'].values\n",
    "                group_spoof_scores = group_df[group_df['label'] == 'spoof']['score'].values\n",
    "                \n",
    "                group_result = {\n",
    "                    'bonafide_accuracy': bonafide_accuracy,\n",
    "                    'spoof_accuracy': spoof_accuracy,\n",
    "                    'overall_accuracy': overall_accuracy,\n",
    "                    'bonafide_correct': bonafide_correct,\n",
    "                    'bonafide_total': bonafide_total,\n",
    "                    'spoof_correct': spoof_correct,\n",
    "                    'spoof_total': spoof_total,\n",
    "                    'total_samples': len(group_df)\n",
    "                }\n",
    "                \n",
    "                if calculate_eer:\n",
    "                    # Calculate EER for this attack type using bonafide vs spoof within the group\n",
    "                    if len(group_bona_scores) > 0 and len(group_spoof_scores) > 0:\n",
    "                        group_eer, group_threshold = em.compute_eer(group_bona_scores, group_spoof_scores)\n",
    "                        group_result.update({\n",
    "                            'eer': group_eer * 100,\n",
    "                            'threshold': group_threshold,\n",
    "                        })\n",
    "                    else:\n",
    "                        group_result.update({\n",
    "                            'eer': None,\n",
    "                            'threshold': None,\n",
    "                        })\n",
    "                \n",
    "                group_metrics[group] = group_result\n",
    "                \n",
    "            results['groups'] = group_metrics\n",
    "        \n",
    "        return results\n",
    "\n",
    "def extract_attack_type_from_path(path: str) -> str:\n",
    "    \"\"\"Extract attack type from the file path.\"\"\"\n",
    "    # Extract directory structure to get attack type\n",
    "    path_parts = path.split('/')\n",
    "    \n",
    "    # Look for the attack type in the path\n",
    "    # Common patterns: asv19/wav/ATTACK_TYPE/bona-fide/... or asv19/wav/ATTACK_TYPE/spoof/...\n",
    "    for i, part in enumerate(path_parts):\n",
    "        if part == 'wav' and i + 1 < len(path_parts):\n",
    "            attack_type = path_parts[i + 1]\n",
    "            return attack_type\n",
    "    \n",
    "    # Fallback: use unknown\n",
    "    return 'unknown'\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    \"\"\"Load and process metadata files with proper column handling.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing merged metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load protocol file\n",
    "        metadata = pd.read_csv(METADATA_PATH, sep=\" \", header=None)\n",
    "        metadata.columns = [\"path\", \"subset\", \"label\"]\n",
    "        \n",
    "        print(f\"Protocol file loaded: {len(metadata)} entries\")\n",
    "        print(f\"Label distribution in protocol file:\")\n",
    "        print(metadata['label'].value_counts())\n",
    "        \n",
    "        # Extract attack type from path\n",
    "        metadata['attack_type'] = metadata['path'].apply(extract_attack_type_from_path)\n",
    "        \n",
    "        print(f\"Attack type distribution:\")\n",
    "        print(metadata['attack_type'].value_counts())\n",
    "        \n",
    "        # Show cross-tabulation of attack_type vs label\n",
    "        print(f\"\\nAttack type vs Label cross-tabulation:\")\n",
    "        print(pd.crosstab(metadata['attack_type'], metadata['label'], margins=True))\n",
    "        \n",
    "        # Try to load CSV metadata if it exists and is not too large\n",
    "        try:\n",
    "            # Check file size first\n",
    "            csv_size = os.path.getsize(META_CSV_PATH)\n",
    "            print(f\"CSV metadata file size: {csv_size / (1024*1024):.1f} MB\")\n",
    "            \n",
    "            if csv_size < 50 * 1024 * 1024:  # Less than 50MB\n",
    "                meta_csv = pd.read_csv(META_CSV_PATH, sep=\"|\")\n",
    "                print(f\"CSV metadata loaded: {len(meta_csv)} entries\")\n",
    "                print(f\"CSV columns: {meta_csv.columns.tolist()}\")\n",
    "                \n",
    "                # rename columns from file_path to path_audio\n",
    "                if 'file_path' in meta_csv.columns:\n",
    "                    meta_csv = meta_csv.rename(columns={'file_path': 'path_audio'})\n",
    "                elif 'path' in meta_csv.columns:\n",
    "                    meta_csv = meta_csv.rename(columns={'path': 'path_audio'})\n",
    "                \n",
    "                # Handle column conflicts before merge by dropping conflicting columns from meta_csv\n",
    "                conflicting_cols = ['subset', 'label', 'path', 'attack_type']\n",
    "                for col in conflicting_cols:\n",
    "                    if col in meta_csv.columns:\n",
    "                        print(f\"Dropping conflicting column '{col}' from CSV metadata\")\n",
    "                        meta_csv = meta_csv.drop(columns=[col])\n",
    "                \n",
    "                # Check if 'path_audio' column exists\n",
    "                if 'path_audio' not in meta_csv.columns:\n",
    "                    # Try to find similar column names\n",
    "                    audio_cols = [col for col in meta_csv.columns if 'path' in col.lower() or 'audio' in col.lower()]\n",
    "                    if audio_cols:\n",
    "                        meta_csv['path_audio'] = meta_csv[audio_cols[0]]\n",
    "                        print(f\"Using column '{audio_cols[0]}' as path_audio\")\n",
    "                    else:\n",
    "                        print(\"Warning: No suitable audio path column found in CSV metadata\")\n",
    "                        return metadata  # Return without merge\n",
    "                \n",
    "                # Perform merge\n",
    "                print(\"Merging protocol and CSV metadata...\")\n",
    "                merged_metadata = metadata.merge(meta_csv, left_on='path', right_on='path_audio', how='left')\n",
    "                print(f\"Merged metadata: {len(merged_metadata)} entries\")\n",
    "                \n",
    "                return merged_metadata\n",
    "            else:\n",
    "                print(\"CSV file too large, using only protocol file\")\n",
    "                return metadata\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load CSV metadata: {str(e)}\")\n",
    "            print(\"Using only protocol file\")\n",
    "            return metadata\n",
    "        \n",
    "        return metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load metadata: {str(e)}\")\n",
    "\n",
    "def process_prediction_file(score_file: str, metadata_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process a single prediction file and return results DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        score_file: Path to the prediction file\n",
    "        metadata_df: DataFrame containing metadata\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing processed predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "        pred_df.columns = [\"path\", \"spoof\", \"score\"]\n",
    "        pred_df = pred_df.drop_duplicates(subset=['path'])\n",
    "        \n",
    "        print(f\"Prediction file loaded: {len(pred_df)} entries\")\n",
    "        \n",
    "        # Perform merge - try both path columns\n",
    "        merge_column = 'path_audio' if 'path_audio' in metadata_df.columns else 'path'\n",
    "        merged_df = pred_df.merge(metadata_df, left_on='path', right_on=merge_column, how='left')\n",
    "        \n",
    "        print(f\"Merged prediction+metadata: {len(merged_df)} entries\")\n",
    "        print(f\"Available columns: {merged_df.columns.tolist()}\")\n",
    "        \n",
    "        # Handle column naming issues after merge\n",
    "        if 'label' not in merged_df.columns:\n",
    "            if 'label_x' in merged_df.columns:\n",
    "                merged_df['label'] = merged_df['label_x']\n",
    "                merged_df = merged_df.drop(columns=['label_x'])\n",
    "            elif 'label_y' in merged_df.columns:\n",
    "                merged_df['label'] = merged_df['label_y']\n",
    "                merged_df = merged_df.drop(columns=['label_y'])\n",
    "            else:\n",
    "                raise ValueError(\"No label column found after merge\")\n",
    "        \n",
    "        # Ensure we have attack_type column\n",
    "        if 'attack_type' not in merged_df.columns:\n",
    "            print(\"No attack_type column found, extracting from path...\")\n",
    "            merged_df['attack_type'] = merged_df['path'].apply(extract_attack_type_from_path)\n",
    "        \n",
    "        # Create predictions\n",
    "        merged_df['pred'] = merged_df.apply(\n",
    "            lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "        \n",
    "        # Verify label consistency\n",
    "        print(f\"Final label distribution: {merged_df['label'].value_counts().to_dict()}\")\n",
    "        print(f\"Attack type distribution: {merged_df['attack_type'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Show attack type vs label cross-tabulation for verification\n",
    "        print(f\"\\nAttack type vs Label cross-tabulation:\")\n",
    "        print(pd.crosstab(merged_df['attack_type'], merged_df['label'], margins=True))\n",
    "        \n",
    "        # Remove rows with missing labels\n",
    "        before_filter = len(merged_df)\n",
    "        merged_df = merged_df.dropna(subset=['label'])\n",
    "        after_filter = len(merged_df)\n",
    "        if before_filter != after_filter:\n",
    "            print(f\"Removed {before_filter - after_filter} rows with missing labels\")\n",
    "        \n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to process prediction file {score_file}: {str(e)}\")\n",
    "\n",
    "def print_results(model_results: Dict, model_name: str, original_results: Optional[Dict] = None) -> None:\n",
    "    \"\"\"Print evaluation results in a formatted way matching your reference code.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary containing model metrics\n",
    "        model_name: Name of the model\n",
    "        original_results: Optional dictionary containing original model results for comparison\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    print(f\"\\nTotal Samples: {model_results['total_samples']}\")\n",
    "    print(f\"Overall Accuracy: {model_results['overall']:.2f}%\")\n",
    "    \n",
    "    if model_results.get('overall_eer') is not None:\n",
    "        print(f\"Overall EER: {model_results['overall_eer']:.2f}%\")\n",
    "    else:\n",
    "        print(\"Overall EER: Not calculated (EER calculation disabled)\")\n",
    "    \n",
    "    if 'groups' in model_results:\n",
    "        print(\"\\nBONAFIDE Accuracy by Attack Type:\")\n",
    "        for group, metrics in sorted(model_results['groups'].items()):\n",
    "            print(f\"{group},{metrics['bonafide_correct']},{metrics['bonafide_accuracy']:.2f}\")\n",
    "        \n",
    "        print(\"\\nSPOOF Accuracy by Attack Type:\")\n",
    "        for group, metrics in sorted(model_results['groups'].items()):\n",
    "            print(f\"{group},{metrics['spoof_correct']},{metrics['spoof_accuracy']:.2f}\")\n",
    "        \n",
    "        print(\"\\nDetailed Metrics by Attack Type:\")\n",
    "        for group, metrics in sorted(model_results['groups'].items()):\n",
    "            print(f\"  {group}:\")\n",
    "            print(f\"    Bonafide: {metrics['bonafide_correct']}/{metrics['bonafide_total']} ({metrics['bonafide_accuracy']:.2f}%)\")\n",
    "            print(f\"    Spoof: {metrics['spoof_correct']}/{metrics['spoof_total']} ({metrics['spoof_accuracy']:.2f}%)\")\n",
    "            print(f\"    Overall: {metrics['overall_accuracy']:.2f}%\")\n",
    "            if metrics.get('eer') is not None:\n",
    "                print(f\"    EER: {metrics['eer']:.2f}%\")\n",
    "                print(f\"    Threshold: {metrics['threshold']:.4f}\")\n",
    "            else:\n",
    "                print(f\"    EER: Not calculated\")\n",
    "            print(f\"    Total Samples: {metrics['total_samples']}\")\n",
    "    \n",
    "    print(\"\\nAdditional overall metrics:\")\n",
    "    print(f\"  F1 Score: {model_results['f1']:.4f}\")\n",
    "    print(f\"  Recall: {model_results['recall']:.4f}\")\n",
    "    print(f\"  Precision: {model_results['precision']:.4f}\")\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "def print_eer_csv(model_results: Dict, model_name: str):\n",
    "    if 'groups' not in model_results:\n",
    "        print(\"No group EERs to print.\")\n",
    "        return\n",
    "\n",
    "    # Check if EER was calculated\n",
    "    first_group = next(iter(model_results['groups'].values()))\n",
    "    if first_group.get('eer') is None:\n",
    "        print(\"EER values not calculated (EER calculation was disabled).\")\n",
    "        return\n",
    "\n",
    "    # Collect attack types and EERs\n",
    "    attack_types = sorted(model_results['groups'].keys())\n",
    "    eers = []\n",
    "    for atk in attack_types:\n",
    "        if model_results['groups'][atk].get('eer') is not None:\n",
    "            eers.append(model_results['groups'][atk]['eer'])\n",
    "\n",
    "    if not eers:\n",
    "        print(\"No valid EER values found.\")\n",
    "        return\n",
    "\n",
    "    # Calculate pooled EER (mean of all attack type EERs)\n",
    "    pooled_eer = np.mean(eers)\n",
    "\n",
    "    # Print header\n",
    "    print(','.join(attack_types + ['Pooled EER']))\n",
    "    # Print values\n",
    "    print(','.join([f\"{eer:.2f}\" for eer in eers] + [f\"{pooled_eer:.2f}\"]))\n",
    "\n",
    "def print_accuracy_csv(model_results: Dict, model_name: str):\n",
    "    \"\"\"Print accuracy results in CSV format matching your reference code.\"\"\"\n",
    "    if 'groups' not in model_results:\n",
    "        print(\"No group accuracies to print.\")\n",
    "        return\n",
    "\n",
    "    # Collect all attack types\n",
    "    attack_types = sorted(model_results['groups'].keys())\n",
    "    \n",
    "    # Print bonafide accuracies\n",
    "    print(\"=== Bonafide Accuracy CSV ===\")\n",
    "    bonafide_accs = [model_results['groups'][atk]['bonafide_accuracy'] for atk in attack_types]\n",
    "    pooled_bonafide = np.mean(bonafide_accs) if bonafide_accs else 0.0\n",
    "    print(','.join(attack_types + ['Pooled Bonafide']))\n",
    "    print(','.join([f\"{acc:.2f}\" for acc in bonafide_accs] + [f\"{pooled_bonafide:.2f}\"]))\n",
    "    \n",
    "    # Print spoof accuracies\n",
    "    print(\"\\n=== Spoof Accuracy CSV ===\")\n",
    "    spoof_accs = [model_results['groups'][atk]['spoof_accuracy'] for atk in attack_types]\n",
    "    pooled_spoof = np.mean(spoof_accs) if spoof_accs else 0.0\n",
    "    print(','.join(attack_types + ['Pooled Spoof']))\n",
    "    print(','.join([f\"{acc:.2f}\" for acc in spoof_accs] + [f\"{pooled_spoof:.2f}\"]))\n",
    "    \n",
    "    # Print overall accuracies per attack type\n",
    "    print(\"\\n=== Overall Attack Type Accuracy CSV ===\")\n",
    "    overall_accs = [model_results['groups'][atk]['overall_accuracy'] for atk in attack_types]\n",
    "    pooled_overall = np.mean(overall_accs) if overall_accs else 0.0\n",
    "    print(','.join(attack_types + ['Pooled Overall']))\n",
    "    print(','.join([f\"{acc:.2f}\" for acc in overall_accs] + [f\"{pooled_overall:.2f}\"]))\n",
    "\n",
    "def print_comparative_csv(all_results: Dict[str, Dict]):\n",
    "    \"\"\"Print comparative CSV results across all models.\"\"\"\n",
    "    if not all_results:\n",
    "        return\n",
    "    \n",
    "    # Get all unique attack types from all models\n",
    "    all_attack_types = set()\n",
    "    for results in all_results.values():\n",
    "        if 'groups' in results:\n",
    "            all_attack_types.update(results['groups'].keys())\n",
    "    \n",
    "    attack_types = sorted(all_attack_types)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARATIVE RESULTS ACROSS ALL MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check if EER was calculated\n",
    "    eer_calculated = False\n",
    "    for results in all_results.values():\n",
    "        if 'groups' in results:\n",
    "            first_group = next(iter(results['groups'].values()))\n",
    "            if first_group.get('eer') is not None:\n",
    "                eer_calculated = True\n",
    "                break\n",
    "    \n",
    "    # EER Comparison (only if calculated)\n",
    "    if eer_calculated:\n",
    "        print(\"\\n=== EER Comparison CSV ===\")\n",
    "        if attack_types:\n",
    "            header = ['Model'] + attack_types + ['Pooled EER']\n",
    "            print(','.join(header))\n",
    "            \n",
    "            for model_name, results in all_results.items():\n",
    "                if 'groups' not in results:\n",
    "                    continue\n",
    "                \n",
    "                model_short = model_name[-50:]  # Last 50 characters of the model name\n",
    "                eers = []\n",
    "                for atk in attack_types:\n",
    "                    if atk in results['groups'] and results['groups'][atk].get('eer') is not None:\n",
    "                        eers.append(f\"{results['groups'][atk]['eer']:.2f}\")\n",
    "                    else:\n",
    "                        eers.append(\"N/A\")\n",
    "                \n",
    "                # Calculate pooled EER\n",
    "                valid_eers = [results['groups'][atk]['eer'] for atk in attack_types \n",
    "                             if atk in results['groups'] and results['groups'][atk].get('eer') is not None]\n",
    "                pooled_eer = np.mean(valid_eers) if valid_eers else 0.0\n",
    "                \n",
    "                row = [model_short] + eers + [f\"{pooled_eer:.2f}\"]\n",
    "                print(','.join(row))\n",
    "    else:\n",
    "        print(\"\\n=== EER Comparison CSV ===\")\n",
    "        print(\"EER calculations were disabled - no EER comparison available\")\n",
    "    \n",
    "    # Bonafide Accuracy Comparison\n",
    "    print(\"\\n=== Bonafide Accuracy Comparison CSV ===\")\n",
    "    if attack_types:\n",
    "        header = ['Model'] + attack_types + ['Pooled Bonafide']\n",
    "        print(','.join(header))\n",
    "        \n",
    "        for model_name, results in all_results.items():\n",
    "            if 'groups' not in results:\n",
    "                continue\n",
    "            \n",
    "            model_short = model_name[-50:]\n",
    "            row = [model_short]\n",
    "            \n",
    "            bonafide_accs = []\n",
    "            for atk in attack_types:\n",
    "                if atk in results['groups']:\n",
    "                    acc = results['groups'][atk]['bonafide_accuracy']\n",
    "                    row.append(f\"{acc:.2f}\")\n",
    "                    bonafide_accs.append(acc)\n",
    "                else:\n",
    "                    row.append(\"N/A\")\n",
    "            \n",
    "            pooled_bonafide = np.mean(bonafide_accs) if bonafide_accs else 0.0\n",
    "            row.append(f\"{pooled_bonafide:.2f}\")\n",
    "            \n",
    "            print(','.join(row))\n",
    "    \n",
    "    # Spoof Accuracy Comparison\n",
    "    print(\"\\n=== Spoof Accuracy Comparison CSV ===\")\n",
    "    if attack_types:\n",
    "        header = ['Model'] + attack_types + ['Pooled Spoof']\n",
    "        print(','.join(header))\n",
    "        \n",
    "        for model_name, results in all_results.items():\n",
    "            if 'groups' not in results:\n",
    "                continue\n",
    "            \n",
    "            model_short = model_name[-50:]\n",
    "            row = [model_short]\n",
    "            \n",
    "            spoof_accs = []\n",
    "            for atk in attack_types:\n",
    "                if atk in results['groups']:\n",
    "                    acc = results['groups'][atk]['spoof_accuracy']\n",
    "                    row.append(f\"{acc:.2f}\")\n",
    "                    spoof_accs.append(acc)\n",
    "                else:\n",
    "                    row.append(\"N/A\")\n",
    "            \n",
    "            pooled_spoof = np.mean(spoof_accs) if spoof_accs else 0.0\n",
    "            row.append(f\"{pooled_spoof:.2f}\")\n",
    "            \n",
    "            print(','.join(row))\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to run the evaluation pipeline.\"\"\"\n",
    "    try:\n",
    "        print(f\"EER Calculation: {'Enabled' if CALCULATE_EER else 'Disabled (for faster processing)'}\")\n",
    "        print(\"Loading metadata...\")\n",
    "        metadata_df = load_metadata()\n",
    "        print(f\"Loaded metadata with shape: {metadata_df.shape}\")\n",
    "        print(f\"Columns: {metadata_df.columns.tolist()}\")\n",
    "        \n",
    "        print(f\"\\nFound {len(PREDICTION_FILES)} prediction files to process\")\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for i, score_file in enumerate(PREDICTION_FILES, 1):\n",
    "            model_name = Path(score_file).name\n",
    "            print(f\"\\n[{i}/{len(PREDICTION_FILES)}] Processing {model_name}...\")\n",
    "            \n",
    "            if not os.path.exists(score_file):\n",
    "                print(f\"Warning: File {score_file} does not exist, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                results_df = process_prediction_file(score_file, metadata_df)\n",
    "                print(f\"Processed results shape: {results_df.shape}\")\n",
    "                print(f\"Required columns present: {all(col in results_df.columns for col in ['label', 'pred', 'score'])}\")\n",
    "                \n",
    "                metrics = MetricsCalculator.calculate_metrics(results_df, group_column='attack_type', calculate_eer=CALCULATE_EER)\n",
    "                all_results[model_name] = metrics\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nSuccessfully processed {len(all_results)} models\")\n",
    "        \n",
    "        # Print individual results\n",
    "        for model_name, metrics in all_results.items():\n",
    "            print_results(metrics, model_name)\n",
    "            if CALCULATE_EER:\n",
    "                print(\"\\nEER CSV:\")\n",
    "                print_eer_csv(metrics, model_name)\n",
    "            print(\"\\nAccuracy CSV:\")\n",
    "            print_accuracy_csv(metrics, model_name)\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        # Print comparative results\n",
    "        if len(all_results) > 1:\n",
    "            print_comparative_csv(all_results)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
