{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc9204f",
   "metadata": {},
   "source": [
    "# MLAAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4feb0ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74639/746497849.py:56: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  meta_csv = pd.read_csv(META_CSV_PATH, sep=\"|\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing MLAAD_distil_distil_wav2vec2_n_trans_layers_conformertcm_Distil_W2V_5_Conf-TCM_wo_norm_ws_cv_emb_c_m_stage2_op_fullset.txt...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model: MLAAD_distil_distil_wav2vec2_n_trans_layers_conformertcm_Distil_W2V_5_Conf-TCM_wo_norm_ws_cv_emb_c_m_stage2_op_fullset.txt\n",
      "\n",
      "Total Samples: 418059\n",
      "Overall Accuracy: 89.88%\n",
      "\n",
      "Accuracy by group:\n",
      "  -:\n",
      "    Accuracy: 95.16%\n",
      "    Samples: 243059\n",
      "  FishTTS:\n",
      "    Accuracy: 97.50%\n",
      "    Samples: 3000\n",
      "  Mars5:\n",
      "    Accuracy: 82.50%\n",
      "    Samples: 1000\n",
      "  MatchaTTS:\n",
      "    Accuracy: 99.20%\n",
      "    Samples: 1000\n",
      "  MegaTTS3:\n",
      "    Accuracy: 66.60%\n",
      "    Samples: 2000\n",
      "  MeloTTS:\n",
      "    Accuracy: 98.80%\n",
      "    Samples: 1000\n",
      "  Metavoice-1B:\n",
      "    Accuracy: 63.60%\n",
      "    Samples: 1000\n",
      "  Nari Dia-1.6B:\n",
      "    Accuracy: 57.80%\n",
      "    Samples: 1000\n",
      "  OpenVoiceV2:\n",
      "    Accuracy: 99.62%\n",
      "    Samples: 4000\n",
      "  Resemble.ai (April 12th, 2025):\n",
      "    Accuracy: 97.32%\n",
      "    Samples: 5000\n",
      "  Spark-TTS-0.5B:\n",
      "    Accuracy: 69.20%\n",
      "    Samples: 1000\n",
      "  WhisperSpeech:\n",
      "    Accuracy: 91.80%\n",
      "    Samples: 2000\n",
      "  bark:\n",
      "    Accuracy: 90.22%\n",
      "    Samples: 32000\n",
      "  capacitron-t2-c50:\n",
      "    Accuracy: 98.10%\n",
      "    Samples: 1000\n",
      "  e2-tts:\n",
      "    Accuracy: 91.90%\n",
      "    Samples: 1000\n",
      "  f5-tts:\n",
      "    Accuracy: 93.60%\n",
      "    Samples: 1000\n",
      "  facebook/mms-tts-deu:\n",
      "    Accuracy: 99.70%\n",
      "    Samples: 1000\n",
      "  facebook/mms-tts-eng:\n",
      "    Accuracy: 97.90%\n",
      "    Samples: 1000\n",
      "  facebook/mms-tts-fin:\n",
      "    Accuracy: 84.90%\n",
      "    Samples: 1000\n",
      "  facebook/mms-tts-fra:\n",
      "    Accuracy: 99.10%\n",
      "    Samples: 1000\n",
      "  facebook/mms-tts-hun:\n",
      "    Accuracy: 74.90%\n",
      "    Samples: 1000\n",
      "  facebook/mms-tts-nld:\n",
      "    Accuracy: 97.80%\n",
      "    Samples: 1000\n",
      "  facebook/mms-tts-ron:\n",
      "    Accuracy: 61.90%\n",
      "    Samples: 1000\n",
      "  facebook/mms-tts-rus:\n",
      "    Accuracy: 98.30%\n",
      "    Samples: 1000\n",
      "  facebook/mms-tts-swe:\n",
      "    Accuracy: 96.90%\n",
      "    Samples: 1000\n",
      "  facebook/mms-tts-ukr:\n",
      "    Accuracy: 97.80%\n",
      "    Samples: 1000\n",
      "  fast_pitch:\n",
      "    Accuracy: 98.40%\n",
      "    Samples: 1000\n",
      "  glow-tts:\n",
      "    Accuracy: 97.27%\n",
      "    Samples: 6000\n",
      "  griffin_lim:\n",
      "    Accuracy: 42.27%\n",
      "    Samples: 8000\n",
      "  jenny:\n",
      "    Accuracy: 16.60%\n",
      "    Samples: 1000\n",
      "  kokoro:\n",
      "    Accuracy: 96.24%\n",
      "    Samples: 5000\n",
      "  microsoft/speecht5_tts:\n",
      "    Accuracy: 96.40%\n",
      "    Samples: 1000\n",
      "  neural_hmm:\n",
      "    Accuracy: 84.10%\n",
      "    Samples: 1000\n",
      "  optispeech:\n",
      "    Accuracy: 62.90%\n",
      "    Samples: 1000\n",
      "  orpheus-tts-0.1-finetune:\n",
      "    Accuracy: 89.70%\n",
      "    Samples: 1000\n",
      "  overflow:\n",
      "    Accuracy: 97.30%\n",
      "    Samples: 1000\n",
      "  parler_tts_large_v1:\n",
      "    Accuracy: 60.50%\n",
      "    Samples: 1000\n",
      "  parler_tts_mini_v0.1:\n",
      "    Accuracy: 67.60%\n",
      "    Samples: 1000\n",
      "  parler_tts_mini_v1:\n",
      "    Accuracy: 60.20%\n",
      "    Samples: 1000\n",
      "  sesame_csm:\n",
      "    Accuracy: 74.80%\n",
      "    Samples: 1000\n",
      "  speedy-speech:\n",
      "    Accuracy: 99.80%\n",
      "    Samples: 1000\n",
      "  suno/bark:\n",
      "    Accuracy: 90.90%\n",
      "    Samples: 1000\n",
      "  suno/bark-small:\n",
      "    Accuracy: 92.70%\n",
      "    Samples: 1000\n",
      "  tacotron-DDC:\n",
      "    Accuracy: 100.00%\n",
      "    Samples: 1000\n",
      "  tacotron2:\n",
      "    Accuracy: 91.90%\n",
      "    Samples: 1000\n",
      "  tacotron2-DCA:\n",
      "    Accuracy: 95.15%\n",
      "    Samples: 2000\n",
      "  tacotron2-DDC:\n",
      "    Accuracy: 82.35%\n",
      "    Samples: 4000\n",
      "  tacotron2-DDC-GST:\n",
      "    Accuracy: 86.30%\n",
      "    Samples: 1000\n",
      "  tacotron2-DDC_ph:\n",
      "    Accuracy: 94.00%\n",
      "    Samples: 1000\n",
      "  tortoise-v2:\n",
      "    Accuracy: 98.80%\n",
      "    Samples: 1000\n",
      "  vits:\n",
      "    Accuracy: 69.96%\n",
      "    Samples: 25000\n",
      "  vits--neon:\n",
      "    Accuracy: 80.10%\n",
      "    Samples: 1000\n",
      "  vits-female:\n",
      "    Accuracy: 53.70%\n",
      "    Samples: 1000\n",
      "  vits-male:\n",
      "    Accuracy: 64.20%\n",
      "    Samples: 1000\n",
      "  vits-neon:\n",
      "    Accuracy: 58.90%\n",
      "    Samples: 1000\n",
      "  vixTTS:\n",
      "    Accuracy: 94.05%\n",
      "    Samples: 2000\n",
      "  xtts_v1.1:\n",
      "    Accuracy: 99.22%\n",
      "    Samples: 14000\n",
      "  xtts_v2:\n",
      "    Accuracy: 71.77%\n",
      "    Samples: 17000\n",
      "  zonosTTS-v0.1:\n",
      "    Accuracy: 55.15%\n",
      "    Samples: 2000\n",
      "\n",
      "Additional metrics:\n",
      "  F1 Score: 0.9162\n",
      "  Recall: 0.9516\n",
      "  Precision: 0.8833\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Dict, Optional, List, Union, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "from pathlib import Path\n",
    "\n",
    "# Constants\n",
    "METADATA_PATH = \"/nvme1_hungdx/Lightning-hydra/data/MLAAD/protocol.txt\"\n",
    "META_CSV_PATH = \"/nvme1_hungdx/Lightning-hydra/data/MLAAD/meta.csv\"\n",
    "PREDICTION_FILE = \"/home/hungdx/code/Lightning-hydra/logs/results/benchmark_kd/Distil_W2V_5_Conf-TCM_wo_norm_ws_cv_emb_c_m_stage2_op_fullset/MLAAD_distil_distil_wav2vec2_n_trans_layers_conformertcm_Distil_W2V_5_Conf-TCM_wo_norm_ws_cv_emb_c_m_stage2_op_fullset.txt\"\n",
    "# PREDICTION_FILE=\"/nvme1/hungdx/Lightning-hydra/logs/results/cnsl_benchmark/ToP_April/MLAAD_cnsl_xlsr_vib_large_corpus_ToP_April.txt\"\n",
    "\n",
    "class MetricsCalculator:\n",
    "    @staticmethod\n",
    "    def calculate_metrics(df: pd.DataFrame, group_column: Optional[str] = None) -> Dict[str, Union[float, Dict[str, Union[float, int]]]]:\n",
    "        \"\"\"Calculate various metrics for the given DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing predictions and ground truth\n",
    "            group_column: Optional column name to group results by\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing calculated metrics and sample counts\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'overall': accuracy_score(df[\"label\"], df[\"pred\"]) * 100,\n",
    "            'f1': f1_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'recall': recall_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'precision': precision_score(df['label'], df['pred'], pos_label='bonafide'),\n",
    "            'total_samples': len(df)\n",
    "        }\n",
    "        \n",
    "        if group_column and group_column in df.columns:\n",
    "            group_metrics = {}\n",
    "            for group, group_df in df.groupby(group_column):\n",
    "                group_metrics[group] = {\n",
    "                    'accuracy': accuracy_score(group_df[\"label\"], group_df[\"pred\"]) * 100,\n",
    "                    'samples': len(group_df)\n",
    "                }\n",
    "            results['groups'] = group_metrics\n",
    "        \n",
    "        return results\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    \"\"\"Load and process metadata files.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing merged metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metadata = pd.read_csv(METADATA_PATH, sep=\" \", header=None)\n",
    "        metadata.columns = [\"path\", \"subset\", \"label\"]\n",
    "        \n",
    "        meta_csv = pd.read_csv(META_CSV_PATH, sep=\"|\")\n",
    "        \n",
    "        metadata = metadata.merge(meta_csv, on='path', how='left')\n",
    "        metadata.rename(columns={\n",
    "            'subset_x': 'subset',\n",
    "            'label_y': 'label'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load metadata: {str(e)}\")\n",
    "\n",
    "def process_prediction_file(score_file: str, metadata_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process a single prediction file and return results DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        score_file: Path to the prediction file\n",
    "        metadata_df: DataFrame containing metadata\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing processed predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "        pred_df.columns = [\"path\", \"spoof\", \"score\"]\n",
    "        pred_df = pred_df.drop_duplicates(subset=['path'])\n",
    "        \n",
    "        merged_df = pred_df.merge(metadata_df, on='path', how='left')\n",
    "        merged_df['pred'] = merged_df.apply(\n",
    "            lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "        \n",
    "        return merged_df[merged_df['subset'] == 'eval'].copy()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to process prediction file {score_file}: {str(e)}\")\n",
    "\n",
    "def print_results(model_results: Dict, model_name: str, original_results: Optional[Dict] = None) -> None:\n",
    "    \"\"\"Print evaluation results in a formatted way.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary containing model metrics\n",
    "        model_name: Name of the model\n",
    "        original_results: Optional dictionary containing original model results for comparison\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    print(f\"\\nTotal Samples: {model_results['total_samples']}\")\n",
    "    print(f\"Overall Accuracy: {model_results['overall']:.2f}%\")\n",
    "    \n",
    "    if 'groups' in model_results:\n",
    "        print(\"\\nAccuracy by group:\")\n",
    "        for group, metrics in model_results['groups'].items():\n",
    "            print(f\"  {group}:\")\n",
    "            print(f\"    Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "            print(f\"    Samples: {metrics['samples']}\")\n",
    "    \n",
    "    print(\"\\nAdditional metrics:\")\n",
    "    print(f\"  F1 Score: {model_results['f1']:.4f}\")\n",
    "    print(f\"  Recall: {model_results['recall']:.4f}\")\n",
    "    print(f\"  Precision: {model_results['precision']:.4f}\")\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to run the evaluation pipeline.\"\"\"\n",
    "    try:\n",
    "        print(\"Loading metadata...\")\n",
    "        metadata_df = load_metadata()\n",
    "        \n",
    "        prediction_files = [PREDICTION_FILE]\n",
    "        prediction_files = sorted(prediction_files)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for score_file in prediction_files:\n",
    "            model_name = Path(score_file).name\n",
    "            print(f\"\\nProcessing {model_name}...\")\n",
    "            \n",
    "            results_df = process_prediction_file(score_file, metadata_df)\n",
    "            metrics = MetricsCalculator.calculate_metrics(results_df, group_column='architecture')\n",
    "            all_results[model_name] = metrics\n",
    "        \n",
    "        for model_name, metrics in all_results.items():\n",
    "            print_results(metrics, model_name)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
