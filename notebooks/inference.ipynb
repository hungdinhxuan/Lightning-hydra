{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungdx/miniconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.models.components.xlsr_conformertcm_baseline import Model as XLSRConformerTCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ckpt = torch.load(\n",
    "    '/nvme1/hungdx/Lightning-hydra/logs/train/runs/2025-03-31_09-25-21/checkpoints/epoch_026.ckpt', weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ckpt['hyper_parameters']['args']['conformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungdx/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (front_end): SSLModel(\n",
       "    (model): Wav2Vec2Model(\n",
       "      (feature_extractor): ConvFeatureExtractionModel(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Sequential(\n",
       "              (0): TransposeLast()\n",
       "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): TransposeLast()\n",
       "            )\n",
       "            (3): GELU(approximate='none')\n",
       "          )\n",
       "          (1-4): 4 x Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Sequential(\n",
       "              (0): TransposeLast()\n",
       "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): TransposeLast()\n",
       "            )\n",
       "            (3): GELU(approximate='none')\n",
       "          )\n",
       "          (5-6): 2 x Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Sequential(\n",
       "              (0): TransposeLast()\n",
       "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): TransposeLast()\n",
       "            )\n",
       "            (3): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout_input): Dropout(p=0.0, inplace=False)\n",
       "      (dropout_features): Dropout(p=0.0, inplace=False)\n",
       "      (quantizer): GumbelVectorQuantizer(\n",
       "        (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
       "      )\n",
       "      (project_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (encoder): TransformerEncoder(\n",
       "        (pos_conv): Sequential(\n",
       "          (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (1): SamePad()\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.0, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (final_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (LL): Linear(in_features=1024, out_features=144, bias=True)\n",
       "  (first_bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (selu): SELU(inplace=True)\n",
       "  (backend): MyConformer(\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-3): 4 x ConformerBlock(\n",
       "        (ff1): Scale(\n",
       "          (fn): PreNorm(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=144, out_features=576, bias=True)\n",
       "                (1): Swish()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=576, out_features=144, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (attn): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (qkv): Linear(in_features=144, out_features=432, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (ht_proj): Linear(in_features=36, out_features=144, bias=True)\n",
       "            (ht_norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (conv): ConformerConvModule(\n",
       "          (net): Sequential(\n",
       "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Rearrange('b n c -> b c n')\n",
       "            (2): Conv1d(144, 576, kernel_size=(1,), stride=(1,))\n",
       "            (3): GLU()\n",
       "            (4): DepthWiseConv1d(\n",
       "              (conv): Conv1d(288, 288, kernel_size=(31,), stride=(1,), groups=288)\n",
       "            )\n",
       "            (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (6): Swish()\n",
       "            (7): Conv1d(288, 144, kernel_size=(1,), stride=(1,))\n",
       "            (8): Rearrange('b c n -> b n c')\n",
       "            (9): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ff2): Scale(\n",
       "          (fn): PreNorm(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=144, out_features=576, bias=True)\n",
       "                (1): Swish()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=576, out_features=144, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (fc5): Linear(in_features=144, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = XLSRConformerTCM(args=args,\n",
    "                         ssl_pretrained_path='/nvme1/hungdx/Towards-Real-Time-Deepfake-Speech-Detection-in-Resource-Limited-Scenarios/pretrained/xlsr2_300m.pt')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 589 parameters from checkpoint\n",
      "Successfully loaded 588 parameters\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def load_model_weights(model, checkpoint, required_prefix='net'):\n",
    "    \"\"\"\n",
    "    Load model weights from checkpoint with specific key prefix requirements.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to load weights into\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "        required_prefix: Required prefix for state dict keys (default: 'net')\n",
    "        \n",
    "    Returns:\n",
    "        model: Model with loaded weights\n",
    "    \"\"\"\n",
    "  \n",
    "    state_dict = checkpoint if isinstance(\n",
    "        checkpoint, dict) else checkpoint.state_dict()\n",
    "    \n",
    "    print(f\"Loading {len(state_dict)} parameters from checkpoint\")\n",
    "\n",
    "    # Create new state dict with processed keys\n",
    "    new_state_dict = OrderedDict()\n",
    "\n",
    "    # Process the state dict keys\n",
    "    for key, value in state_dict.items():\n",
    "        # Skip keys that don't start with the required prefix\n",
    "        if not key.startswith(required_prefix):\n",
    "            continue\n",
    "\n",
    "        # Remove the 'net.' prefix to match model's state dict keys\n",
    "        new_key = key[len(required_prefix) + 1:]  # +1 for the dot after prefix\n",
    "        new_state_dict[new_key] = value\n",
    "\n",
    "    # Load the processed state dict\n",
    "    try:\n",
    "        model.load_state_dict(new_state_dict, strict=True)\n",
    "        print(f\"Successfully loaded {len(new_state_dict)} parameters\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error loading state dict: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_model_weights(model, ckpt['state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'MDT_202504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2523835/134978345.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  new_ckpt = torch.load(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['front_end.model.mask_emb', 'front_end.model.feature_extractor.conv_layers.0.0.weight', 'front_end.model.feature_extractor.conv_layers.0.0.bias', 'front_end.model.feature_extractor.conv_layers.0.2.1.weight', 'front_end.model.feature_extractor.conv_layers.0.2.1.bias', 'front_end.model.feature_extractor.conv_layers.1.0.weight', 'front_end.model.feature_extractor.conv_layers.1.0.bias', 'front_end.model.feature_extractor.conv_layers.1.2.1.weight', 'front_end.model.feature_extractor.conv_layers.1.2.1.bias', 'front_end.model.feature_extractor.conv_layers.2.0.weight', 'front_end.model.feature_extractor.conv_layers.2.0.bias', 'front_end.model.feature_extractor.conv_layers.2.2.1.weight', 'front_end.model.feature_extractor.conv_layers.2.2.1.bias', 'front_end.model.feature_extractor.conv_layers.3.0.weight', 'front_end.model.feature_extractor.conv_layers.3.0.bias', 'front_end.model.feature_extractor.conv_layers.3.2.1.weight', 'front_end.model.feature_extractor.conv_layers.3.2.1.bias', 'front_end.model.feature_extractor.conv_layers.4.0.weight', 'front_end.model.feature_extractor.conv_layers.4.0.bias', 'front_end.model.feature_extractor.conv_layers.4.2.1.weight', 'front_end.model.feature_extractor.conv_layers.4.2.1.bias', 'front_end.model.feature_extractor.conv_layers.5.0.weight', 'front_end.model.feature_extractor.conv_layers.5.0.bias', 'front_end.model.feature_extractor.conv_layers.5.2.1.weight', 'front_end.model.feature_extractor.conv_layers.5.2.1.bias', 'front_end.model.feature_extractor.conv_layers.6.0.weight', 'front_end.model.feature_extractor.conv_layers.6.0.bias', 'front_end.model.feature_extractor.conv_layers.6.2.1.weight', 'front_end.model.feature_extractor.conv_layers.6.2.1.bias', 'front_end.model.post_extract_proj.weight', 'front_end.model.post_extract_proj.bias', 'front_end.model.quantizer.vars', 'front_end.model.quantizer.weight_proj.weight', 'front_end.model.quantizer.weight_proj.bias', 'front_end.model.project_q.weight', 'front_end.model.project_q.bias', 'front_end.model.encoder.pos_conv.0.bias', 'front_end.model.encoder.pos_conv.0.weight_g', 'front_end.model.encoder.pos_conv.0.weight_v', 'front_end.model.encoder.layers.0.self_attn.k_proj.weight', 'front_end.model.encoder.layers.0.self_attn.k_proj.bias', 'front_end.model.encoder.layers.0.self_attn.v_proj.weight', 'front_end.model.encoder.layers.0.self_attn.v_proj.bias', 'front_end.model.encoder.layers.0.self_attn.q_proj.weight', 'front_end.model.encoder.layers.0.self_attn.q_proj.bias', 'front_end.model.encoder.layers.0.self_attn.out_proj.weight', 'front_end.model.encoder.layers.0.self_attn.out_proj.bias', 'front_end.model.encoder.layers.0.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.0.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.0.fc1.weight', 'front_end.model.encoder.layers.0.fc1.bias', 'front_end.model.encoder.layers.0.fc2.weight', 'front_end.model.encoder.layers.0.fc2.bias', 'front_end.model.encoder.layers.0.final_layer_norm.weight', 'front_end.model.encoder.layers.0.final_layer_norm.bias', 'front_end.model.encoder.layers.1.self_attn.k_proj.weight', 'front_end.model.encoder.layers.1.self_attn.k_proj.bias', 'front_end.model.encoder.layers.1.self_attn.v_proj.weight', 'front_end.model.encoder.layers.1.self_attn.v_proj.bias', 'front_end.model.encoder.layers.1.self_attn.q_proj.weight', 'front_end.model.encoder.layers.1.self_attn.q_proj.bias', 'front_end.model.encoder.layers.1.self_attn.out_proj.weight', 'front_end.model.encoder.layers.1.self_attn.out_proj.bias', 'front_end.model.encoder.layers.1.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.1.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.1.fc1.weight', 'front_end.model.encoder.layers.1.fc1.bias', 'front_end.model.encoder.layers.1.fc2.weight', 'front_end.model.encoder.layers.1.fc2.bias', 'front_end.model.encoder.layers.1.final_layer_norm.weight', 'front_end.model.encoder.layers.1.final_layer_norm.bias', 'front_end.model.encoder.layers.2.self_attn.k_proj.weight', 'front_end.model.encoder.layers.2.self_attn.k_proj.bias', 'front_end.model.encoder.layers.2.self_attn.v_proj.weight', 'front_end.model.encoder.layers.2.self_attn.v_proj.bias', 'front_end.model.encoder.layers.2.self_attn.q_proj.weight', 'front_end.model.encoder.layers.2.self_attn.q_proj.bias', 'front_end.model.encoder.layers.2.self_attn.out_proj.weight', 'front_end.model.encoder.layers.2.self_attn.out_proj.bias', 'front_end.model.encoder.layers.2.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.2.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.2.fc1.weight', 'front_end.model.encoder.layers.2.fc1.bias', 'front_end.model.encoder.layers.2.fc2.weight', 'front_end.model.encoder.layers.2.fc2.bias', 'front_end.model.encoder.layers.2.final_layer_norm.weight', 'front_end.model.encoder.layers.2.final_layer_norm.bias', 'front_end.model.encoder.layers.3.self_attn.k_proj.weight', 'front_end.model.encoder.layers.3.self_attn.k_proj.bias', 'front_end.model.encoder.layers.3.self_attn.v_proj.weight', 'front_end.model.encoder.layers.3.self_attn.v_proj.bias', 'front_end.model.encoder.layers.3.self_attn.q_proj.weight', 'front_end.model.encoder.layers.3.self_attn.q_proj.bias', 'front_end.model.encoder.layers.3.self_attn.out_proj.weight', 'front_end.model.encoder.layers.3.self_attn.out_proj.bias', 'front_end.model.encoder.layers.3.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.3.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.3.fc1.weight', 'front_end.model.encoder.layers.3.fc1.bias', 'front_end.model.encoder.layers.3.fc2.weight', 'front_end.model.encoder.layers.3.fc2.bias', 'front_end.model.encoder.layers.3.final_layer_norm.weight', 'front_end.model.encoder.layers.3.final_layer_norm.bias', 'front_end.model.encoder.layers.4.self_attn.k_proj.weight', 'front_end.model.encoder.layers.4.self_attn.k_proj.bias', 'front_end.model.encoder.layers.4.self_attn.v_proj.weight', 'front_end.model.encoder.layers.4.self_attn.v_proj.bias', 'front_end.model.encoder.layers.4.self_attn.q_proj.weight', 'front_end.model.encoder.layers.4.self_attn.q_proj.bias', 'front_end.model.encoder.layers.4.self_attn.out_proj.weight', 'front_end.model.encoder.layers.4.self_attn.out_proj.bias', 'front_end.model.encoder.layers.4.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.4.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.4.fc1.weight', 'front_end.model.encoder.layers.4.fc1.bias', 'front_end.model.encoder.layers.4.fc2.weight', 'front_end.model.encoder.layers.4.fc2.bias', 'front_end.model.encoder.layers.4.final_layer_norm.weight', 'front_end.model.encoder.layers.4.final_layer_norm.bias', 'front_end.model.encoder.layers.5.self_attn.k_proj.weight', 'front_end.model.encoder.layers.5.self_attn.k_proj.bias', 'front_end.model.encoder.layers.5.self_attn.v_proj.weight', 'front_end.model.encoder.layers.5.self_attn.v_proj.bias', 'front_end.model.encoder.layers.5.self_attn.q_proj.weight', 'front_end.model.encoder.layers.5.self_attn.q_proj.bias', 'front_end.model.encoder.layers.5.self_attn.out_proj.weight', 'front_end.model.encoder.layers.5.self_attn.out_proj.bias', 'front_end.model.encoder.layers.5.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.5.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.5.fc1.weight', 'front_end.model.encoder.layers.5.fc1.bias', 'front_end.model.encoder.layers.5.fc2.weight', 'front_end.model.encoder.layers.5.fc2.bias', 'front_end.model.encoder.layers.5.final_layer_norm.weight', 'front_end.model.encoder.layers.5.final_layer_norm.bias', 'front_end.model.encoder.layers.6.self_attn.k_proj.weight', 'front_end.model.encoder.layers.6.self_attn.k_proj.bias', 'front_end.model.encoder.layers.6.self_attn.v_proj.weight', 'front_end.model.encoder.layers.6.self_attn.v_proj.bias', 'front_end.model.encoder.layers.6.self_attn.q_proj.weight', 'front_end.model.encoder.layers.6.self_attn.q_proj.bias', 'front_end.model.encoder.layers.6.self_attn.out_proj.weight', 'front_end.model.encoder.layers.6.self_attn.out_proj.bias', 'front_end.model.encoder.layers.6.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.6.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.6.fc1.weight', 'front_end.model.encoder.layers.6.fc1.bias', 'front_end.model.encoder.layers.6.fc2.weight', 'front_end.model.encoder.layers.6.fc2.bias', 'front_end.model.encoder.layers.6.final_layer_norm.weight', 'front_end.model.encoder.layers.6.final_layer_norm.bias', 'front_end.model.encoder.layers.7.self_attn.k_proj.weight', 'front_end.model.encoder.layers.7.self_attn.k_proj.bias', 'front_end.model.encoder.layers.7.self_attn.v_proj.weight', 'front_end.model.encoder.layers.7.self_attn.v_proj.bias', 'front_end.model.encoder.layers.7.self_attn.q_proj.weight', 'front_end.model.encoder.layers.7.self_attn.q_proj.bias', 'front_end.model.encoder.layers.7.self_attn.out_proj.weight', 'front_end.model.encoder.layers.7.self_attn.out_proj.bias', 'front_end.model.encoder.layers.7.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.7.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.7.fc1.weight', 'front_end.model.encoder.layers.7.fc1.bias', 'front_end.model.encoder.layers.7.fc2.weight', 'front_end.model.encoder.layers.7.fc2.bias', 'front_end.model.encoder.layers.7.final_layer_norm.weight', 'front_end.model.encoder.layers.7.final_layer_norm.bias', 'front_end.model.encoder.layers.8.self_attn.k_proj.weight', 'front_end.model.encoder.layers.8.self_attn.k_proj.bias', 'front_end.model.encoder.layers.8.self_attn.v_proj.weight', 'front_end.model.encoder.layers.8.self_attn.v_proj.bias', 'front_end.model.encoder.layers.8.self_attn.q_proj.weight', 'front_end.model.encoder.layers.8.self_attn.q_proj.bias', 'front_end.model.encoder.layers.8.self_attn.out_proj.weight', 'front_end.model.encoder.layers.8.self_attn.out_proj.bias', 'front_end.model.encoder.layers.8.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.8.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.8.fc1.weight', 'front_end.model.encoder.layers.8.fc1.bias', 'front_end.model.encoder.layers.8.fc2.weight', 'front_end.model.encoder.layers.8.fc2.bias', 'front_end.model.encoder.layers.8.final_layer_norm.weight', 'front_end.model.encoder.layers.8.final_layer_norm.bias', 'front_end.model.encoder.layers.9.self_attn.k_proj.weight', 'front_end.model.encoder.layers.9.self_attn.k_proj.bias', 'front_end.model.encoder.layers.9.self_attn.v_proj.weight', 'front_end.model.encoder.layers.9.self_attn.v_proj.bias', 'front_end.model.encoder.layers.9.self_attn.q_proj.weight', 'front_end.model.encoder.layers.9.self_attn.q_proj.bias', 'front_end.model.encoder.layers.9.self_attn.out_proj.weight', 'front_end.model.encoder.layers.9.self_attn.out_proj.bias', 'front_end.model.encoder.layers.9.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.9.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.9.fc1.weight', 'front_end.model.encoder.layers.9.fc1.bias', 'front_end.model.encoder.layers.9.fc2.weight', 'front_end.model.encoder.layers.9.fc2.bias', 'front_end.model.encoder.layers.9.final_layer_norm.weight', 'front_end.model.encoder.layers.9.final_layer_norm.bias', 'front_end.model.encoder.layers.10.self_attn.k_proj.weight', 'front_end.model.encoder.layers.10.self_attn.k_proj.bias', 'front_end.model.encoder.layers.10.self_attn.v_proj.weight', 'front_end.model.encoder.layers.10.self_attn.v_proj.bias', 'front_end.model.encoder.layers.10.self_attn.q_proj.weight', 'front_end.model.encoder.layers.10.self_attn.q_proj.bias', 'front_end.model.encoder.layers.10.self_attn.out_proj.weight', 'front_end.model.encoder.layers.10.self_attn.out_proj.bias', 'front_end.model.encoder.layers.10.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.10.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.10.fc1.weight', 'front_end.model.encoder.layers.10.fc1.bias', 'front_end.model.encoder.layers.10.fc2.weight', 'front_end.model.encoder.layers.10.fc2.bias', 'front_end.model.encoder.layers.10.final_layer_norm.weight', 'front_end.model.encoder.layers.10.final_layer_norm.bias', 'front_end.model.encoder.layers.11.self_attn.k_proj.weight', 'front_end.model.encoder.layers.11.self_attn.k_proj.bias', 'front_end.model.encoder.layers.11.self_attn.v_proj.weight', 'front_end.model.encoder.layers.11.self_attn.v_proj.bias', 'front_end.model.encoder.layers.11.self_attn.q_proj.weight', 'front_end.model.encoder.layers.11.self_attn.q_proj.bias', 'front_end.model.encoder.layers.11.self_attn.out_proj.weight', 'front_end.model.encoder.layers.11.self_attn.out_proj.bias', 'front_end.model.encoder.layers.11.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.11.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.11.fc1.weight', 'front_end.model.encoder.layers.11.fc1.bias', 'front_end.model.encoder.layers.11.fc2.weight', 'front_end.model.encoder.layers.11.fc2.bias', 'front_end.model.encoder.layers.11.final_layer_norm.weight', 'front_end.model.encoder.layers.11.final_layer_norm.bias', 'front_end.model.encoder.layers.12.self_attn.k_proj.weight', 'front_end.model.encoder.layers.12.self_attn.k_proj.bias', 'front_end.model.encoder.layers.12.self_attn.v_proj.weight', 'front_end.model.encoder.layers.12.self_attn.v_proj.bias', 'front_end.model.encoder.layers.12.self_attn.q_proj.weight', 'front_end.model.encoder.layers.12.self_attn.q_proj.bias', 'front_end.model.encoder.layers.12.self_attn.out_proj.weight', 'front_end.model.encoder.layers.12.self_attn.out_proj.bias', 'front_end.model.encoder.layers.12.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.12.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.12.fc1.weight', 'front_end.model.encoder.layers.12.fc1.bias', 'front_end.model.encoder.layers.12.fc2.weight', 'front_end.model.encoder.layers.12.fc2.bias', 'front_end.model.encoder.layers.12.final_layer_norm.weight', 'front_end.model.encoder.layers.12.final_layer_norm.bias', 'front_end.model.encoder.layers.13.self_attn.k_proj.weight', 'front_end.model.encoder.layers.13.self_attn.k_proj.bias', 'front_end.model.encoder.layers.13.self_attn.v_proj.weight', 'front_end.model.encoder.layers.13.self_attn.v_proj.bias', 'front_end.model.encoder.layers.13.self_attn.q_proj.weight', 'front_end.model.encoder.layers.13.self_attn.q_proj.bias', 'front_end.model.encoder.layers.13.self_attn.out_proj.weight', 'front_end.model.encoder.layers.13.self_attn.out_proj.bias', 'front_end.model.encoder.layers.13.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.13.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.13.fc1.weight', 'front_end.model.encoder.layers.13.fc1.bias', 'front_end.model.encoder.layers.13.fc2.weight', 'front_end.model.encoder.layers.13.fc2.bias', 'front_end.model.encoder.layers.13.final_layer_norm.weight', 'front_end.model.encoder.layers.13.final_layer_norm.bias', 'front_end.model.encoder.layers.14.self_attn.k_proj.weight', 'front_end.model.encoder.layers.14.self_attn.k_proj.bias', 'front_end.model.encoder.layers.14.self_attn.v_proj.weight', 'front_end.model.encoder.layers.14.self_attn.v_proj.bias', 'front_end.model.encoder.layers.14.self_attn.q_proj.weight', 'front_end.model.encoder.layers.14.self_attn.q_proj.bias', 'front_end.model.encoder.layers.14.self_attn.out_proj.weight', 'front_end.model.encoder.layers.14.self_attn.out_proj.bias', 'front_end.model.encoder.layers.14.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.14.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.14.fc1.weight', 'front_end.model.encoder.layers.14.fc1.bias', 'front_end.model.encoder.layers.14.fc2.weight', 'front_end.model.encoder.layers.14.fc2.bias', 'front_end.model.encoder.layers.14.final_layer_norm.weight', 'front_end.model.encoder.layers.14.final_layer_norm.bias', 'front_end.model.encoder.layers.15.self_attn.k_proj.weight', 'front_end.model.encoder.layers.15.self_attn.k_proj.bias', 'front_end.model.encoder.layers.15.self_attn.v_proj.weight', 'front_end.model.encoder.layers.15.self_attn.v_proj.bias', 'front_end.model.encoder.layers.15.self_attn.q_proj.weight', 'front_end.model.encoder.layers.15.self_attn.q_proj.bias', 'front_end.model.encoder.layers.15.self_attn.out_proj.weight', 'front_end.model.encoder.layers.15.self_attn.out_proj.bias', 'front_end.model.encoder.layers.15.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.15.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.15.fc1.weight', 'front_end.model.encoder.layers.15.fc1.bias', 'front_end.model.encoder.layers.15.fc2.weight', 'front_end.model.encoder.layers.15.fc2.bias', 'front_end.model.encoder.layers.15.final_layer_norm.weight', 'front_end.model.encoder.layers.15.final_layer_norm.bias', 'front_end.model.encoder.layers.16.self_attn.k_proj.weight', 'front_end.model.encoder.layers.16.self_attn.k_proj.bias', 'front_end.model.encoder.layers.16.self_attn.v_proj.weight', 'front_end.model.encoder.layers.16.self_attn.v_proj.bias', 'front_end.model.encoder.layers.16.self_attn.q_proj.weight', 'front_end.model.encoder.layers.16.self_attn.q_proj.bias', 'front_end.model.encoder.layers.16.self_attn.out_proj.weight', 'front_end.model.encoder.layers.16.self_attn.out_proj.bias', 'front_end.model.encoder.layers.16.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.16.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.16.fc1.weight', 'front_end.model.encoder.layers.16.fc1.bias', 'front_end.model.encoder.layers.16.fc2.weight', 'front_end.model.encoder.layers.16.fc2.bias', 'front_end.model.encoder.layers.16.final_layer_norm.weight', 'front_end.model.encoder.layers.16.final_layer_norm.bias', 'front_end.model.encoder.layers.17.self_attn.k_proj.weight', 'front_end.model.encoder.layers.17.self_attn.k_proj.bias', 'front_end.model.encoder.layers.17.self_attn.v_proj.weight', 'front_end.model.encoder.layers.17.self_attn.v_proj.bias', 'front_end.model.encoder.layers.17.self_attn.q_proj.weight', 'front_end.model.encoder.layers.17.self_attn.q_proj.bias', 'front_end.model.encoder.layers.17.self_attn.out_proj.weight', 'front_end.model.encoder.layers.17.self_attn.out_proj.bias', 'front_end.model.encoder.layers.17.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.17.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.17.fc1.weight', 'front_end.model.encoder.layers.17.fc1.bias', 'front_end.model.encoder.layers.17.fc2.weight', 'front_end.model.encoder.layers.17.fc2.bias', 'front_end.model.encoder.layers.17.final_layer_norm.weight', 'front_end.model.encoder.layers.17.final_layer_norm.bias', 'front_end.model.encoder.layers.18.self_attn.k_proj.weight', 'front_end.model.encoder.layers.18.self_attn.k_proj.bias', 'front_end.model.encoder.layers.18.self_attn.v_proj.weight', 'front_end.model.encoder.layers.18.self_attn.v_proj.bias', 'front_end.model.encoder.layers.18.self_attn.q_proj.weight', 'front_end.model.encoder.layers.18.self_attn.q_proj.bias', 'front_end.model.encoder.layers.18.self_attn.out_proj.weight', 'front_end.model.encoder.layers.18.self_attn.out_proj.bias', 'front_end.model.encoder.layers.18.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.18.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.18.fc1.weight', 'front_end.model.encoder.layers.18.fc1.bias', 'front_end.model.encoder.layers.18.fc2.weight', 'front_end.model.encoder.layers.18.fc2.bias', 'front_end.model.encoder.layers.18.final_layer_norm.weight', 'front_end.model.encoder.layers.18.final_layer_norm.bias', 'front_end.model.encoder.layers.19.self_attn.k_proj.weight', 'front_end.model.encoder.layers.19.self_attn.k_proj.bias', 'front_end.model.encoder.layers.19.self_attn.v_proj.weight', 'front_end.model.encoder.layers.19.self_attn.v_proj.bias', 'front_end.model.encoder.layers.19.self_attn.q_proj.weight', 'front_end.model.encoder.layers.19.self_attn.q_proj.bias', 'front_end.model.encoder.layers.19.self_attn.out_proj.weight', 'front_end.model.encoder.layers.19.self_attn.out_proj.bias', 'front_end.model.encoder.layers.19.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.19.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.19.fc1.weight', 'front_end.model.encoder.layers.19.fc1.bias', 'front_end.model.encoder.layers.19.fc2.weight', 'front_end.model.encoder.layers.19.fc2.bias', 'front_end.model.encoder.layers.19.final_layer_norm.weight', 'front_end.model.encoder.layers.19.final_layer_norm.bias', 'front_end.model.encoder.layers.20.self_attn.k_proj.weight', 'front_end.model.encoder.layers.20.self_attn.k_proj.bias', 'front_end.model.encoder.layers.20.self_attn.v_proj.weight', 'front_end.model.encoder.layers.20.self_attn.v_proj.bias', 'front_end.model.encoder.layers.20.self_attn.q_proj.weight', 'front_end.model.encoder.layers.20.self_attn.q_proj.bias', 'front_end.model.encoder.layers.20.self_attn.out_proj.weight', 'front_end.model.encoder.layers.20.self_attn.out_proj.bias', 'front_end.model.encoder.layers.20.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.20.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.20.fc1.weight', 'front_end.model.encoder.layers.20.fc1.bias', 'front_end.model.encoder.layers.20.fc2.weight', 'front_end.model.encoder.layers.20.fc2.bias', 'front_end.model.encoder.layers.20.final_layer_norm.weight', 'front_end.model.encoder.layers.20.final_layer_norm.bias', 'front_end.model.encoder.layers.21.self_attn.k_proj.weight', 'front_end.model.encoder.layers.21.self_attn.k_proj.bias', 'front_end.model.encoder.layers.21.self_attn.v_proj.weight', 'front_end.model.encoder.layers.21.self_attn.v_proj.bias', 'front_end.model.encoder.layers.21.self_attn.q_proj.weight', 'front_end.model.encoder.layers.21.self_attn.q_proj.bias', 'front_end.model.encoder.layers.21.self_attn.out_proj.weight', 'front_end.model.encoder.layers.21.self_attn.out_proj.bias', 'front_end.model.encoder.layers.21.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.21.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.21.fc1.weight', 'front_end.model.encoder.layers.21.fc1.bias', 'front_end.model.encoder.layers.21.fc2.weight', 'front_end.model.encoder.layers.21.fc2.bias', 'front_end.model.encoder.layers.21.final_layer_norm.weight', 'front_end.model.encoder.layers.21.final_layer_norm.bias', 'front_end.model.encoder.layers.22.self_attn.k_proj.weight', 'front_end.model.encoder.layers.22.self_attn.k_proj.bias', 'front_end.model.encoder.layers.22.self_attn.v_proj.weight', 'front_end.model.encoder.layers.22.self_attn.v_proj.bias', 'front_end.model.encoder.layers.22.self_attn.q_proj.weight', 'front_end.model.encoder.layers.22.self_attn.q_proj.bias', 'front_end.model.encoder.layers.22.self_attn.out_proj.weight', 'front_end.model.encoder.layers.22.self_attn.out_proj.bias', 'front_end.model.encoder.layers.22.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.22.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.22.fc1.weight', 'front_end.model.encoder.layers.22.fc1.bias', 'front_end.model.encoder.layers.22.fc2.weight', 'front_end.model.encoder.layers.22.fc2.bias', 'front_end.model.encoder.layers.22.final_layer_norm.weight', 'front_end.model.encoder.layers.22.final_layer_norm.bias', 'front_end.model.encoder.layers.23.self_attn.k_proj.weight', 'front_end.model.encoder.layers.23.self_attn.k_proj.bias', 'front_end.model.encoder.layers.23.self_attn.v_proj.weight', 'front_end.model.encoder.layers.23.self_attn.v_proj.bias', 'front_end.model.encoder.layers.23.self_attn.q_proj.weight', 'front_end.model.encoder.layers.23.self_attn.q_proj.bias', 'front_end.model.encoder.layers.23.self_attn.out_proj.weight', 'front_end.model.encoder.layers.23.self_attn.out_proj.bias', 'front_end.model.encoder.layers.23.self_attn_layer_norm.weight', 'front_end.model.encoder.layers.23.self_attn_layer_norm.bias', 'front_end.model.encoder.layers.23.fc1.weight', 'front_end.model.encoder.layers.23.fc1.bias', 'front_end.model.encoder.layers.23.fc2.weight', 'front_end.model.encoder.layers.23.fc2.bias', 'front_end.model.encoder.layers.23.final_layer_norm.weight', 'front_end.model.encoder.layers.23.final_layer_norm.bias', 'front_end.model.encoder.layer_norm.weight', 'front_end.model.encoder.layer_norm.bias', 'front_end.model.layer_norm.weight', 'front_end.model.layer_norm.bias', 'front_end.model.final_proj.weight', 'front_end.model.final_proj.bias', 'LL.weight', 'LL.bias', 'first_bn.weight', 'first_bn.bias', 'first_bn.running_mean', 'first_bn.running_var', 'first_bn.num_batches_tracked', 'backend.positional_emb', 'backend.class_token', 'backend.encoder_blocks.0.ff1.fn.fn.net.0.weight', 'backend.encoder_blocks.0.ff1.fn.fn.net.0.bias', 'backend.encoder_blocks.0.ff1.fn.fn.net.3.weight', 'backend.encoder_blocks.0.ff1.fn.fn.net.3.bias', 'backend.encoder_blocks.0.ff1.fn.norm.weight', 'backend.encoder_blocks.0.ff1.fn.norm.bias', 'backend.encoder_blocks.0.attn.fn.pos_embed', 'backend.encoder_blocks.0.attn.fn.qkv.weight', 'backend.encoder_blocks.0.attn.fn.proj.weight', 'backend.encoder_blocks.0.attn.fn.proj.bias', 'backend.encoder_blocks.0.attn.fn.ht_proj.weight', 'backend.encoder_blocks.0.attn.fn.ht_proj.bias', 'backend.encoder_blocks.0.attn.fn.ht_norm.weight', 'backend.encoder_blocks.0.attn.fn.ht_norm.bias', 'backend.encoder_blocks.0.attn.norm.weight', 'backend.encoder_blocks.0.attn.norm.bias', 'backend.encoder_blocks.0.conv.net.0.weight', 'backend.encoder_blocks.0.conv.net.0.bias', 'backend.encoder_blocks.0.conv.net.2.weight', 'backend.encoder_blocks.0.conv.net.2.bias', 'backend.encoder_blocks.0.conv.net.4.conv.weight', 'backend.encoder_blocks.0.conv.net.4.conv.bias', 'backend.encoder_blocks.0.conv.net.5.weight', 'backend.encoder_blocks.0.conv.net.5.bias', 'backend.encoder_blocks.0.conv.net.5.running_mean', 'backend.encoder_blocks.0.conv.net.5.running_var', 'backend.encoder_blocks.0.conv.net.5.num_batches_tracked', 'backend.encoder_blocks.0.conv.net.7.weight', 'backend.encoder_blocks.0.conv.net.7.bias', 'backend.encoder_blocks.0.ff2.fn.fn.net.0.weight', 'backend.encoder_blocks.0.ff2.fn.fn.net.0.bias', 'backend.encoder_blocks.0.ff2.fn.fn.net.3.weight', 'backend.encoder_blocks.0.ff2.fn.fn.net.3.bias', 'backend.encoder_blocks.0.ff2.fn.norm.weight', 'backend.encoder_blocks.0.ff2.fn.norm.bias', 'backend.encoder_blocks.0.post_norm.weight', 'backend.encoder_blocks.0.post_norm.bias', 'backend.encoder_blocks.1.ff1.fn.fn.net.0.weight', 'backend.encoder_blocks.1.ff1.fn.fn.net.0.bias', 'backend.encoder_blocks.1.ff1.fn.fn.net.3.weight', 'backend.encoder_blocks.1.ff1.fn.fn.net.3.bias', 'backend.encoder_blocks.1.ff1.fn.norm.weight', 'backend.encoder_blocks.1.ff1.fn.norm.bias', 'backend.encoder_blocks.1.attn.fn.pos_embed', 'backend.encoder_blocks.1.attn.fn.qkv.weight', 'backend.encoder_blocks.1.attn.fn.proj.weight', 'backend.encoder_blocks.1.attn.fn.proj.bias', 'backend.encoder_blocks.1.attn.fn.ht_proj.weight', 'backend.encoder_blocks.1.attn.fn.ht_proj.bias', 'backend.encoder_blocks.1.attn.fn.ht_norm.weight', 'backend.encoder_blocks.1.attn.fn.ht_norm.bias', 'backend.encoder_blocks.1.attn.norm.weight', 'backend.encoder_blocks.1.attn.norm.bias', 'backend.encoder_blocks.1.conv.net.0.weight', 'backend.encoder_blocks.1.conv.net.0.bias', 'backend.encoder_blocks.1.conv.net.2.weight', 'backend.encoder_blocks.1.conv.net.2.bias', 'backend.encoder_blocks.1.conv.net.4.conv.weight', 'backend.encoder_blocks.1.conv.net.4.conv.bias', 'backend.encoder_blocks.1.conv.net.5.weight', 'backend.encoder_blocks.1.conv.net.5.bias', 'backend.encoder_blocks.1.conv.net.5.running_mean', 'backend.encoder_blocks.1.conv.net.5.running_var', 'backend.encoder_blocks.1.conv.net.5.num_batches_tracked', 'backend.encoder_blocks.1.conv.net.7.weight', 'backend.encoder_blocks.1.conv.net.7.bias', 'backend.encoder_blocks.1.ff2.fn.fn.net.0.weight', 'backend.encoder_blocks.1.ff2.fn.fn.net.0.bias', 'backend.encoder_blocks.1.ff2.fn.fn.net.3.weight', 'backend.encoder_blocks.1.ff2.fn.fn.net.3.bias', 'backend.encoder_blocks.1.ff2.fn.norm.weight', 'backend.encoder_blocks.1.ff2.fn.norm.bias', 'backend.encoder_blocks.1.post_norm.weight', 'backend.encoder_blocks.1.post_norm.bias', 'backend.encoder_blocks.2.ff1.fn.fn.net.0.weight', 'backend.encoder_blocks.2.ff1.fn.fn.net.0.bias', 'backend.encoder_blocks.2.ff1.fn.fn.net.3.weight', 'backend.encoder_blocks.2.ff1.fn.fn.net.3.bias', 'backend.encoder_blocks.2.ff1.fn.norm.weight', 'backend.encoder_blocks.2.ff1.fn.norm.bias', 'backend.encoder_blocks.2.attn.fn.pos_embed', 'backend.encoder_blocks.2.attn.fn.qkv.weight', 'backend.encoder_blocks.2.attn.fn.proj.weight', 'backend.encoder_blocks.2.attn.fn.proj.bias', 'backend.encoder_blocks.2.attn.fn.ht_proj.weight', 'backend.encoder_blocks.2.attn.fn.ht_proj.bias', 'backend.encoder_blocks.2.attn.fn.ht_norm.weight', 'backend.encoder_blocks.2.attn.fn.ht_norm.bias', 'backend.encoder_blocks.2.attn.norm.weight', 'backend.encoder_blocks.2.attn.norm.bias', 'backend.encoder_blocks.2.conv.net.0.weight', 'backend.encoder_blocks.2.conv.net.0.bias', 'backend.encoder_blocks.2.conv.net.2.weight', 'backend.encoder_blocks.2.conv.net.2.bias', 'backend.encoder_blocks.2.conv.net.4.conv.weight', 'backend.encoder_blocks.2.conv.net.4.conv.bias', 'backend.encoder_blocks.2.conv.net.5.weight', 'backend.encoder_blocks.2.conv.net.5.bias', 'backend.encoder_blocks.2.conv.net.5.running_mean', 'backend.encoder_blocks.2.conv.net.5.running_var', 'backend.encoder_blocks.2.conv.net.5.num_batches_tracked', 'backend.encoder_blocks.2.conv.net.7.weight', 'backend.encoder_blocks.2.conv.net.7.bias', 'backend.encoder_blocks.2.ff2.fn.fn.net.0.weight', 'backend.encoder_blocks.2.ff2.fn.fn.net.0.bias', 'backend.encoder_blocks.2.ff2.fn.fn.net.3.weight', 'backend.encoder_blocks.2.ff2.fn.fn.net.3.bias', 'backend.encoder_blocks.2.ff2.fn.norm.weight', 'backend.encoder_blocks.2.ff2.fn.norm.bias', 'backend.encoder_blocks.2.post_norm.weight', 'backend.encoder_blocks.2.post_norm.bias', 'backend.encoder_blocks.3.ff1.fn.fn.net.0.weight', 'backend.encoder_blocks.3.ff1.fn.fn.net.0.bias', 'backend.encoder_blocks.3.ff1.fn.fn.net.3.weight', 'backend.encoder_blocks.3.ff1.fn.fn.net.3.bias', 'backend.encoder_blocks.3.ff1.fn.norm.weight', 'backend.encoder_blocks.3.ff1.fn.norm.bias', 'backend.encoder_blocks.3.attn.fn.pos_embed', 'backend.encoder_blocks.3.attn.fn.qkv.weight', 'backend.encoder_blocks.3.attn.fn.proj.weight', 'backend.encoder_blocks.3.attn.fn.proj.bias', 'backend.encoder_blocks.3.attn.fn.ht_proj.weight', 'backend.encoder_blocks.3.attn.fn.ht_proj.bias', 'backend.encoder_blocks.3.attn.fn.ht_norm.weight', 'backend.encoder_blocks.3.attn.fn.ht_norm.bias', 'backend.encoder_blocks.3.attn.norm.weight', 'backend.encoder_blocks.3.attn.norm.bias', 'backend.encoder_blocks.3.conv.net.0.weight', 'backend.encoder_blocks.3.conv.net.0.bias', 'backend.encoder_blocks.3.conv.net.2.weight', 'backend.encoder_blocks.3.conv.net.2.bias', 'backend.encoder_blocks.3.conv.net.4.conv.weight', 'backend.encoder_blocks.3.conv.net.4.conv.bias', 'backend.encoder_blocks.3.conv.net.5.weight', 'backend.encoder_blocks.3.conv.net.5.bias', 'backend.encoder_blocks.3.conv.net.5.running_mean', 'backend.encoder_blocks.3.conv.net.5.running_var', 'backend.encoder_blocks.3.conv.net.5.num_batches_tracked', 'backend.encoder_blocks.3.conv.net.7.weight', 'backend.encoder_blocks.3.conv.net.7.bias', 'backend.encoder_blocks.3.ff2.fn.fn.net.0.weight', 'backend.encoder_blocks.3.ff2.fn.fn.net.0.bias', 'backend.encoder_blocks.3.ff2.fn.fn.net.3.weight', 'backend.encoder_blocks.3.ff2.fn.fn.net.3.bias', 'backend.encoder_blocks.3.ff2.fn.norm.weight', 'backend.encoder_blocks.3.ff2.fn.norm.bias', 'backend.encoder_blocks.3.post_norm.weight', 'backend.encoder_blocks.3.post_norm.bias', 'backend.fc5.weight', 'backend.fc5.bias'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ckpt = torch.load(\n",
    "    '/datad/pretrained/AudioDeepfakeCMs/S_241214_conf-1.pth')\n",
    "new_ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2523835/1426476301.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (front_end): SSLModel(\n",
       "    (model): Wav2Vec2Model(\n",
       "      (feature_extractor): ConvFeatureExtractionModel(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Sequential(\n",
       "              (0): TransposeLast()\n",
       "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): TransposeLast()\n",
       "            )\n",
       "            (3): GELU(approximate='none')\n",
       "          )\n",
       "          (1-4): 4 x Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Sequential(\n",
       "              (0): TransposeLast()\n",
       "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): TransposeLast()\n",
       "            )\n",
       "            (3): GELU(approximate='none')\n",
       "          )\n",
       "          (5-6): 2 x Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Sequential(\n",
       "              (0): TransposeLast()\n",
       "              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): TransposeLast()\n",
       "            )\n",
       "            (3): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout_input): Dropout(p=0.0, inplace=False)\n",
       "      (dropout_features): Dropout(p=0.0, inplace=False)\n",
       "      (quantizer): GumbelVectorQuantizer(\n",
       "        (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
       "      )\n",
       "      (project_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (encoder): TransformerEncoder(\n",
       "        (pos_conv): Sequential(\n",
       "          (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (1): SamePad()\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.0, inplace=False)\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (final_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (LL): Linear(in_features=1024, out_features=144, bias=True)\n",
       "  (first_bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (selu): SELU(inplace=True)\n",
       "  (backend): MyConformer(\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-3): 4 x ConformerBlock(\n",
       "        (ff1): Scale(\n",
       "          (fn): PreNorm(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=144, out_features=576, bias=True)\n",
       "                (1): Swish()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=576, out_features=144, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (attn): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (qkv): Linear(in_features=144, out_features=432, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (ht_proj): Linear(in_features=36, out_features=144, bias=True)\n",
       "            (ht_norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (conv): ConformerConvModule(\n",
       "          (net): Sequential(\n",
       "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Rearrange('b n c -> b c n')\n",
       "            (2): Conv1d(144, 576, kernel_size=(1,), stride=(1,))\n",
       "            (3): GLU()\n",
       "            (4): DepthWiseConv1d(\n",
       "              (conv): Conv1d(288, 288, kernel_size=(31,), stride=(1,), groups=288)\n",
       "            )\n",
       "            (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (6): Swish()\n",
       "            (7): Conv1d(288, 144, kernel_size=(1,), stride=(1,))\n",
       "            (8): Rearrange('b c n -> b n c')\n",
       "            (9): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ff2): Scale(\n",
       "          (fn): PreNorm(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=144, out_features=576, bias=True)\n",
       "                (1): Swish()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=576, out_features=144, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (fc5): Linear(in_features=144, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
