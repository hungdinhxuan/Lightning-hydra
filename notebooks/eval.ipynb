{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InTheWild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AASISST-SSL Multiview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/datad/Datasets/in_the_wild.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAISST-SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/inthewild\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/datad/Datasets/in_the_wild.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/tcm_add/Scores/avg_5_best_baseline_itw_var.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/Lightning-hydra/logs/itw_eval_aasistssl_multiview_conf-2-var_evalvar.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/Lightning-hydra/logs/eval/in_the_wild_xlsr_aasist_multiview_conf-2_avg_var.txt\",\n",
    "      \"/datad/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/tcm_add/Scores/avg_5_best_4_varmultiview_itw_var.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSD_Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_4_multiview_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_baseline_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_1s.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_var.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "import os\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_4_train_multiview_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL Multiview for DSD Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformertcm Multiview for DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/conformertcm_multiview\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformertcm base for DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/3s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/1s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/2s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/3s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-corpus ConfomerTCM + Multiview + trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    #cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_4s.txt\"\n",
    "print(eval_to_score_file(score_file, \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file,\n",
    "          \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_large_corpus_ori_random_start.txt\"\n",
    "print(eval_to_score_file(score_file, \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "print(eval_to_score_file(\"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_var_itw.txt\",\n",
    "      \"/datad/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConformerTCM + train largr corpus + multiview + trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme2/Datasets/MLAAD/protocol_eval.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    # pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt\n",
    "    res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "    print(\"Accuracy: {:.2f}\".format(accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/home/hungdx/MLAAD/protocol_lts.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "#for score_file in list_dir:\n",
    "score_file = \"/home/hungdx/mlaad_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_3s.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/home/hungdx/large_corpus_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_2s.txt\"\n",
    "print(eval_to_score_file(score_file, \"/data/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDT on large-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.15174190132025808\tthreshold: -0.4258384108543396\n",
      "\n",
      "\n",
      "\n",
      "5.042064189910889 -5.569249629974365\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'spoof_score', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return cm_scores, eer_cm, th\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/train/eval/cnsl/large_corpus_xlsr_conformertcm_multiview_large_corpus_v1_4s.txt\"\n",
    "res_df, eer, th = eval_to_score_file(score_file, \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/protocol.txt\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "max_score = max(res_df['score'])\n",
    "min_score = min(res_df['score'])\n",
    "print(max_score, min_score)\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "bona = res_df[(res_df['label'] == 'bonafide')]['score']\n",
    "spoof = res_df[(res_df['label'] == 'spoof')]['score']\n",
    "\n",
    "# Plotting the distributions as line chart histograms\n",
    "plt.hist(bona, bins=100, density=True, cumulative=False, histtype='step', linewidth=1.5, label='Bonafide')\n",
    "plt.hist(spoof, bins=100, density=True, cumulative=False, histtype='step', linewidth=1.5, label='Spoofing')\n",
    "\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Scores')\n",
    "# plt.ylabel('score probability density plot')\n",
    "plt.title('EER = {:.2f}%'.format(eer*100), fontsize=16)\n",
    "# plt.legend(fontsize=14)\n",
    "# plt.stick_params(labelsize=16)\n",
    "\n",
    "# yaxis logit scale\n",
    "plt.yscale('symlog')\n",
    "\n",
    "# Adding a legend\n",
    "# plt.legend(loc='lower right')\n",
    "# Adding vertical lines\n",
    "plt.axvline(x=th, color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "# Set the x-axis tick positions and labels\n",
    "plt.xticks([-6,6], fontsize=16)\n",
    "plt.yticks([])\n",
    "plt.ylim(0,5)\n",
    "\n",
    "# plt.rcParams.update({'font.size': 16})\n",
    "# Displaying the plot\n",
    "# plt.savefig(\"figs/1e-07_3_augall_asvspoof2019_resnet_nll_2l_epoch_49.png\", dpi=600)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDT + BT on large-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.23653145909564455\tthreshold: 1.133873462677002\n",
      "\n",
      "\n",
      "\n",
      "5.076810359954834 -4.980000972747803\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'spoof_score', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return cm_scores, eer_cm, th\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/train/eval/cnsl/large_corpus_xlsr_conformertcm_multiview_large_corpus_scl_4s.txt\"\n",
    "res_df, eer, th = eval_to_score_file(score_file, \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/protocol.txt\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "max_score = max(res_df['score'])\n",
    "min_score = min(res_df['score'])\n",
    "print(max_score, min_score)\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "bona = res_df[(res_df['label'] == 'bonafide')]['score']\n",
    "spoof = res_df[(res_df['label'] == 'spoof')]['score']\n",
    "\n",
    "# Plotting the distributions as line chart histograms\n",
    "plt.hist(bona, bins=100, density=True, cumulative=False, histtype='step', linewidth=1.5, label='Bonafide')\n",
    "plt.hist(spoof, bins=100, density=True, cumulative=False, histtype='step', linewidth=1.5, label='Spoofing')\n",
    "\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Scores')\n",
    "# plt.ylabel('score probability density plot')\n",
    "plt.title('EER = {:.2f}%'.format(eer*100), fontsize=16)\n",
    "# plt.legend(fontsize=14)\n",
    "# plt.stick_params(labelsize=16)\n",
    "\n",
    "# yaxis logit scale\n",
    "plt.yscale('symlog')\n",
    "\n",
    "# Adding a legend\n",
    "# plt.legend(loc='lower right')\n",
    "# Adding vertical lines\n",
    "plt.axvline(x=th, color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "# Set the x-axis tick positions and labels\n",
    "plt.xticks([-6,6], fontsize=16)\n",
    "plt.yticks([])\n",
    "plt.ylim(0,5)\n",
    "\n",
    "# plt.rcParams.update({'font.size': 16})\n",
    "# Displaying the plot\n",
    "# plt.savefig(\"figs/1e-07_3_augall_asvspoof2019_resnet_nll_2l_epoch_49.png\", dpi=600)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD_v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config1 + Nov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AASIST-SSL conf-2 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/mlaad_v5/xlsr_aasist_multiview_conf-2_2s.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConformerTCM baseline train LA19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/nvme1/hungdx/tcm_add/Scores/avg_5_best_ConformerTCM_baseline_train_LA19_eval_2s_mlaad_v5.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConformerTCM_MDT_train_LA19_eval_1s_mlaad_v5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/nvme1/hungdx/tcm_add/Scores/avg_5_best_4_ConformerTCM_MDT_train_LA19_eval_4s_mlaad_v5.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AASIST-SSL baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/nvme1/hungdx/tcm_add/Scores/Best_LA_model_for_DF_AASIST-SSL_baseline_train_LA19_eval_3s_mlaad_v5.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PartialSpoof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_conf-2_var.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 10.53683200443646\tthreshold: 3.185546875\n",
      "\n",
      "0.1053683200443646\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_conf-2_eval_3s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 14.009758943085304\tthreshold: 4.177801132202148\n",
      "\n",
      "0.14009758943085304\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/tcm_add/Scores/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_eval_3s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 14.26545078194924\tthreshold: 0.3583984375\n",
      "\n",
      "0.1426545078194924\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_conf-2_chunking_3s_overlap_1.5s.txt\"\n",
    "mode = 'mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/tcm_add/Scores/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_var_eval.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_epoch15_conf-2_1s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_conf-2_1s_random_start.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunking 1s overlap 0.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_overlap_0.5s_chunking.txt',\n",
    "                 sep=' ', header=None, names=['utt_id', 'bonafide_score'])\n",
    "\n",
    "# Extract the base utt_id by removing the chunk suffix\n",
    "df['base_utt_id'] = df['utt_id'].apply(\n",
    "    lambda x: '_'.join(x.split('___')[:-1]))\n",
    "\n",
    "# df2 = pd.read_csv('docs_eval_final/eval_3_augall_wavlm_conformertcm_res2net_seblock_sclnormal_large_corpus_epoch16_6s.txt', sep=' ', header=None, names=['utt_id', 'spoof_score', 'bonafide_score'])\n",
    "\n",
    "# ## Check which file is missing between df and df2\n",
    "# print(df2[~df2['utt_id'].isin(df['base_utt_id'])])\n",
    "\n",
    "# print(df['base_utt_id'].nunique())\n",
    "\n",
    "# Group by the base utt_id and calculate the average scores\n",
    "grouped_df = df.groupby('base_utt_id').agg(\n",
    "    {'bonafide_score': 'min'}).reset_index()\n",
    "\n",
    "# Rename the columns to match the final format\n",
    "grouped_df.rename(columns={'base_utt_id': 'utt_id'}, inplace=True)\n",
    "\n",
    "# grouped_df.head()\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "grouped_df.to_csv('/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_overlap_0.5s_chunking_min.txt',\n",
    "                  sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/partialspoof_xlsr_aasist_multiview_conf-2_mean_chunking_1s_overlap_0.5s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('/nvme1/hungdx/tcm_add/Scores/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_chunking_4s_overlap_2s.txt',\n",
    "                 sep=' ', header=None, names=['utt_id', 'bonafide_score'])\n",
    "\n",
    "# Extract the base utt_id by removing the chunk suffix\n",
    "df['base_utt_id'] = df['utt_id'].apply(\n",
    "    lambda x: '_'.join(x.split('_')[:-1]))\n",
    "\n",
    "# df2 = pd.read_csv('docs_eval_final/eval_3_augall_wavlm_conformertcm_res2net_seblock_sclnormal_large_corpus_epoch16_6s.txt', sep=' ', header=None, names=['utt_id', 'spoof_score', 'bonafide_score'])\n",
    "\n",
    "# ## Check which file is missing between df and df2\n",
    "# print(df2[~df2['utt_id'].isin(df['base_utt_id'])])\n",
    "\n",
    "# print(df['base_utt_id'].nunique())\n",
    "\n",
    "# Group by the base utt_id and calculate the average scores\n",
    "grouped_df = df.groupby('base_utt_id').agg(\n",
    "    {'bonafide_score': 'mean'}).reset_index()\n",
    "\n",
    "# Rename the columns to match the final format\n",
    "grouped_df.rename(columns={'base_utt_id': 'utt_id'}, inplace=True)\n",
    "\n",
    "# grouped_df.head()\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "grouped_df.to_csv('/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_mean_chunking_4s_overlap_2s.txt',\n",
    "                  sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_mean_chunking_1s_overlap_0.5s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunking 2s overlap 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/partialspoof_xlsr_aasist_multiview_conf-2_mean_chunking_2s_overlap_1s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_mean_chunking_2s_overlap_1s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking 4s overlap 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_mean_chunking_4s_overlap_2s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConformerTCM baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/partialspoof_baseline_conformertcm_train_la19_mean_chunking_4s_overlap_2_eval.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s random start inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-2_eval_1s_rs_lts.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-2_eval_1s_lts.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s mean chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-2_eval_1s_overlap_0.5s_chunking_mean.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s random start inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-3_eval_1s_rs_lts.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 1 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-3_eval_1s_lts.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-3_eval_1s_overlap_0.5s_chunking_mean.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_1s_overlap_0.5s_chunking_mean.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s rs inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_1s_rs.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 11.28509577000297\tthreshold: 5.129763126373291\n",
      "\n",
      "0.1128509577000297\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_4s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 10.143231239293574\tthreshold: 5.016756057739258\n",
      "\n",
      "0.10143231239293574\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_3s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 14.548312872320611\tthreshold: 4.678284645080566\n",
      "\n",
      "0.14548312872320612\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_2s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_overlap_0.5s_chunking_mean.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"#+\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_2s_overlap_1s_chunking.txt\"\n",
    "mode='mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "      mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_3s_overlap_1.5s_chunking.txt\"\n",
    "mode = 'mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_4s_overlap_2s_chunking.txt\"\n",
    "mode = 'min'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s random start inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_rs.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 8.742516980502653\tthreshold: 4.965228080749512\n",
      "\n",
      "0.08742516980502653\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_var.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_2s_overlap_1s_chunking.txt\"\n",
    "mode = 'mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 15.026533029722906\tthreshold: -0.03892254829406738\n",
      "\n",
      "0.15026533029722905\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_3s_overlap_1.5s_chunking.txt\"\n",
    "mode = 'mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_4s_overlap_2s_chunking.txt\"\n",
    "mode = 'mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 13.222516094603327\tthreshold: 4.661870002746582\n",
      "\n",
      "0.13222516094603326\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_4s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 7.221135178045704\tthreshold: 5.5662336349487305\n",
      "\n",
      "0.07221135178045704\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_3s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 11.228702580477929\tthreshold: 5.223989963531494\n",
      "\n",
      "0.1122870258047793\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_2s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 5.356043096198929\tthreshold: 5.576591968536377\n",
      "\n",
      "0.05356043096198929\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_var.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD track 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conf-1.1 (RMDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_rs.txt\n",
      "eer: 35.90776548582365\tthreshold: 0.4829051196575165\n",
      "\n",
      "0.3590776548582365\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_2s_rs.txt\n",
      "eer: 28.872058545596797\tthreshold: 1.1376407146453855\n",
      "\n",
      "0.28872058545596796\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_3s_rs.txt\n",
      "eer: 25.971029810885778\tthreshold: 1.758313536643982\n",
      "\n",
      "0.25971029810885776\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_4s_rs.txt\n",
      "eer: 24.93812201801109\tthreshold: 2.519417524337769\n",
      "\n",
      "0.24938122018011089\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_var.txt\n",
      "eer: 24.771201269579443\tthreshold: 3.0062925815582275\n",
      "\n",
      "0.24771201269579443\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/rmdt\")\n",
    "\n",
    "# filter if the file is a txt file\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/rmdt/\" + x for x in score_file_list]\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_chunking.txt\n",
      "eer: 31.14374479185653\tthreshold: 0.44332343339920044\n",
      "\n",
      "0.3114374479185653\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_2s_chunking.txt\n",
      "eer: 28.872058545596797\tthreshold: 1.1376407146453855\n",
      "\n",
      "0.28872058545596796\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_3s_chunking.txt\n",
      "eer: 29.58894941656256\tthreshold: 0.2161173373460769\n",
      "\n",
      "0.2958894941656256\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_4s_chunking.txt\n",
      "eer: 29.686146357434097\tthreshold: 2.042598009109497\n",
      "\n",
      "0.29686146357434096\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # import sys\n",
    "    # print(submission_scores_grouped_df.head())\n",
    "    # print(cm_data.head())\n",
    "    # sys.exit()\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    \n",
    "    \n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir('/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/rmdt/chunking')\n",
    "\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/rmdt/chunking/\" + x for x in score_file_list]\n",
    "\n",
    "mode = 'mean'\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\",\n",
    "                            mode=mode))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_3s.txt\n",
      "eer: 29.755870164994203\tthreshold: -0.106929525732994\n",
      "\n",
      "0.297558701649942\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_4s.txt\n",
      "eer: 30.27262876483326\tthreshold: -1.0986006259918213\n",
      "\n",
      "0.3027262876483326\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_1s.txt\n",
      "eer: 40.13329283602395\tthreshold: 2.794327735900879\n",
      "\n",
      "0.4013329283602395\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_2s.txt\n",
      "eer: 30.99916819415607\tthreshold: 2.0904321670532227\n",
      "\n",
      "0.3099916819415607\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_var.txt\n",
      "eer: 27.155471404451408\tthreshold: -1.2492107152938845\n",
      "\n",
      "0.2715547140445141\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/base\")\n",
    "\n",
    "# filter if the file is a txt file\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/base/\" + x for x in score_file_list]\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_chunking_1s.txt\n",
      "eer: 36.04904132135414\tthreshold: 0.6246887048085531\n",
      "\n",
      "0.36049041321354136\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_chunking_2s.txt\n",
      "eer: 30.99916819415607\tthreshold: 2.0904321670532227\n",
      "\n",
      "0.3099916819415607\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_chunking_3s.txt\n",
      "eer: 32.89928184508063\tthreshold: -1.0790154933929443\n",
      "\n",
      "0.3289928184508063\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_chunking_4s.txt\n",
      "eer: 31.823513971153826\tthreshold: -1.733583927154541\n",
      "\n",
      "0.3182351397115383\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # import sys\n",
    "    # print(submission_scores_grouped_df.head())\n",
    "    # #print(cm_data.head())\n",
    "    # sys.exit()\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    \n",
    "    \n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir('/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/base/chunking')\n",
    "\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/base/chunking/\" + x for x in score_file_list]\n",
    "\n",
    "mode = 'mean'\n",
    "suffix = \"_\"\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\",\n",
    "                            mode=mode,\n",
    "                            suffix=suffix))\n",
    "                           \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_track2_xlsr_aasist_multiview_conf-2_eval_1s_rs.txt\n",
      "eer: 39.41249179608479\tthreshold: -2.74609375\n",
      "\n",
      "0.39412491796084786\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_2s_rs.txt\n",
      "eer: 33.76587829632709\tthreshold: -2.830078125\n",
      "\n",
      "0.3376587829632709\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_3s_rs.txt\n",
      "eer: 32.38252324524158\tthreshold: -2.841796875\n",
      "\n",
      "0.3238252324524158\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_4s_rs.txt\n",
      "eer: 32.3179284202617\tthreshold: -2.80078125\n",
      "\n",
      "0.323179284202617\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_var.txt\n",
      "eer: 26.288874953204953\tthreshold: -1.7490234375\n",
      "\n",
      "0.26288874953204955\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/mdt\")\n",
    "\n",
    "# filter if the file is a txt file\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/mdt/\" + x for x in score_file_list]\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_track2_xlsr_aasist_multiview_conf-2_eval_1s_chunking.txt\n",
      "eer: 38.686561773565394\tthreshold: -2.576171875\n",
      "\n",
      "0.3868656177356539\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_2s_chunking.txt\n",
      "eer: 33.33333333333333\tthreshold: -2.8125\n",
      "\n",
      "0.3333333333333333\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_3s_chunking.txt\n",
      "eer: 35.8060489691753\tthreshold: -3.06640625\n",
      "\n",
      "0.358060489691753\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_4s_chunking.txt\n",
      "eer: 35.29441935191648\tthreshold: -2.990234375\n",
      "\n",
      "0.35294419351916484\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # import sys\n",
    "    # print(submission_scores_grouped_df.head())\n",
    "    # #print(cm_data.head())\n",
    "    # sys.exit()\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir(\n",
    "    '/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/mdt/chunking')\n",
    "\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/mdt/chunking/\" + x for x in score_file_list]\n",
    "\n",
    "mode = 'mean'\n",
    "suffix = \"___\"\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\",\n",
    "                             mode=mode,\n",
    "                             suffix=suffix))\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate EER group by duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## Create protocol df file\n",
    "\n",
    "index_col = 'trial'\n",
    "# =====\n",
    "# Configuration to load CM protocol and score file\n",
    "# =====\n",
    "# name of data series for procotol file\n",
    "p_names = ['speaker', index_col, 'compr', 'source', 'attack',\n",
    "                'label', 'trim', 'subset', 'vocoder', \n",
    "                'task', 'team', 'gender-pair', 'language']\n",
    "\n",
    "cm_file = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/logs/asvspoof-challenge-2021/eval-package/keys/DF/CM/trial_metadata.txt\", sep=' ', header=None)\n",
    "\n",
    "cm_file.columns = p_names\n",
    "\n",
    "# keep only following columns: [trial, subset, label]\n",
    "\n",
    "cm_file = cm_file[[index_col, 'subset', 'label']]\n",
    "\n",
    "# save to file\n",
    "cm_file.to_csv(\"ASVspoof2021_DF_protocol.txt\", sep=' ', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "\n",
    "duration_df = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/notebooks/ASVspoof2021_DF_eval_durations.csv\", sep=',')\n",
    "\n",
    "# colums are [filename, duration]\n",
    "# add a new column call group_duration based on following conditions:\n",
    "# 1. if duration < 1s, group_duration = \"<1s\"\n",
    "# 2. if duration >= 1s and duration < 2s, group_duration = \"1~2s\"\n",
    "# 3. if duration >= 2s and duration < 3s, group_duration = \"2~3s\"\n",
    "# 4. if duration >= 3s and duration < 4s, group_duration = \"3~4s\"\n",
    "# 5. if duration >= 4s and duration < 5s, group_duration = \"4~5s\"\n",
    "# 6. if duration >= 5s, group_duration = \">5s\"\n",
    "\n",
    "\n",
    "duration_df['group_duration'] = duration_df['duration'].apply(lambda x: \"<1s\" if x < 1 else (\"1~2s\" if x < 2 else (\"2~3s\" if x < 3 else (\"3~4s\" if x < 4 else (\"4~5s\" if x < 5 else \">5s\")))))\n",
    "\n",
    "\n",
    "# remove .flac extension in filename\n",
    "duration_df['filename'] = duration_df['filename'].apply(lambda x: x.split(\".\")[0])\n",
    "# save to file\n",
    "\n",
    "cm_file = pd.read_csv('ASVspoof2021_DF_protocol.txt', sep=' ', header=None)\n",
    "\n",
    "cm_file.columns = ['filename', 'subset', 'label']\n",
    "\n",
    "# merge duration_df with cm_file\n",
    "duration_df = duration_df.merge(cm_file, on='filename', how='left') \n",
    "\n",
    "duration_df = duration_df[['filename', 'subset', 'label', 'group_duration']]\n",
    "\n",
    "\n",
    "duration_df.to_csv(\"ASVspoof2021_DF_eval_durations_grouped.csv\", sep=' ', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing evaluations: 100%|██████████| 84/84 [00:18<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Results saved to evaluation_results.json\n",
      "Visualization saved to results_visualization.png\n"
     ]
    }
   ],
   "source": [
    "import eval_metrics_DF as em\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_single_evaluation(args):\n",
    "    \"\"\"Process a single evaluation combination\"\"\"\n",
    "    score_file, subset, duration_group = args\n",
    "    try:\n",
    "        cm_data = pd.read_csv(\n",
    "            \"ASVspoof2021_DF_eval_durations_grouped.csv\", sep=' ', header=None)\n",
    "        cm_data.columns = ['filename', 'subset', 'label', 'duration_group']\n",
    "\n",
    "        submission_scores = pd.read_csv(\n",
    "            score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "        submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "        cm_scores = submission_scores.merge(\n",
    "            cm_data, on='filename', how='inner')\n",
    "        cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "        cm_scores = cm_scores[cm_scores['duration_group'] == duration_group]\n",
    "\n",
    "        if len(cm_scores) == 0:\n",
    "            return {\n",
    "                'file': os.path.basename(score_file).split(\".\")[0],\n",
    "                'subset': subset,\n",
    "                'duration_group': duration_group,\n",
    "                'eer': None,\n",
    "                'threshold': None\n",
    "            }\n",
    "\n",
    "        bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "        spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "        if len(bona_cm) == 0 or len(spoof_cm) == 0:\n",
    "            return {\n",
    "                'file': os.path.basename(score_file).split(\".\")[0],\n",
    "                'subset': subset,\n",
    "                'duration_group': duration_group,\n",
    "                'eer': None,\n",
    "                'threshold': None\n",
    "            }\n",
    "\n",
    "        eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "\n",
    "        return {\n",
    "            'file': os.path.basename(score_file).split(\".\")[0],\n",
    "            'subset': subset,\n",
    "            'duration_group': duration_group,\n",
    "            'eer': float(eer_cm * 100),  # Convert to percentage\n",
    "            'threshold': float(th)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error processing {score_file} - {subset} - {duration_group}: {str(e)}\")\n",
    "        return {\n",
    "            'file': os.path.basename(score_file).split(\".\")[0],\n",
    "            'subset': subset,\n",
    "            'duration_group': duration_group,\n",
    "            'eer': None,\n",
    "            'threshold': None\n",
    "        }\n",
    "\n",
    "\n",
    "def plot_results(results_data, output_path=\"results_visualization.png\"):\n",
    "    \"\"\"Create bar chart visualization for eval subset results\"\"\"\n",
    "    # Filter for eval subset only and remove None values\n",
    "    eval_results = [r for r in results_data if r['subset']\n",
    "                    == 'eval' and r['eer'] is not None]\n",
    "\n",
    "    if not eval_results:\n",
    "        print(\"No valid eval results to plot\")\n",
    "        return\n",
    "\n",
    "    # Group results by file and duration\n",
    "    files = sorted(list(set(r['file'] for r in eval_results)))\n",
    "    durations = DURATION_GROUP\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    data = {}\n",
    "    for file in files:\n",
    "        data[file] = []\n",
    "        for duration in durations:\n",
    "            result = next((r['eer'] for r in eval_results\n",
    "                           if r['file'] == file and r['duration_group'] == duration), None)\n",
    "            data[file].append(result if result is not None else np.nan)\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    x = np.arange(len(durations))\n",
    "    width = 0.8 / len(files)\n",
    "\n",
    "    for i, (file, values) in enumerate(data.items()):\n",
    "        plt.bar(x + i * width, values, width, label=file)\n",
    "\n",
    "    plt.xlabel('Duration Groups')\n",
    "    plt.ylabel('EER (%)')\n",
    "    plt.title('EER by Duration Group for Different Systems (Eval Set)')\n",
    "    plt.xticks(x + width * (len(files) - 1) / 2, durations, rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "DURATION_GROUP = [\"<1s\", \"1~2s\", \"2~3s\", \"3~4s\", \"4~5s\", \">5s\"]\n",
    "SUBSETS = [\"eval\", \"hidden\"]\n",
    "FILE_LIST_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/var_analysis\"\n",
    "\n",
    "# Get score files\n",
    "score_file_list = [os.path.join(FILE_LIST_DIR, x)\n",
    "                    for x in os.listdir(FILE_LIST_DIR)\n",
    "                    if x.endswith(\".txt\")]\n",
    "\n",
    "# Create all combinations of parameters\n",
    "combinations = list(product(score_file_list, SUBSETS, DURATION_GROUP))\n",
    "\n",
    "# Process in parallel\n",
    "num_processes = max(1, cpu_count() - 1)  # Leave one CPU free\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    results = list(tqdm(\n",
    "        pool.imap_unordered(process_single_evaluation, combinations),\n",
    "        total=len(combinations),\n",
    "        desc=\"Processing evaluations\"\n",
    "    ))\n",
    "\n",
    "# Save results to JSON\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Create visualization\n",
    "plot_results(results)\n",
    "\n",
    "print(\"Processing complete!\")\n",
    "print(\"Results saved to evaluation_results.json\")\n",
    "print(\"Visualization saved to results_visualization.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>bonafide</th>\n",
       "      <th>spoof</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration_group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1~2s</th>\n",
       "      <td>2733</td>\n",
       "      <td>140173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2~3s</th>\n",
       "      <td>7031</td>\n",
       "      <td>186116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3~4s</th>\n",
       "      <td>6824</td>\n",
       "      <td>127953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4~5s</th>\n",
       "      <td>3652</td>\n",
       "      <td>95279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;1s</th>\n",
       "      <td>277</td>\n",
       "      <td>7038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;5s</th>\n",
       "      <td>2100</td>\n",
       "      <td>32653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label           bonafide   spoof\n",
       "duration_group                  \n",
       "1~2s                2733  140173\n",
       "2~3s                7031  186116\n",
       "3~4s                6824  127953\n",
       "4~5s                3652   95279\n",
       "<1s                  277    7038\n",
       ">5s                 2100   32653"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Deeper analysis\n",
    "import pandas as pd\n",
    "\n",
    "duration_df = pd.read_csv(\"ASVspoof2021_DF_eval_durations_grouped.csv\", sep=' ', header=None)\n",
    "duration_df.columns = ['filename', 'subset', 'label', 'duration_group']\n",
    "\n",
    "# How many files in each duration group, group by label\n",
    "duration_df.groupby(['duration_group', 'label']).size().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larcorpus eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/base/largecorpus_xlsr_conformertcm_adapter_mdt_ft24_4s.txt\n",
      "min_score:  -5.781251430511475\n",
      "max_score:  5.385744094848633\n",
      "eer: 0.162156710316127\tthreshold: -0.8763607144355774\n",
      "\n",
      "0.0016215671031612699\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "    \n",
    "    # min_score of cm_scores['score']\n",
    "    print(\"min_score: \", min(cm_scores['score']))\n",
    "    # max_score of cm_scores['score']\n",
    "    print(\"max_score: \", max(cm_scores['score']))\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/base\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlaad_v5 eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5/mlaad_v5_xlsr_conformertcm_24_lora_all_4s.txt\n",
      "Accuracy: 94.00\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5/mlaadv5_wavlm_conformertcm_adapter_mdt_ft12_4s.txt\n",
      "Accuracy: 91.61\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5/mlaadv5_xlsr_conformertcm_adapter_mdt_ft12_4s.txt\n",
      "Accuracy: 94.56\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5/mlaadv5_xlsr_conformertcm_adapter_mdt_fz24_4s.txt\n",
      "Accuracy: 90.21\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5/mlaadv5_xlsr_conformertcm_mdt_ft24_4s.txt\n",
      "Accuracy: 93.73\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    #score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    # pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt\n",
    "    res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "    print(\"Accuracy: {:.2f}\".format(\n",
    "        accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tmp results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified files:\n",
      "                                                filename     spoof     score  \\\n",
      "10078                       Real/LJSpeech/LJ005-0300.wav  0.704709 -0.475826   \n",
      "10548                    Real/ASVspoof5/D_0001014595.wav  4.418746 -4.886073   \n",
      "11098                       Real/inthewild_bona/7490.wav  0.379503 -1.570372   \n",
      "11215   Real/CommonVoice/es/common_voice_es_19615002.wav  0.521142 -0.677068   \n",
      "11357                        Real/inthewild_bona/934.wav  0.687202 -1.645550   \n",
      "...                                                  ...       ...       ...   \n",
      "21193  asv19_wav/AMPLITUDE_MODULATION/bona-fide/f0486...  4.784845 -5.325810   \n",
      "21195  mlaad_wav/AMPLITUDE_MODULATION/bona-fide/82ce3...  5.055811 -5.366982   \n",
      "21196  mlaad_wav/AMPLITUDE_MODULATION/bona-fide/6a767...  4.511868 -4.826271   \n",
      "21197  mlaad_wav/AUTOTUNE/bona-fide/33d598205c8b16044...  5.014217 -5.566197   \n",
      "21199  mlaad_wav/AUTOTUNE/bona-fide/1c187221ef3459147...  4.194440 -4.847738   \n",
      "\n",
      "      subset      label   pred  \n",
      "10078    dev   bonafide  spoof  \n",
      "10548    dev   bonafide  spoof  \n",
      "11098    dev   bonafide  spoof  \n",
      "11215    dev   bonafide  spoof  \n",
      "11357    dev   bonafide  spoof  \n",
      "...      ...        ...    ...  \n",
      "21193    dev  bona-fide  spoof  \n",
      "21195    dev  bona-fide  spoof  \n",
      "21196    dev  bona-fide  spoof  \n",
      "21197    dev  bona-fide  spoof  \n",
      "21199    dev  bona-fide  spoof  \n",
      "\n",
      "[618 rows x 6 columns]\n",
      "eer: 0.07773584905660377\tthreshold: -1.4205422401428225\n",
      "\n",
      "0.0007773584905660378\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'spoof', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    \n",
    "    cm_scores['pred'] = cm_scores.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "    \n",
    "    # print confusion matrix\n",
    "    print(\"Confusion matrix:\")\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    cm = confusion_matrix(cm_scores['label'], cm_scores['pred'], labels=[\"spoof\",\"bonafide\"])\n",
    "    cm = confusion_matrix(cm_scores[\"label\"], cm_scores[\"pred\"], labels=[\"spoof\",\"bonafide\"])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"spoof\",\"bonafide\"])\n",
    "    disp.plot(cmap='Greens', values_format='g')\n",
    "    plt.show()\n",
    "    \n",
    "    # Listing mis-classified files\n",
    "    misclassified = cm_scores[cm_scores['label'] != cm_scores['pred']]\n",
    "    print(\"Misclassified files:\")\n",
    "    print(misclassified)\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/tmp_results_echo_autune_amp_v2.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/notebooks/protocols/protocl_for_echo_autotune_amp.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new elevenlabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/xlsr_conformertcm_multiview_large_corpus_more_elevenlabs.txt\n",
      "Accuracy: 99.97\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/more_evelenlabs_more_kr_telephoney_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# filter with subset = \"eval\"\n",
    "# full_df = full_df[full_df[\"subset\"] == \"eval\"]\n",
    "\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    #score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    # pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt\n",
    "    res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "    print(\"Accuracy: {:.2f}\".format(\n",
    "        accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new AIHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_april_xlsr_vib_large_corpus_s202412.txt\n",
      "Number of samples 3130\n",
      "Accuracy: 52.52\n",
      "\n",
      "\n",
      "min_score:  -3.400717735290528\n",
      "max_score:  3.6708192825317374\n",
      "eer: 7.389922090280393\tthreshold: 3.558494806289673\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_lora_replay_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Number of samples 3130\n",
      "Accuracy: 100.00\n",
      "\n",
      "\n",
      "min_score:  -5.297204971313477\n",
      "max_score:  5.230615139007568\n",
      "eer: 0.5108087338231388\tthreshold: 0.4304629564285278\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_lora_xlsr_conformertcm_mdt_large_corpus_s202412_v2.txt\n",
      "Number of samples 3130\n",
      "Accuracy: 62.01\n",
      "\n",
      "\n",
      "min_score:  -5.630342960357666\n",
      "max_score:  5.225861549377441\n",
      "eer: 1.99984637227016\tthreshold: 4.322917938232422\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_lora_xlsr_conformertcm_mdt_large_corpus_s202412_v2_e5.txt\n",
      "Number of samples 3130\n",
      "Accuracy: 62.01\n",
      "\n",
      "\n",
      "min_score:  -5.630342960357666\n",
      "max_score:  5.225861549377441\n",
      "eer: 1.99984637227016\tthreshold: 4.322917938232422\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_lora_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Number of samples 3130\n",
      "Accuracy: 72.72\n",
      "\n",
      "\n",
      "min_score:  -5.557672500610352\n",
      "max_score:  5.330420970916748\n",
      "eer: 0.7623351276219884\tthreshold: 4.152626514434815\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_xlsr_conformertcm_multiview_large_corpus.txt\n",
      "Number of samples 3130\n",
      "Accuracy: 46.04\n",
      "\n",
      "\n",
      "min_score:  -5.610496520996094\n",
      "max_score:  5.380556583404541\n",
      "eer: 2.3619049699784367\tthreshold: 4.893696308135986\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_xlsr_conformertcm_multiview_large_corpus_from_s202412_e65.txt\n",
      "Number of samples 3130\n",
      "Accuracy: 69.30\n",
      "\n",
      "\n",
      "min_score:  -5.582256317138672\n",
      "max_score:  5.256197452545166\n",
      "eer: 1.5561812793340533\tthreshold: 3.6176908016204834\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_xlsr_conformertcm_multiview_large_corpus_more_elevenlabs_e26.txt\n",
      "Number of samples 3130\n",
      "Accuracy: 54.86\n",
      "\n",
      "\n",
      "min_score:  -5.4795002937316895\n",
      "max_score:  5.4679059982299805\n",
      "eer: 1.6878832456996375\tthreshold: 4.598993301391602\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Number of samples 3130\n",
      "Accuracy: 61.41\n",
      "\n",
      "\n",
      "min_score:  -5.6006574630737305\n",
      "max_score:  5.221170902252197\n",
      "eer: 2.1052079453626273\tthreshold: 4.331591606140137\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/Aihub_new_lora_v4_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Number of samples 3130\n",
      "Accuracy: 100.00\n",
      "\n",
      "\n",
      "min_score:  -5.583332538604736\n",
      "max_score:  5.281163215637207\n",
      "eer: 0.01446285434079832\tthreshold: 0.7497708797454834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'spoof', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "    \n",
    "    # min_score of cm_scores['score']\n",
    "    print(\"min_score: \", min(cm_scores['score']))\n",
    "    # max_score of cm_scores['score']\n",
    "    print(\"max_score: \", max(cm_scores['score']))\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHub/protocol_for_cm.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# Filter with utt startwith Elevenlabs only\n",
    "full_df = full_df[full_df[\"utt\"].str.startswith(\"Elevenlabs\")]\n",
    "\n",
    "# filter with subset = \"eval\"\n",
    "# full_df = full_df[full_df[\"subset\"] == \"eval\"]\n",
    "\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    #score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    # pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt\n",
    "    res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "    print(\"Number of samples\", len(res_df))\n",
    "\n",
    "    print(\"Accuracy: {:.2f}\".format(\n",
    "        accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    ## EER\n",
    "    eval_to_score_file(score_file, \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHub/protocol_for_cm.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "meta_csv = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/meta_nov27.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/Large_corpus_xlsr_vib_large_corpus.txt\n",
      "Accuracy: 95.21\n",
      "\n",
      "\n",
      "min_score:  -3.4577999114990234\n",
      "max_score:  3.689281702041626\n",
      "eer: 3.4445322187099765\tthreshold: -3.2064414024353027\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/Largecorpus_new_lora_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Accuracy: 99.80\n",
      "\n",
      "\n",
      "min_score:  -5.794503688812256\n",
      "max_score:  5.249507427215576\n",
      "eer: 0.2076297275517688\tthreshold: -0.4276063144207001\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/Largecorpus_new_xlsr_conformertcm_multiview_large_corpus_from_s202412_e65.txt\n",
      "Accuracy: 99.42\n",
      "\n",
      "\n",
      "min_score:  -5.758472442626953\n",
      "max_score:  5.255980968475342\n",
      "eer: 0.6323724378632378\tthreshold: -1.6459542512893677\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/Largecorpus_new_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Accuracy: 99.79\n",
      "\n",
      "\n",
      "min_score:  -5.80772590637207\n",
      "max_score:  5.182519912719727\n",
      "eer: 0.17803859182730547\tthreshold: -1.1230292320251465\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/large_corpus_eval_new_lora_replay_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Accuracy: 99.23\n",
      "\n",
      "\n",
      "min_score:  -5.814205169677734\n",
      "max_score:  5.248363971710205\n",
      "eer: 0.6836256941156187\tthreshold: 0.6124406456947327\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/large_corpus_eval_new_lora_v4_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Accuracy: 99.77\n",
      "\n",
      "\n",
      "min_score:  -5.794478893280029\n",
      "max_score:  5.252584457397461\n",
      "eer: 0.2234877579448605\tthreshold: -0.2705420255661011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'spoof', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "    \n",
    "    # min_score of cm_scores['score']\n",
    "    print(\"min_score: \", min(cm_scores['score']))\n",
    "    # max_score of cm_scores['score']\n",
    "    print(\"max_score: \", max(cm_scores['score']))\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "full_df = full_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    #score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    # merge meta_csv with pred_df\n",
    "    pred_df = pred_df.merge(meta_csv, on='utt', how='left')\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt\n",
    "    res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "    print(\"Accuracy: {:.2f}\".format(\n",
    "        accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    ## EER\n",
    "    eval_to_score_file(score_file, \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/protocol.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/Large_corpus_xlsr_vib_large_corpus.txt\n",
      "Overall Accuracy: 95.21\n",
      "\n",
      "Accuracy by group:\n",
      "group 'SNS2024': 98.46%\n",
      "group 'AIHUB': 99.99%\n",
      "group 'InWild': 95.29%\n",
      "group 'LibriSpeech': 99.91%\n",
      "group 'VCTK': 92.84%\n",
      "group 'LJSpeech': 72.75%\n",
      "group 'CommonVoice': 69.70%\n",
      "group 'OpenVoice': 100.00%\n",
      "group 'InWild_spoof': 98.46%\n",
      "group 'ASVspoof2019_spoof': 99.42%\n",
      "group 'QuickVC': 99.70%\n",
      "group 'AIHUB_VITS': 99.45%\n",
      "group 'StarGANv2-VC': 100.00%\n",
      "group 'VITS-TTS': 99.39%\n",
      "group 'MMSTTS': 99.89%\n",
      "group 'Tortoise-TTS': 99.95%\n",
      "group 'Elevenlabs': 99.68%\n",
      "group 'Diff-HierVC': 100.00%\n",
      "group 'StyleTTS2': 98.79%\n",
      "group 'SeamlessM4T-TTS': 100.00%\n",
      "group 'DDDM-VC': 99.95%\n",
      "group 'MeloTTS': 99.30%\n",
      "group 'DiffGAN': 99.80%\n",
      "group '202410_Elevenlabs': 74.55%\n",
      "group 'ASVspoof5_spoof': 99.63%\n",
      "group 'ASVspoof5': 96.71%\n",
      "\n",
      "Additional metrics:\n",
      "F1 Score: 0.9461\n",
      "Recall: 0.9085\n",
      "Precision: 0.9869\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/Largecorpus_new_lora_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Overall Accuracy: 99.80\n",
      "\n",
      "Accuracy by group:\n",
      "group 'SNS2024': 97.84%\n",
      "group 'AIHUB': 100.00%\n",
      "group 'InWild': 99.46%\n",
      "group 'LibriSpeech': 99.99%\n",
      "group 'VCTK': 99.72%\n",
      "group 'LJSpeech': 98.75%\n",
      "group 'CommonVoice': 99.82%\n",
      "group 'OpenVoice': 100.00%\n",
      "group 'InWild_spoof': 99.34%\n",
      "group 'ASVspoof2019_spoof': 99.99%\n",
      "group 'QuickVC': 99.97%\n",
      "group 'AIHUB_VITS': 99.92%\n",
      "group 'StarGANv2-VC': 100.00%\n",
      "group 'VITS-TTS': 99.39%\n",
      "group 'MMSTTS': 99.39%\n",
      "group 'Tortoise-TTS': 99.97%\n",
      "group 'Elevenlabs': 99.88%\n",
      "group 'Diff-HierVC': 100.00%\n",
      "group 'StyleTTS2': 99.67%\n",
      "group 'SeamlessM4T-TTS': 100.00%\n",
      "group 'DDDM-VC': 100.00%\n",
      "group 'MeloTTS': 99.80%\n",
      "group 'DiffGAN': 100.00%\n",
      "group '202410_Elevenlabs': 100.00%\n",
      "group 'ASVspoof5_spoof': 99.85%\n",
      "group 'ASVspoof5': 100.00%\n",
      "\n",
      "Additional metrics:\n",
      "F1 Score: 0.9979\n",
      "Recall: 0.9978\n",
      "Precision: 0.9979\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/Largecorpus_new_xlsr_conformertcm_multiview_large_corpus_from_s202412_e65.txt\n",
      "Overall Accuracy: 99.42\n",
      "\n",
      "Accuracy by group:\n",
      "group 'SNS2024': 98.46%\n",
      "group 'AIHUB': 99.99%\n",
      "group 'InWild': 99.02%\n",
      "group 'LibriSpeech': 99.90%\n",
      "group 'VCTK': 94.61%\n",
      "group 'LJSpeech': 96.50%\n",
      "group 'CommonVoice': 99.48%\n",
      "group 'OpenVoice': 100.00%\n",
      "group 'InWild_spoof': 99.64%\n",
      "group 'ASVspoof2019_spoof': 100.00%\n",
      "group 'QuickVC': 100.00%\n",
      "group 'AIHUB_VITS': 99.59%\n",
      "group 'StarGANv2-VC': 100.00%\n",
      "group 'VITS-TTS': 99.78%\n",
      "group 'MMSTTS': 99.94%\n",
      "group 'Tortoise-TTS': 100.00%\n",
      "group 'Elevenlabs': 99.96%\n",
      "group 'Diff-HierVC': 100.00%\n",
      "group 'StyleTTS2': 98.93%\n",
      "group 'SeamlessM4T-TTS': 100.00%\n",
      "group 'DDDM-VC': 100.00%\n",
      "group 'MeloTTS': 100.00%\n",
      "group 'DiffGAN': 100.00%\n",
      "group '202410_Elevenlabs': 100.00%\n",
      "group 'ASVspoof5_spoof': 99.81%\n",
      "group 'ASVspoof5': 99.23%\n",
      "\n",
      "Additional metrics:\n",
      "F1 Score: 0.9937\n",
      "Recall: 0.9892\n",
      "Precision: 0.9982\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/Largecorpus_new_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Overall Accuracy: 99.79\n",
      "\n",
      "Accuracy by group:\n",
      "group 'SNS2024': 98.77%\n",
      "group 'AIHUB': 100.00%\n",
      "group 'InWild': 99.21%\n",
      "group 'LibriSpeech': 99.99%\n",
      "group 'VCTK': 99.65%\n",
      "group 'LJSpeech': 94.50%\n",
      "group 'CommonVoice': 99.69%\n",
      "group 'OpenVoice': 100.00%\n",
      "group 'InWild_spoof': 99.68%\n",
      "group 'ASVspoof2019_spoof': 99.99%\n",
      "group 'QuickVC': 99.90%\n",
      "group 'AIHUB_VITS': 99.96%\n",
      "group 'StarGANv2-VC': 100.00%\n",
      "group 'VITS-TTS': 99.51%\n",
      "group 'MMSTTS': 99.78%\n",
      "group 'Tortoise-TTS': 100.00%\n",
      "group 'Elevenlabs': 99.92%\n",
      "group 'Diff-HierVC': 100.00%\n",
      "group 'StyleTTS2': 99.60%\n",
      "group 'SeamlessM4T-TTS': 100.00%\n",
      "group 'DDDM-VC': 100.00%\n",
      "group 'MeloTTS': 100.00%\n",
      "group 'DiffGAN': 100.00%\n",
      "group '202410_Elevenlabs': 100.00%\n",
      "group 'ASVspoof5_spoof': 99.93%\n",
      "group 'ASVspoof5': 99.92%\n",
      "\n",
      "Additional metrics:\n",
      "F1 Score: 0.9978\n",
      "Recall: 0.9968\n",
      "Precision: 0.9988\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/large_corpus_eval_new_lora_replay_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Overall Accuracy: 99.23\n",
      "\n",
      "Accuracy by group:\n",
      "group 'SNS2024': 97.02%\n",
      "group 'AIHUB': 99.63%\n",
      "group 'InWild': 99.39%\n",
      "group 'LibriSpeech': 99.99%\n",
      "group 'VCTK': 99.77%\n",
      "group 'LJSpeech': 95.50%\n",
      "group 'CommonVoice': 99.66%\n",
      "group 'OpenVoice': 96.04%\n",
      "group 'InWild_spoof': 95.91%\n",
      "group 'ASVspoof2019_spoof': 99.80%\n",
      "group 'QuickVC': 97.47%\n",
      "group 'AIHUB_VITS': 99.56%\n",
      "group 'StarGANv2-VC': 100.00%\n",
      "group 'VITS-TTS': 98.05%\n",
      "group 'MMSTTS': 92.61%\n",
      "group 'Tortoise-TTS': 99.90%\n",
      "group 'Elevenlabs': 99.88%\n",
      "group 'Diff-HierVC': 100.00%\n",
      "group 'StyleTTS2': 97.79%\n",
      "group 'SeamlessM4T-TTS': 98.64%\n",
      "group 'DDDM-VC': 99.75%\n",
      "group 'MeloTTS': 99.00%\n",
      "group 'DiffGAN': 100.00%\n",
      "group '202410_Elevenlabs': 99.95%\n",
      "group 'ASVspoof5_spoof': 99.63%\n",
      "group 'ASVspoof5': 100.00%\n",
      "\n",
      "Additional metrics:\n",
      "F1 Score: 0.9918\n",
      "Recall: 0.9965\n",
      "Precision: 0.9870\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus/large_corpus_eval_new_lora_v4_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Overall Accuracy: 99.77\n",
      "\n",
      "Accuracy by group:\n",
      "group 'SNS2024': 97.74%\n",
      "group 'AIHUB': 99.99%\n",
      "group 'InWild': 99.46%\n",
      "group 'LibriSpeech': 99.99%\n",
      "group 'VCTK': 99.69%\n",
      "group 'LJSpeech': 98.25%\n",
      "group 'CommonVoice': 99.81%\n",
      "group 'OpenVoice': 100.00%\n",
      "group 'InWild_spoof': 99.43%\n",
      "group 'ASVspoof2019_spoof': 99.97%\n",
      "group 'QuickVC': 100.00%\n",
      "group 'AIHUB_VITS': 99.82%\n",
      "group 'StarGANv2-VC': 100.00%\n",
      "group 'VITS-TTS': 99.02%\n",
      "group 'MMSTTS': 99.22%\n",
      "group 'Tortoise-TTS': 100.00%\n",
      "group 'Elevenlabs': 99.92%\n",
      "group 'Diff-HierVC': 100.00%\n",
      "group 'StyleTTS2': 98.79%\n",
      "group 'SeamlessM4T-TTS': 100.00%\n",
      "group 'DDDM-VC': 100.00%\n",
      "group 'MeloTTS': 99.70%\n",
      "group 'DiffGAN': 100.00%\n",
      "group '202410_Elevenlabs': 100.00%\n",
      "group 'ASVspoof5_spoof': 99.74%\n",
      "group 'ASVspoof5': 100.00%\n",
      "\n",
      "Additional metrics:\n",
      "F1 Score: 0.9975\n",
      "Recall: 0.9978\n",
      "Precision: 0.9973\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# Fix: Save full_df after parsing the utt column, not overwrite it\n",
    "full_df['utt_id'] = full_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    #score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    pred_df['utt_id'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    # merge meta_csv with pred_df - note: meta_csv is not defined in your code\n",
    "    # Assuming meta_csv is supposed to be full_df, otherwise define meta_csv\n",
    "    pred_df = pred_df.merge(full_df, left_on='utt_id', right_on='utt_id', how='left')\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt (note: eval_df is not defined in your code)\n",
    "    # Let's assume res_df is already the result of the merge above\n",
    "    \n",
    "\n",
    "    res_df = pd.merge(pred_df, meta_csv, left_on='utt_id', right_on='utt')\n",
    "    \n",
    "    # Filter on eval subset\n",
    "    pred_df = pred_df[pred_df['subset'] == 'eval']\n",
    "    \n",
    "    # rename label_x to label\n",
    "    res_df.rename(columns={'label_x': 'label'}, inplace=True)\n",
    "\n",
    "    # Overall accuracy\n",
    "    print(\"Overall Accuracy: {:.2f}\".format(\n",
    "        accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    \n",
    "    # Group by group and calculate accuracy for each group\n",
    "    groups = res_df['group'].unique()\n",
    "    \n",
    "    print(\"\\nAccuracy by group:\")\n",
    "    for group in groups:\n",
    "        group_df = res_df[res_df['group'] == group]\n",
    "        group_acc = accuracy_score(group_df[\"label\"], group_df[\"pred\"])*100\n",
    "        print(f\"group '{group}': {group_acc:.2f}%\")\n",
    "    \n",
    "    # Additional metrics if needed\n",
    "    print(\"\\nAdditional metrics:\")\n",
    "    print(f\"F1 Score: {f1_score(res_df['label'], res_df['pred'], pos_label='bonafide'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(res_df['label'], res_df['pred'], pos_label='bonafide'):.4f}\")\n",
    "    print(f\"Precision: {precision_score(res_df['label'], res_df['pred'], pos_label='bonafide'):.4f}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "\n",
      "Processing Largecorpus_new_lora_xlsr_conformertcm_multiview_large_corpus_s202412.txt...\n",
      "\n",
      "Processing Largecorpus_new_xlsr_conformertcm_multiview_large_corpus_from_s202412_e65.txt...\n",
      "\n",
      "Processing Largecorpus_new_xlsr_conformertcm_multiview_large_corpus_s202412.txt...\n",
      "\n",
      "Processing large_corpus_eval_new_lora_replay_xlsr_conformertcm_multiview_large_corpus_s202412.txt...\n",
      "\n",
      "Processing large_corpus_eval_new_lora_v4_xlsr_conformertcm_multiview_large_corpus_s202412.txt...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model: Largecorpus_new_lora_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "\n",
      "Overall Accuracy: 99.78% (+0.02%)\n",
      "\n",
      "Accuracy by group:\n",
      "  SNS2024: 96.86% (-1.26%)\n",
      "  AIHUB: 100.00% (+0.00%)\n",
      "  InWild: 99.44% (+0.29%)\n",
      "  LibriSpeech: 99.99% (+0.00%)\n",
      "  VCTK: 99.70% (+0.10%)\n",
      "  LJSpeech: 98.44% (+4.38%)\n",
      "  CommonVoice: 99.79% (+0.16%)\n",
      "  OpenVoice: 100.00% (+0.00%)\n",
      "  InWild_spoof: 99.29% (-0.36%)\n",
      "  ASVspoof2019_spoof: 99.98% (-0.01%)\n",
      "  QuickVC: 99.96% (+0.00%)\n",
      "  AIHUB_VITS: 99.91% (-0.04%)\n",
      "  StarGANv2-VC: 100.00% (+0.00%)\n",
      "  VITS-TTS: 99.36% (-0.12%)\n",
      "  MMSTTS: 99.31% (-0.42%)\n",
      "  Tortoise-TTS: 99.96% (-0.04%)\n",
      "  Elevenlabs: 99.88% (-0.03%)\n",
      "  Diff-HierVC: 100.00% (+0.00%)\n",
      "  StyleTTS2: 99.58% (+0.00%)\n",
      "  SeamlessM4T-TTS: 100.00% (+0.00%)\n",
      "  DDDM-VC: 100.00% (+0.00%)\n",
      "  MeloTTS: 99.75% (-0.25%)\n",
      "  DiffGAN: 100.00% (+0.00%)\n",
      "  202410_Elevenlabs: 100.00% (+0.00%)\n",
      "\n",
      "Additional metrics:\n",
      "  F1 Score: 0.9978 (+0.0002)\n",
      "  Recall: 0.9976 (+0.0013)\n",
      "  Precision: 0.9980 (-0.0008)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model: Largecorpus_new_xlsr_conformertcm_multiview_large_corpus_from_s202412_e65.txt\n",
      "\n",
      "Overall Accuracy: 99.34% (-0.41%)\n",
      "\n",
      "Accuracy by group:\n",
      "  SNS2024: 97.65% (-0.47%)\n",
      "  AIHUB: 99.99% (-0.01%)\n",
      "  InWild: 98.99% (-0.15%)\n",
      "  LibriSpeech: 99.91% (-0.08%)\n",
      "  VCTK: 94.28% (-5.32%)\n",
      "  LJSpeech: 96.56% (+2.50%)\n",
      "  CommonVoice: 99.42% (-0.22%)\n",
      "  OpenVoice: 100.00% (+0.00%)\n",
      "  InWild_spoof: 99.64% (-0.01%)\n",
      "  ASVspoof2019_spoof: 99.99% (+0.00%)\n",
      "  QuickVC: 100.00% (+0.04%)\n",
      "  AIHUB_VITS: 99.56% (-0.39%)\n",
      "  StarGANv2-VC: 100.00% (+0.00%)\n",
      "  VITS-TTS: 99.79% (+0.30%)\n",
      "  MMSTTS: 99.93% (+0.21%)\n",
      "  Tortoise-TTS: 100.00% (+0.00%)\n",
      "  Elevenlabs: 99.95% (+0.05%)\n",
      "  Diff-HierVC: 100.00% (+0.00%)\n",
      "  StyleTTS2: 98.91% (-0.67%)\n",
      "  SeamlessM4T-TTS: 100.00% (+0.00%)\n",
      "  DDDM-VC: 100.00% (+0.00%)\n",
      "  MeloTTS: 100.00% (+0.00%)\n",
      "  DiffGAN: 100.00% (+0.00%)\n",
      "  202410_Elevenlabs: 100.00% (+0.00%)\n",
      "\n",
      "Additional metrics:\n",
      "  F1 Score: 0.9935 (-0.0041)\n",
      "  Recall: 0.9885 (-0.0078)\n",
      "  Precision: 0.9985 (-0.0003)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model: Largecorpus_new_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "\n",
      "Overall Accuracy: 99.75% (+0.00%)\n",
      "\n",
      "Accuracy by group:\n",
      "  SNS2024: 98.12% (+0.00%)\n",
      "  AIHUB: 100.00% (+0.00%)\n",
      "  InWild: 99.14% (+0.00%)\n",
      "  LibriSpeech: 99.99% (+0.00%)\n",
      "  VCTK: 99.60% (+0.00%)\n",
      "  LJSpeech: 94.06% (+0.00%)\n",
      "  CommonVoice: 99.63% (+0.00%)\n",
      "  OpenVoice: 100.00% (+0.00%)\n",
      "  InWild_spoof: 99.65% (+0.00%)\n",
      "  ASVspoof2019_spoof: 99.99% (+0.00%)\n",
      "  QuickVC: 99.96% (+0.00%)\n",
      "  AIHUB_VITS: 99.95% (+0.00%)\n",
      "  StarGANv2-VC: 100.00% (+0.00%)\n",
      "  VITS-TTS: 99.48% (+0.00%)\n",
      "  MMSTTS: 99.72% (+0.00%)\n",
      "  Tortoise-TTS: 100.00% (+0.00%)\n",
      "  Elevenlabs: 99.90% (+0.00%)\n",
      "  Diff-HierVC: 100.00% (+0.00%)\n",
      "  StyleTTS2: 99.58% (+0.00%)\n",
      "  SeamlessM4T-TTS: 100.00% (+0.00%)\n",
      "  DDDM-VC: 100.00% (+0.00%)\n",
      "  MeloTTS: 100.00% (+0.00%)\n",
      "  DiffGAN: 100.00% (+0.00%)\n",
      "  202410_Elevenlabs: 100.00% (+0.00%)\n",
      "\n",
      "Additional metrics:\n",
      "  F1 Score: 0.9976 (+0.0000)\n",
      "  Recall: 0.9963 (+0.0000)\n",
      "  Precision: 0.9988 (+0.0000)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model: large_corpus_eval_new_lora_replay_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "\n",
      "Overall Accuracy: 99.15% (-0.61%)\n",
      "\n",
      "Accuracy by group:\n",
      "  SNS2024: 96.08% (-2.04%)\n",
      "  AIHUB: 99.61% (-0.39%)\n",
      "  InWild: 99.33% (+0.19%)\n",
      "  LibriSpeech: 99.98% (-0.01%)\n",
      "  VCTK: 99.78% (+0.18%)\n",
      "  LJSpeech: 94.38% (+0.31%)\n",
      "  CommonVoice: 99.62% (-0.01%)\n",
      "  OpenVoice: 96.15% (-3.85%)\n",
      "  InWild_spoof: 95.86% (-3.79%)\n",
      "  ASVspoof2019_spoof: 99.79% (-0.21%)\n",
      "  QuickVC: 97.21% (-2.75%)\n",
      "  AIHUB_VITS: 99.58% (-0.38%)\n",
      "  StarGANv2-VC: 100.00% (+0.00%)\n",
      "  VITS-TTS: 97.90% (-1.59%)\n",
      "  MMSTTS: 92.64% (-7.08%)\n",
      "  Tortoise-TTS: 99.92% (-0.08%)\n",
      "  Elevenlabs: 99.88% (-0.03%)\n",
      "  Diff-HierVC: 100.00% (+0.00%)\n",
      "  StyleTTS2: 97.66% (-1.92%)\n",
      "  SeamlessM4T-TTS: 98.64% (-1.36%)\n",
      "  DDDM-VC: 99.69% (-0.31%)\n",
      "  MeloTTS: 98.75% (-1.25%)\n",
      "  DiffGAN: 100.00% (+0.00%)\n",
      "  202410_Elevenlabs: 99.94% (-0.06%)\n",
      "\n",
      "Additional metrics:\n",
      "  F1 Score: 0.9916 (-0.0060)\n",
      "  Recall: 0.9961 (-0.0002)\n",
      "  Precision: 0.9871 (-0.0117)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Model: large_corpus_eval_new_lora_v4_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "\n",
      "Overall Accuracy: 99.76% (+0.00%)\n",
      "\n",
      "Accuracy by group:\n",
      "  SNS2024: 96.55% (-1.57%)\n",
      "  AIHUB: 99.99% (-0.01%)\n",
      "  InWild: 99.44% (+0.29%)\n",
      "  LibriSpeech: 99.99% (+0.00%)\n",
      "  VCTK: 99.69% (+0.09%)\n",
      "  LJSpeech: 97.81% (+3.75%)\n",
      "  CommonVoice: 99.78% (+0.15%)\n",
      "  OpenVoice: 100.00% (+0.00%)\n",
      "  InWild_spoof: 99.43% (-0.22%)\n",
      "  ASVspoof2019_spoof: 99.97% (-0.03%)\n",
      "  QuickVC: 100.00% (+0.04%)\n",
      "  AIHUB_VITS: 99.80% (-0.15%)\n",
      "  StarGANv2-VC: 100.00% (+0.00%)\n",
      "  VITS-TTS: 98.93% (-0.55%)\n",
      "  MMSTTS: 99.10% (-0.62%)\n",
      "  Tortoise-TTS: 100.00% (+0.00%)\n",
      "  Elevenlabs: 99.90% (+0.00%)\n",
      "  Diff-HierVC: 100.00% (+0.00%)\n",
      "  StyleTTS2: 98.74% (-0.84%)\n",
      "  SeamlessM4T-TTS: 100.00% (+0.00%)\n",
      "  DDDM-VC: 100.00% (+0.00%)\n",
      "  MeloTTS: 99.62% (-0.38%)\n",
      "  DiffGAN: 100.00% (+0.00%)\n",
      "  202410_Elevenlabs: 100.00% (+0.00%)\n",
      "\n",
      "Additional metrics:\n",
      "  F1 Score: 0.9976 (+0.0000)\n",
      "  Recall: 0.9976 (+0.0013)\n",
      "  Precision: 0.9976 (-0.0012)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# Define the original model name for comparison\n",
    "ORIGINAL_MODEL = \"Largecorpus_new_xlsr_conformertcm_multiview_large_corpus_s202412\"\n",
    "\n",
    "# Load metadata file with ground truth labels\n",
    "def load_metadata():\n",
    "    metadata_path = \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/protocol.txt\"\n",
    "    metadata = pd.read_csv(metadata_path, sep=\" \", header=None)\n",
    "    metadata.columns = [\"utt\", \"subset\", \"label\"]\n",
    "    metadata['utt_id'] = metadata['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    #\n",
    "    meta_csv = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/meta_nov27.csv\", sep=\",\")\n",
    "    \n",
    "    # Merge with metadata to get ground truth labels\n",
    "    metadata = metadata.merge(meta_csv, left_on='utt_id', right_on='utt')\n",
    "    #print(metadata.columns)#\n",
    "    # rename subset_x to subset\n",
    "    metadata.rename(columns={'subset_x': 'subset'}, inplace=True)\n",
    "    # rename label_y to label\n",
    "    metadata.rename(columns={'label_y': 'label'}, inplace=True)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Process a single prediction file and return results DataFrame\n",
    "def process_prediction_file(score_file, metadata_df):\n",
    "    # Load predictions\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "    \n",
    "    # Extract utterance ID for merging\n",
    "    pred_df['utt_id'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    # Merge with metadata to get ground truth labels\n",
    "    merged_df = pred_df.merge(metadata_df, on='utt_id', how='left')\n",
    "    \n",
    "    # Determine prediction based on scores\n",
    "    merged_df['pred'] = merged_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "    \n",
    "    # Filter evaluation subset if needed\n",
    "    eval_df = merged_df[merged_df['subset'] == 'eval'].copy()\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "# Calculate accuracy metrics for a dataset\n",
    "def calculate_metrics(df, group_column=None):\n",
    "    results = {}\n",
    "    \n",
    "    # Overall accuracy\n",
    "    results['overall'] = accuracy_score(df[\"label\"], df[\"pred\"]) * 100\n",
    "    \n",
    "    # Group-wise accuracy if specified\n",
    "    if group_column and group_column in df.columns:\n",
    "        group_results = {}\n",
    "        for group in df[group_column].unique():\n",
    "            group_df = df[df[group_column] == group]\n",
    "            group_acc = accuracy_score(group_df[\"label\"], group_df[\"pred\"]) * 100\n",
    "            group_results[group] = group_acc\n",
    "        results['groups'] = group_results\n",
    "    \n",
    "    # Additional metrics\n",
    "    results['f1'] = f1_score(df['label'], df['pred'], pos_label='bonafide')\n",
    "    results['recall'] = recall_score(df['label'], df['pred'], pos_label='bonafide')\n",
    "    results['precision'] = precision_score(df['label'], df['pred'], pos_label='bonafide')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Print results with comparison to original model\n",
    "def print_results(model_results, model_name, original_results=None):\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    # Overall accuracy\n",
    "    print(f\"\\nOverall Accuracy: {model_results['overall']:.2f}%\", end=\"\")\n",
    "    if original_results and model_name != ORIGINAL_MODEL:\n",
    "        diff = model_results['overall'] - original_results['overall']\n",
    "        sign = \"+\" if diff >= 0 else \"\"\n",
    "        print(f\" ({sign}{diff:.2f}%)\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    # Group accuracy if available\n",
    "    if 'groups' in model_results:\n",
    "        print(\"\\nAccuracy by group:\")\n",
    "        for group, acc in model_results['groups'].items():\n",
    "            print(f\"  {group}: {acc:.2f}%\", end=\"\")\n",
    "            if original_results and 'groups' in original_results and model_name != ORIGINAL_MODEL:\n",
    "                if group in original_results['groups']:\n",
    "                    diff = acc - original_results['groups'][group]\n",
    "                    sign = \"+\" if diff >= 0 else \"\"\n",
    "                    print(f\" ({sign}{diff:.2f}%)\")\n",
    "                else:\n",
    "                    print()\n",
    "            else:\n",
    "                print()\n",
    "    \n",
    "    # Additional metrics\n",
    "    print(\"\\nAdditional metrics:\")\n",
    "    print(f\"  F1 Score: {model_results['f1']:.4f}\", end=\"\")\n",
    "    if original_results and model_name != ORIGINAL_MODEL:\n",
    "        diff = model_results['f1'] - original_results['f1']\n",
    "        sign = \"+\" if diff >= 0 else \"\"\n",
    "        print(f\" ({sign}{diff:.4f})\")\n",
    "    else:\n",
    "        print()\n",
    "        \n",
    "    print(f\"  Recall: {model_results['recall']:.4f}\", end=\"\")\n",
    "    if original_results and model_name != ORIGINAL_MODEL:\n",
    "        diff = model_results['recall'] - original_results['recall']\n",
    "        sign = \"+\" if diff >= 0 else \"\"\n",
    "        print(f\" ({sign}{diff:.4f})\")\n",
    "    else:\n",
    "        print()\n",
    "        \n",
    "    print(f\"  Precision: {model_results['precision']:.4f}\", end=\"\")\n",
    "    if original_results and model_name != ORIGINAL_MODEL:\n",
    "        diff = model_results['precision'] - original_results['precision']\n",
    "        sign = \"+\" if diff >= 0 else \"\"\n",
    "        print(f\" ({sign}{diff:.4f})\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "def main():\n",
    "    # Load metadata\n",
    "    print(\"Loading metadata...\")\n",
    "    metadata_df = load_metadata()\n",
    "    \n",
    "    # Get prediction files\n",
    "    BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/largecorpus\"\n",
    "    prediction_files = [os.path.join(BASE_DIR, f) for f in os.listdir(BASE_DIR)]\n",
    "    prediction_files = sorted(prediction_files)\n",
    "    \n",
    "    # Process each model's predictions\n",
    "    all_results = {}\n",
    "    original_model_results = None\n",
    "    \n",
    "    for score_file in prediction_files:\n",
    "        # Extract model name from file path\n",
    "        model_name = os.path.basename(score_file)\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "        \n",
    "        # Process predictions\n",
    "        results_df = process_prediction_file(score_file, metadata_df)\n",
    "        \n",
    "        # Calculate metrics (assuming 'group' column exists in metadata_df)\n",
    "        metrics = calculate_metrics(results_df, group_column='group')\n",
    "        all_results[model_name] = metrics\n",
    "        \n",
    "        # Save original model results for comparison\n",
    "        if ORIGINAL_MODEL in model_name:\n",
    "            original_model_results = metrics\n",
    "    \n",
    "    # Print results with comparison\n",
    "    for model_name, metrics in all_results.items():\n",
    "        print_results(metrics, model_name, original_model_results)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new elevenlabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/new_elevenlabs/Apr_hungle_elevenlabs_4000s_new_lora_replay_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "4000\n",
      "Overall Accuracy: 97.82\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/new_elevenlabs/Apr_hungle_elevenlabs_4000s_new_lora_v4_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "4000\n",
      "Overall Accuracy: 98.40\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/new_elevenlabs/Apr_hungle_elevenlabs_4000s_new_lora_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "4000\n",
      "Overall Accuracy: 98.47\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/new_elevenlabs/Apr_hungle_elevenlabs_4000s_new_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "4000\n",
      "Overall Accuracy: 96.15\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/new_elevenlabs/hungle_elevenlabs_april_xlsr_vib_large_corpus.txt\n",
      "4000\n",
      "Overall Accuracy: 55.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/Apr_hungle_elevenlabs_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# Fix: Save full_df after parsing the utt column, not overwrite it\n",
    "full_df['utt_id'] = full_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/new_elevenlabs\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    #score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    pred_df['utt_id'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    # merge meta_csv with pred_df - note: meta_csv is not defined in your code\n",
    "    # Assuming meta_csv is supposed to be full_df, otherwise define meta_csv\n",
    "    pred_df = pred_df.merge(full_df, left_on='utt_id', right_on='utt_id', how='left')\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt (note: eval_df is not defined in your code)\n",
    "    # Let's assume res_df is already the result of the merge above\n",
    "    \n",
    "\n",
    "    res_df = pred_df\n",
    "    print(len(res_df))\n",
    "    \n",
    "    # Filter on eval subset\n",
    "    pred_df = pred_df[pred_df['subset'] == 'eval']\n",
    "    \n",
    "    # rename label_x to label\n",
    "    #res_df.rename(columns={'label_x': 'label'}, inplace=True)\n",
    "\n",
    "    # Overall accuracy\n",
    "    print(\"Overall Accuracy: {:.2f}\".format(\n",
    "        accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bonafide voxceleb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing score file: /nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/voxceleb_bonafide/Apr_hungle_elevenlabs_4000s_bonafide_new_lora_replay_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Overall Accuracy for Apr_hungle_elevenlabs_4000s_bonafide_new_lora_replay_xlsr_conformertcm_multiview_large_corpus_s202412.txt: 99.16%\n",
      "Processing score file: /nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/voxceleb_bonafide/voxceleb2_bonafide_from_Apr_hungle_new_lora_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Overall Accuracy for voxceleb2_bonafide_from_Apr_hungle_new_lora_xlsr_conformertcm_multiview_large_corpus_s202412.txt: 99.12%\n",
      "Processing score file: /nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/voxceleb_bonafide/voxceleb2_bonafide_from_Apr_hungle_new_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Overall Accuracy for voxceleb2_bonafide_from_Apr_hungle_new_xlsr_conformertcm_multiview_large_corpus_s202412.txt: 99.43%\n",
      "Processing score file: /nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/voxceleb_bonafide/voxceleb_bonafide_Apr_hungle_elevenlabs_4000s_new_lora_v4_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "Overall Accuracy for voxceleb_bonafide_Apr_hungle_elevenlabs_4000s_new_lora_v4_xlsr_conformertcm_multiview_large_corpus_s202412.txt: 99.43%\n",
      "Processing score file: /nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/voxceleb_bonafide/voxceleb_bonafide_Apr_hungle_elevenlabs_april_xlsr_vib_large_corpus.txt\n",
      "Overall Accuracy for voxceleb_bonafide_Apr_hungle_elevenlabs_april_xlsr_vib_large_corpus.txt: 95.05%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the protocol file\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/notebooks/voxceleb2_bonafide_from_Apr_hungle_elevenlabs_protocol.txt\",\n",
    "    sep=\" \",\n",
    "    header=None\n",
    ")\n",
    "full_df.columns = [\"utt_id\", \"subset\", \"label\"]\n",
    "\n",
    "# Remove duplicates from full_df based on utt_id\n",
    "full_df = full_df.drop_duplicates(subset=['utt_id'])\n",
    "#print(f\"Full dataframe size after deduplication: {len(full_df)}\")\n",
    "\n",
    "# Base directory for score files\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/voxceleb_bonafide\"\n",
    "list_dir = sorted(os.path.join(BASE_DIR, x) for x in os.listdir(BASE_DIR))\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(f\"Processing score file: {score_file}\")\n",
    "    \n",
    "    # Load prediction file\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    #print(\"Pred_df length: \", len(pred_df))\n",
    "    pred_df.columns = [\"utt_id\", \"spoof\", \"score\"]\n",
    "\n",
    "    \n",
    "    # Remove duplicates from pred_df based on utt_id\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt_id'])\n",
    "    #print(f\"Prediction dataframe size after deduplication: {len(pred_df)}\")\n",
    "    \n",
    "    # Merge pred_df with full_df on utt_id\n",
    "    res_df = pred_df.merge(\n",
    "        full_df[['utt_id', 'subset', 'label']],\n",
    "        on='utt_id',\n",
    "        how='inner'  # Use inner to only keep matching utt_id\n",
    "    )\n",
    "    \n",
    "    # Check for duplicates after merge\n",
    "    if res_df['utt_id'].duplicated().any():\n",
    "        print(f\"Warning: Duplicates found in merged dataframe for {score_file}\")\n",
    "        res_df = res_df.drop_duplicates(subset=['utt_id'])\n",
    "        #print(f\"Merged dataframe size after deduplication: {len(res_df)}\")\n",
    "    \n",
    "    # Predict bonafide or spoof based on spoof and score\n",
    "    res_df['pred'] = res_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1\n",
    "    )\n",
    "    \n",
    "    # Filter for eval subset\n",
    "    eval_df = res_df[res_df['subset'] == 'eval']\n",
    "    #print(f\"Eval subset size: {len(eval_df)}\")\n",
    "    \n",
    "    # Calculate and print accuracy\n",
    "    if not eval_df.empty:\n",
    "        accuracy = accuracy_score(eval_df[\"label\"], eval_df[\"pred\"]) * 100\n",
    "        print(f\"Overall Accuracy for {os.path.basename(score_file)}: {accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(f\"No eval subset data for {os.path.basename(score_file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter list audio fail to detect from MDT_s202412_v2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_lora_xlsr_conformertcm_mdt_large_corpus_s202412_v2.txt\n",
      "Accuracy: 62.01\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'spoof', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "    \n",
    "    # min_score of cm_scores['score']\n",
    "    print(\"min_score: \", min(cm_scores['score']))\n",
    "    # max_score of cm_scores['score']\n",
    "    print(\"max_score: \", max(cm_scores['score']))\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/AIHub/protocol_for_cm.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# Filter with utt startwith Elevenlabs only\n",
    "full_df = full_df[full_df[\"utt\"].str.startswith(\"Elevenlabs\")]\n",
    "\n",
    "# filter with subset = \"eval\"\n",
    "full_df = full_df[full_df[\"subset\"] == \"eval\"]\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/aihub_new/AIHUB_new_lora_xlsr_conformertcm_mdt_large_corpus_s202412_v2.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "#pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")\n",
    "    \n",
    "# Filter fail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoreanReadSpeechCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/KoreanReadSpeechCorpus/KoreanReadSpeechCorpus_april_xlsr_vib_large_corpus_s202412.txt\n",
      "1697\n",
      "Overall Accuracy: 92.40\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/KoreanReadSpeechCorpus/KoreanReadSpeechCorpus_lorav4_xlsr_conformertcm_multiview_large_corpus_s202412.txt\n",
      "1697\n",
      "Overall Accuracy: 77.02\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/KoreanReadSpeechCorpus/KoreanReadSpeechCorpus_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# Fix: Save full_df after parsing the utt column, not overwrite it\n",
    "full_df['utt_id'] = full_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl/KoreanReadSpeechCorpus\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    #score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    pred_df['utt_id'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "    \n",
    "    # merge meta_csv with pred_df - note: meta_csv is not defined in your code\n",
    "    # Assuming meta_csv is supposed to be full_df, otherwise define meta_csv\n",
    "    pred_df = pred_df.merge(full_df, left_on='utt_id', right_on='utt_id', how='left')\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt (note: eval_df is not defined in your code)\n",
    "    # Let's assume res_df is already the result of the merge above\n",
    "    \n",
    "\n",
    "    res_df = pred_df\n",
    "    print(len(res_df))\n",
    "    \n",
    "    # Filter on eval subset\n",
    "    pred_df = pred_df[pred_df['subset'] == 'eval']\n",
    "    \n",
    "    # rename label_x to label\n",
    "    #res_df.rename(columns={'label_x': 'label'}, inplace=True)\n",
    "\n",
    "    # Overall accuracy\n",
    "    print(\"Overall Accuracy: {:.2f}\".format(\n",
    "        accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
