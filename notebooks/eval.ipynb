{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InTheWild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AASISST-SSL Multiview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild/itw_xlsr_aasist_multiview_conf-2_epoch15_1s.txt\n",
      "eer: 15.056856750662389\tthreshold: -2.7421875\n",
      "\n",
      "0.1505685675066239\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild/itw_xlsr_aasist_multiview_conf-2_epoch15_2s.txt\n",
      "eer: 9.968987226911425\tthreshold: -3.037109375\n",
      "\n",
      "0.09968987226911424\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild/itw_xlsr_aasist_multiview_conf-2_epoch15_3s.txt\n",
      "eer: 9.326796296235376\tthreshold: -2.986328125\n",
      "\n",
      "0.09326796296235375\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild/itw_xlsr_aasist_multiview_conf-2_epoch15_4s.txt\n",
      "eer: 9.480607844892198\tthreshold: -2.91796875\n",
      "\n",
      "0.09480607844892197\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/datad/Datasets/in_the_wild.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAISST-SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/inthewild\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/datad/Datasets/in_the_wild.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dataa/Datasets/in_the_wild.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out_data)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m eer_cm\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43meval_to_score_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/hungdx/tcm_add/Scores/avg_5_best_baseline_itw_var.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/dataa/Datasets/in_the_wild.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36meval_to_score_file\u001b[0;34m(score_file, cm_key_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_to_score_file\u001b[39m(score_file, cm_key_file):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# CM key file is the metadata file that contains the ground truth labels for the eval set\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# score file is the output of the system that contains the scores for the eval set\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# phase is the phase of the eval set (dev or eval)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     cm_data \u001b[38;5;241m=\u001b[39m \u001b[43mpandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcm_key_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     submission_scores \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m     15\u001b[0m         score_file, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, skipinitialspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# check here for progress vs eval set\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/dataa/Datasets/in_the_wild.txt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/tcm_add/Scores/avg_5_best_baseline_itw_var.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 8.143285711606419\tthreshold: -3.943359375\n",
      "\n",
      "0.08143285711606418\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/Lightning-hydra/logs/itw_eval_aasistssl_multiview_conf-2-var_evalvar.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 12.281341207367685\tthreshold: -2.8203125\n",
      "\n",
      "0.12281341207367685\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/Lightning-hydra/logs/eval/in_the_wild_xlsr_aasist_multiview_conf-2_avg_var.txt\",\n",
    "      \"/datad/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 16.20590869852626\tthreshold: -3.991901397705078\n",
      "\n",
      "0.1620590869852626\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/tcm_add/Scores/avg_5_best_4_varmultiview_itw_var.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSD_Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 25.30815091379387\tthreshold: -1.560262680053711\n",
      "\n",
      "0.2530815091379387\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_4_multiview_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 18.64048962177763\tthreshold: -0.8904756307601929\n",
      "\n",
      "0.18640489621777628\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_baseline_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.8659293089861955\tthreshold: -0.1556183844804763\n",
      "\n",
      "0.008659293089861955\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.6156233094713074\tthreshold: -0.1305226236581802\n",
      "\n",
      "0.006156233094713074\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 10.001487928865409\tthreshold: 0.2573180198669433\n",
      "\n",
      "0.10001487928865409\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_1s.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 1.1838866621967168\tthreshold: 0.1702563166618347\n",
      "\n",
      "0.011838866621967169\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_var.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\n",
      "eer: 0.8659293089861955\tthreshold: -0.1556183844804763\n",
      "\n",
      "0.008659293089861955\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_1s.txt\n",
      "eer: 10.001487928865409\tthreshold: 0.2573180198669433\n",
      "\n",
      "0.10001487928865409\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_2s.txt\n",
      "eer: 2.3501879815915236\tthreshold: 0.3509694933891296\n",
      "\n",
      "0.023501879815915238\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_3s.txt\n",
      "eer: 1.0986499656991737\tthreshold: 0.608413577079773\n",
      "\n",
      "0.010986499656991737\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_var.txt\n",
      "eer: 1.1838866621967168\tthreshold: 0.1702563166618347\n",
      "\n",
      "0.011838866621967169\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "import os\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.5750324972539275\tthreshold: 0.6296007037162781\n",
      "\n",
      "0.005750324972539275\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_4_train_multiview_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL Multiview for DSD Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/multiview_dsdcoprus.txt\n",
      "eer: 0.5777385514017528\tthreshold: -0.1685791015625\n",
      "\n",
      "0.005777385514017528\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/multiview_dsdcoprus_1s.txt\n",
      "eer: 2.063346241922039\tthreshold: 0.35107421875\n",
      "\n",
      "0.02063346241922039\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/multiview_dsdcoprus_2s.txt\n",
      "eer: 0.8172203343253397\tthreshold: 0.11444091796875\n",
      "\n",
      "0.008172203343253397\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/multiview_dsdcoprus_3s.txt\n",
      "eer: 0.6372717426539101\tthreshold: 0.1741943359375\n",
      "\n",
      "0.006372717426539101\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/multiview_dsdcoprus_var.txt\n",
      "eer: 0.488438764523517\tthreshold: -0.1036376953125\n",
      "\n",
      "0.0048843876452351696\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformertcm Multiview for DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/conformertcm_multiview/avg_5_best_4_train_multiview_dsd_corpus.txt\n",
      "eer: 0.5750324972539275\tthreshold: 0.6296007037162781\n",
      "\n",
      "0.005750324972539275\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_multiview/avg_5_best_4_train_multiview_dsd_corpus_1s.txt\n",
      "eer: 2.4557240933567117\tthreshold: -0.0493079088628292\n",
      "\n",
      "0.024557240933567117\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_multiview/avg_5_best_4_train_multiview_dsd_corpus_2s.txt\n",
      "eer: 0.9200503919427021\tthreshold: 0.2275602966547012\n",
      "\n",
      "0.009200503919427021\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_multiview/avg_5_best_4_train_multiview_dsd_corpus_3s.txt\n",
      "eer: 0.645389905097386\tthreshold: 0.5721814036369324\n",
      "\n",
      "0.0064538990509738605\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_multiview/avg_5_best_4_train_multiview_dsd_corpus_var.txt\n",
      "eer: 0.44514189815831173\tthreshold: 1.5501747131347656\n",
      "\n",
      "0.004451418981583117\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/conformertcm_multiview\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformertcm base for DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\n",
      "eer: 0.6156233094713074\tthreshold: -0.1305226236581802\n",
      "\n",
      "0.006156233094713074\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_var.txt\n",
      "eer: 0.9471109334209554\tthreshold: 1.0857112407684326\n",
      "\n",
      "0.009471109334209554\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores/avg_5_best_4_baseline_train_dsd_corpus_1s.txt\n",
      "eer: 9.341218735113937\tthreshold: -3.316864252090454\n",
      "\n",
      "0.09341218735113938\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores/avg_5_best_4_baseline_train_dsd_corpus_2s.txt\n",
      "eer: 2.6897937679846473\tthreshold: -3.237324476242065\n",
      "\n",
      "0.026897937679846474\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores/avg_5_best_4_baseline_train_dsd_corpus_3s.txt\n",
      "eer: 0.8442808758035929\tthreshold: -1.81674599647522\n",
      "\n",
      "0.008442808758035929\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train/1s_eval.txt\n",
      "eer: 3.335183673082034\tthreshold: 0.0565097704529762\n",
      "\n",
      "0.03335183673082034\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train/ConformerTCM_train_dsd_corpus_1s_avg_best_conformertcm_train_dsd_corpus_1s_eval_2s.txt\n",
      "eer: 7.57282437698695\tthreshold: 0.5469886660575867\n",
      "\n",
      "0.0757282437698695\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train/ConformerTCM_train_dsd_corpus_1s_avg_best_conformertcm_train_dsd_corpus_1s_eval_3s.txt\n",
      "eer: 12.192018715735237\tthreshold: 0.3394620418548584\n",
      "\n",
      "0.12192018715735238\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train/ConformerTCM_train_dsd_corpus_1s_avg_best_conformertcm_train_dsd_corpus_1s_eval_4s.txt\n",
      "eer: 14.831754491144073\tthreshold: 0.3808308243751526\n",
      "\n",
      "0.14831754491144072\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train/ConformerTCM_train_dsd_corpus_2s_avg_best_conformertcm_train_dsd_corpus_2s_eval_1s.txt\n",
      "eer: 5.682665600525732\tthreshold: 0.5776277184486389\n",
      "\n",
      "0.056826656005257326\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train/ConformerTCM_train_dsd_corpus_2s_avg_best_conformertcm_train_dsd_corpus_2s_eval_2s.txt\n",
      "eer: 1.1568261207184636\tthreshold: -0.4768339991569519\n",
      "\n",
      "0.011568261207184636\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train/ConformerTCM_train_dsd_corpus_2s_avg_best_conformertcm_train_dsd_corpus_2s_eval_3s.txt\n",
      "eer: 1.0553530993339686\tthreshold: 0.041158989071846\n",
      "\n",
      "0.010553530993339685\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train/ConformerTCM_train_dsd_corpus_2s_avg_best_conformertcm_train_dsd_corpus_2s_eval_4s.txt\n",
      "eer: 1.524849484822708\tthreshold: 0.2138361930847168\n",
      "\n",
      "0.015248494848227082\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/3s_train/ConformerTCM_train_dsd_corpus_3s_avg_best_conformertcm_train_dsd_corpus_3s_eval_1s.txt\n",
      "eer: 8.242568769414763\tthreshold: -2.873412847518921\n",
      "\n",
      "0.08242568769414764\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/3s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/1s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_1s_aasist_ssl_train_1s_eval_1s.txt\n",
      "eer: 3.118699341256008\tthreshold: 0.2435822337865829\n",
      "\n",
      "0.031186993412560076\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/1s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_1s_aasist_ssl_train_1s_eval_4s.txt\n",
      "eer: 12.60198190997182\tthreshold: -1.363247632980347\n",
      "\n",
      "0.1260198190997182\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/1s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/2s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_2s_aasist_ssl_train_2s_eval_1s.txt\n",
      "eer: 6.462001176781518\tthreshold: 2.4476194381713867\n",
      "\n",
      "0.06462001176781518\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/2s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_2s_aasist_ssl_train_2s_eval_2s.txt\n",
      "eer: 1.4923768350488043\tthreshold: -0.489857941865921\n",
      "\n",
      "0.014923768350488042\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/2s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_2s_aasist_ssl_train_2s_eval_4s.txt\n",
      "eer: 1.7697433760419454\tthreshold: -1.6990463733673096\n",
      "\n",
      "0.017697433760419455\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/2s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/3s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_3s_aasist_ssl_train_3s_eval_1s.txt\n",
      "eer: 8.104560007875673\tthreshold: 0.8643546104431152\n",
      "\n",
      "0.08104560007875672\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/3s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_3s_aasist_ssl_train_3s_eval_3s.txt\n",
      "eer: 1.2542440700401754\tthreshold: -0.9507216215133668\n",
      "\n",
      "0.012542440700401752\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/3s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_3s_aasist_ssl_train_3s_eval_4s.txt\n",
      "eer: 1.3327196403271098\tthreshold: -1.521807074546814\n",
      "\n",
      "0.013327196403271098\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/3s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-corpus ConfomerTCM + Multiview + trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.20603931906037296\tthreshold: 0.2680955827236175\n",
      "\n",
      "0.0020603931906037296\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    #cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_4s.txt\"\n",
    "print(eval_to_score_file(score_file, \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_1s_rs.txt\n",
      "eer: 1.5768987193684538\tthreshold: 0.1454499810934066\n",
      "\n",
      "0.01576898719368454\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_2s.txt\n",
      "eer: 0.41951778740350937\tthreshold: 0.2718126773834228\n",
      "\n",
      "0.004195177874035094\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_3s.txt\n",
      "eer: 0.21794195791279453\tthreshold: 0.1673624962568283\n",
      "\n",
      "0.0021794195791279454\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_4s.txt\n",
      "eer: 0.20603931906037296\tthreshold: 0.2680955827236175\n",
      "\n",
      "0.0020603931906037296\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file,\n",
    "          \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.3057071754012865\tthreshold: 0.899989664554596\n",
      "\n",
      "0.003057071754012865\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_large_corpus_ori_random_start.txt\"\n",
    "print(eval_to_score_file(score_file, \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.40598974046468017\tthreshold: 0.3207651078701019\n",
      "\n",
      "0.004059897404646802\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "print(eval_to_score_file(\"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_var_itw.txt\",\n",
    "      \"/datad/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConformerTCM + train largr corpus + multiview + trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_1s_rs_mlaad.txt\n",
      "Accuracy: 80.05\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_2s_rs_mlaad.txt\n",
      "Accuracy: 85.37\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_3s_rs_mlaad.txt\n",
      "Accuracy: 86.98\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_4s_rs_mlaad.txt\n",
      "Accuracy: 87.62\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme2/Datasets/MLAAD/protocol_eval.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    # pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt\n",
    "    res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "    print(\"Accuracy: {:.2f}\".format(accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hungdx/mlaad_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_3s.txt\n",
      "Accuracy: 93.48\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/home/hungdx/MLAAD/protocol_lts.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "#for score_file in list_dir:\n",
    "score_file = \"/home/hungdx/mlaad_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_3s.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.254385417374277\tthreshold: -0.7286882996559143\n",
      "\n",
      "0.00254385417374277\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/home/hungdx/large_corpus_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_2s.txt\"\n",
    "print(eval_to_score_file(score_file, \"/data/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD_v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config1 + Nov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\n",
      "Accuracy: 95.18\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PartialSpoof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 8.932396195900877\tthreshold: 3.01953125\n",
      "\n",
      "0.08932396195900877\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_conf-2_4s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
