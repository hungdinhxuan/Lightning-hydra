{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InTheWild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AASISST-SSL Multiview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/datad/Datasets/in_the_wild.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAISST-SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/inthewild\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/datad/Datasets/in_the_wild.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/tcm_add/Scores/avg_5_best_baseline_itw_var.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/Lightning-hydra/logs/itw_eval_aasistssl_multiview_conf-2-var_evalvar.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/Lightning-hydra/logs/eval/in_the_wild_xlsr_aasist_multiview_conf-2_avg_var.txt\",\n",
    "      \"/datad/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/tcm_add/Scores/avg_5_best_4_varmultiview_itw_var.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSD_Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_4_multiview_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_baseline_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_1s.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_var.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "import os\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_4_train_multiview_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL Multiview for DSD Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformertcm Multiview for DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/conformertcm_multiview\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformertcm base for DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/3s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/1s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/2s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/3s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-corpus ConfomerTCM + Multiview + trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    #cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_4s.txt\"\n",
    "print(eval_to_score_file(score_file, \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file,\n",
    "          \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_large_corpus_ori_random_start.txt\"\n",
    "print(eval_to_score_file(score_file, \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "print(eval_to_score_file(\"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_var_itw.txt\",\n",
    "      \"/datad/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConformerTCM + train largr corpus + multiview + trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme2/Datasets/MLAAD/protocol_eval.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    # pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt\n",
    "    res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "    print(\"Accuracy: {:.2f}\".format(accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/home/hungdx/MLAAD/protocol_lts.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "#for score_file in list_dir:\n",
    "score_file = \"/home/hungdx/mlaad_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_3s.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/home/hungdx/large_corpus_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_2s.txt\"\n",
    "print(eval_to_score_file(score_file, \"/data/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDT on large-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.15174190132025808\tthreshold: -0.4258384108543396\n",
      "\n",
      "\n",
      "\n",
      "5.042064189910889 -5.569249629974365\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'spoof_score', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return cm_scores, eer_cm, th\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/train/eval/cnsl/large_corpus_xlsr_conformertcm_multiview_large_corpus_v1_4s.txt\"\n",
    "res_df, eer, th = eval_to_score_file(score_file, \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/protocol.txt\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "max_score = max(res_df['score'])\n",
    "min_score = min(res_df['score'])\n",
    "print(max_score, min_score)\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "bona = res_df[(res_df['label'] == 'bonafide')]['score']\n",
    "spoof = res_df[(res_df['label'] == 'spoof')]['score']\n",
    "\n",
    "# Plotting the distributions as line chart histograms\n",
    "plt.hist(bona, bins=100, density=True, cumulative=False, histtype='step', linewidth=1.5, label='Bonafide')\n",
    "plt.hist(spoof, bins=100, density=True, cumulative=False, histtype='step', linewidth=1.5, label='Spoofing')\n",
    "\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Scores')\n",
    "# plt.ylabel('score probability density plot')\n",
    "plt.title('EER = {:.2f}%'.format(eer*100), fontsize=16)\n",
    "# plt.legend(fontsize=14)\n",
    "# plt.stick_params(labelsize=16)\n",
    "\n",
    "# yaxis logit scale\n",
    "plt.yscale('symlog')\n",
    "\n",
    "# Adding a legend\n",
    "# plt.legend(loc='lower right')\n",
    "# Adding vertical lines\n",
    "plt.axvline(x=th, color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "# Set the x-axis tick positions and labels\n",
    "plt.xticks([-6,6], fontsize=16)\n",
    "plt.yticks([])\n",
    "plt.ylim(0,5)\n",
    "\n",
    "# plt.rcParams.update({'font.size': 16})\n",
    "# Displaying the plot\n",
    "# plt.savefig(\"figs/1e-07_3_augall_asvspoof2019_resnet_nll_2l_epoch_49.png\", dpi=600)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDT + BT on large-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.23653145909564455\tthreshold: 1.133873462677002\n",
      "\n",
      "\n",
      "\n",
      "5.076810359954834 -4.980000972747803\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'spoof_score', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return cm_scores, eer_cm, th\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/train/eval/cnsl/large_corpus_xlsr_conformertcm_multiview_large_corpus_scl_4s.txt\"\n",
    "res_df, eer, th = eval_to_score_file(score_file, \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/protocol.txt\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "max_score = max(res_df['score'])\n",
    "min_score = min(res_df['score'])\n",
    "print(max_score, min_score)\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "bona = res_df[(res_df['label'] == 'bonafide')]['score']\n",
    "spoof = res_df[(res_df['label'] == 'spoof')]['score']\n",
    "\n",
    "# Plotting the distributions as line chart histograms\n",
    "plt.hist(bona, bins=100, density=True, cumulative=False, histtype='step', linewidth=1.5, label='Bonafide')\n",
    "plt.hist(spoof, bins=100, density=True, cumulative=False, histtype='step', linewidth=1.5, label='Spoofing')\n",
    "\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Scores')\n",
    "# plt.ylabel('score probability density plot')\n",
    "plt.title('EER = {:.2f}%'.format(eer*100), fontsize=16)\n",
    "# plt.legend(fontsize=14)\n",
    "# plt.stick_params(labelsize=16)\n",
    "\n",
    "# yaxis logit scale\n",
    "plt.yscale('symlog')\n",
    "\n",
    "# Adding a legend\n",
    "# plt.legend(loc='lower right')\n",
    "# Adding vertical lines\n",
    "plt.axvline(x=th, color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "# Set the x-axis tick positions and labels\n",
    "plt.xticks([-6,6], fontsize=16)\n",
    "plt.yticks([])\n",
    "plt.ylim(0,5)\n",
    "\n",
    "# plt.rcParams.update({'font.size': 16})\n",
    "# Displaying the plot\n",
    "# plt.savefig(\"figs/1e-07_3_augall_asvspoof2019_resnet_nll_2l_epoch_49.png\", dpi=600)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD_v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config1 + Nov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AASIST-SSL conf-2 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/mlaad_v5/xlsr_aasist_multiview_conf-2_2s.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConformerTCM baseline train LA19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/nvme1/hungdx/tcm_add/Scores/avg_5_best_ConformerTCM_baseline_train_LA19_eval_2s_mlaad_v5.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConformerTCM_MDT_train_LA19_eval_1s_mlaad_v5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/nvme1/hungdx/tcm_add/Scores/avg_5_best_4_ConformerTCM_MDT_train_LA19_eval_4s_mlaad_v5.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AASIST-SSL baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "# for score_file in list_dir:\n",
    "score_file = \"/nvme1/hungdx/tcm_add/Scores/Best_LA_model_for_DF_AASIST-SSL_baseline_train_LA19_eval_3s_mlaad_v5.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(\n",
    "    accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PartialSpoof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_conf-2_var.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 10.53683200443646\tthreshold: 3.185546875\n",
      "\n",
      "0.1053683200443646\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_conf-2_eval_3s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 14.009758943085304\tthreshold: 4.177801132202148\n",
      "\n",
      "0.14009758943085304\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/tcm_add/Scores/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_eval_3s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 14.26545078194924\tthreshold: 0.3583984375\n",
      "\n",
      "0.1426545078194924\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_conf-2_chunking_3s_overlap_1.5s.txt\"\n",
    "mode = 'mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/tcm_add/Scores/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_var_eval.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_epoch15_conf-2_1s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/partialspoof_xlsr_aasist_multiview_conf-2_1s_random_start.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunking 1s overlap 0.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_overlap_0.5s_chunking.txt',\n",
    "                 sep=' ', header=None, names=['utt_id', 'bonafide_score'])\n",
    "\n",
    "# Extract the base utt_id by removing the chunk suffix\n",
    "df['base_utt_id'] = df['utt_id'].apply(\n",
    "    lambda x: '_'.join(x.split('___')[:-1]))\n",
    "\n",
    "# df2 = pd.read_csv('docs_eval_final/eval_3_augall_wavlm_conformertcm_res2net_seblock_sclnormal_large_corpus_epoch16_6s.txt', sep=' ', header=None, names=['utt_id', 'spoof_score', 'bonafide_score'])\n",
    "\n",
    "# ## Check which file is missing between df and df2\n",
    "# print(df2[~df2['utt_id'].isin(df['base_utt_id'])])\n",
    "\n",
    "# print(df['base_utt_id'].nunique())\n",
    "\n",
    "# Group by the base utt_id and calculate the average scores\n",
    "grouped_df = df.groupby('base_utt_id').agg(\n",
    "    {'bonafide_score': 'min'}).reset_index()\n",
    "\n",
    "# Rename the columns to match the final format\n",
    "grouped_df.rename(columns={'base_utt_id': 'utt_id'}, inplace=True)\n",
    "\n",
    "# grouped_df.head()\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "grouped_df.to_csv('/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_overlap_0.5s_chunking_min.txt',\n",
    "                  sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/partialspoof_xlsr_aasist_multiview_conf-2_mean_chunking_1s_overlap_0.5s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('/nvme1/hungdx/tcm_add/Scores/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_chunking_4s_overlap_2s.txt',\n",
    "                 sep=' ', header=None, names=['utt_id', 'bonafide_score'])\n",
    "\n",
    "# Extract the base utt_id by removing the chunk suffix\n",
    "df['base_utt_id'] = df['utt_id'].apply(\n",
    "    lambda x: '_'.join(x.split('_')[:-1]))\n",
    "\n",
    "# df2 = pd.read_csv('docs_eval_final/eval_3_augall_wavlm_conformertcm_res2net_seblock_sclnormal_large_corpus_epoch16_6s.txt', sep=' ', header=None, names=['utt_id', 'spoof_score', 'bonafide_score'])\n",
    "\n",
    "# ## Check which file is missing between df and df2\n",
    "# print(df2[~df2['utt_id'].isin(df['base_utt_id'])])\n",
    "\n",
    "# print(df['base_utt_id'].nunique())\n",
    "\n",
    "# Group by the base utt_id and calculate the average scores\n",
    "grouped_df = df.groupby('base_utt_id').agg(\n",
    "    {'bonafide_score': 'mean'}).reset_index()\n",
    "\n",
    "# Rename the columns to match the final format\n",
    "grouped_df.rename(columns={'base_utt_id': 'utt_id'}, inplace=True)\n",
    "\n",
    "# grouped_df.head()\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "grouped_df.to_csv('/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_mean_chunking_4s_overlap_2s.txt',\n",
    "                  sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_mean_chunking_1s_overlap_0.5s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunking 2s overlap 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/partialspoof_xlsr_aasist_multiview_conf-2_mean_chunking_2s_overlap_1s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_mean_chunking_2s_overlap_1s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking 4s overlap 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/Best_LA_model_for_DF_partialspoof_baseline_aasist_ssl_train_la19_mean_chunking_4s_overlap_2s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConformerTCM baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/chunking/partialspoof_baseline_conformertcm_train_la19_mean_chunking_4s_overlap_2_eval.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s random start inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-2_eval_1s_rs_lts.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-2_eval_1s_lts.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s mean chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-2_eval_1s_overlap_0.5s_chunking_mean.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s random start inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-3_eval_1s_rs_lts.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 1 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-3_eval_1s_lts.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-3_eval_1s_overlap_0.5s_chunking_mean.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_1s_overlap_0.5s_chunking_mean.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s rs inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_1s_rs.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 11.28509577000297\tthreshold: 5.129763126373291\n",
      "\n",
      "0.1128509577000297\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_4s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 10.143231239293574\tthreshold: 5.016756057739258\n",
      "\n",
      "0.10143231239293574\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_3s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 14.548312872320611\tthreshold: 4.678284645080566\n",
      "\n",
      "0.14548312872320612\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_2s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_overlap_0.5s_chunking_mean.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"#+\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_2s_overlap_1s_chunking.txt\"\n",
    "mode='mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "      mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_3s_overlap_1.5s_chunking.txt\"\n",
    "mode = 'mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_4s_overlap_2s_chunking.txt\"\n",
    "mode = 'min'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1s random start inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_rs.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 8.742516980502653\tthreshold: 4.965228080749512\n",
      "\n",
      "0.08742516980502653\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1.1_eval_var.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_2s_overlap_1s_chunking.txt\"\n",
    "mode = 'mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 15.026533029722906\tthreshold: -0.03892254829406738\n",
      "\n",
      "0.15026533029722905\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_3s_overlap_1.5s_chunking.txt\"\n",
    "mode = 'mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4s chunking inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_4s_overlap_2s_chunking.txt\"\n",
    "mode = 'mean'\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\",\n",
    "                         mode=mode))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 13.222516094603327\tthreshold: 4.661870002746582\n",
      "\n",
      "0.13222516094603326\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_4s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 7.221135178045704\tthreshold: 5.5662336349487305\n",
      "\n",
      "0.07221135178045704\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_3s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 11.228702580477929\tthreshold: 5.223989963531494\n",
      "\n",
      "0.1122870258047793\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_2s.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 5.356043096198929\tthreshold: 5.576591968536377\n",
      "\n",
      "0.05356043096198929\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme1/hungdx/Lightning-hydra/logs/eval/ps/ps_xlsr_aasist_multiview_conf-beta-1_eval_var.txt\"\n",
    "print(eval_to_score_file(score_file,\n",
    "      \"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD track 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conf-1.1 (RMDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_rs.txt\n",
      "eer: 35.90776548582365\tthreshold: 0.4829051196575165\n",
      "\n",
      "0.3590776548582365\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_2s_rs.txt\n",
      "eer: 28.872058545596797\tthreshold: 1.1376407146453855\n",
      "\n",
      "0.28872058545596796\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_3s_rs.txt\n",
      "eer: 25.971029810885778\tthreshold: 1.758313536643982\n",
      "\n",
      "0.25971029810885776\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_4s_rs.txt\n",
      "eer: 24.93812201801109\tthreshold: 2.519417524337769\n",
      "\n",
      "0.24938122018011089\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_var.txt\n",
      "eer: 24.771201269579443\tthreshold: 3.0062925815582275\n",
      "\n",
      "0.24771201269579443\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/rmdt\")\n",
    "\n",
    "# filter if the file is a txt file\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/rmdt/\" + x for x in score_file_list]\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_1s_chunking.txt\n",
      "eer: 31.14374479185653\tthreshold: 0.44332343339920044\n",
      "\n",
      "0.3114374479185653\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_2s_chunking.txt\n",
      "eer: 28.872058545596797\tthreshold: 1.1376407146453855\n",
      "\n",
      "0.28872058545596796\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_3s_chunking.txt\n",
      "eer: 29.58894941656256\tthreshold: 0.2161173373460769\n",
      "\n",
      "0.2958894941656256\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-beta-1.1_eval_4s_chunking.txt\n",
      "eer: 29.686146357434097\tthreshold: 2.042598009109497\n",
      "\n",
      "0.29686146357434096\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # import sys\n",
    "    # print(submission_scores_grouped_df.head())\n",
    "    # print(cm_data.head())\n",
    "    # sys.exit()\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    \n",
    "    \n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir('/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/rmdt/chunking')\n",
    "\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/rmdt/chunking/\" + x for x in score_file_list]\n",
    "\n",
    "mode = 'mean'\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\",\n",
    "                            mode=mode))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_3s.txt\n",
      "eer: 29.755870164994203\tthreshold: -0.106929525732994\n",
      "\n",
      "0.297558701649942\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_4s.txt\n",
      "eer: 30.27262876483326\tthreshold: -1.0986006259918213\n",
      "\n",
      "0.3027262876483326\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_1s.txt\n",
      "eer: 40.13329283602395\tthreshold: 2.794327735900879\n",
      "\n",
      "0.4013329283602395\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_2s.txt\n",
      "eer: 30.99916819415607\tthreshold: 2.0904321670532227\n",
      "\n",
      "0.3099916819415607\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_var.txt\n",
      "eer: 27.155471404451408\tthreshold: -1.2492107152938845\n",
      "\n",
      "0.2715547140445141\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/base\")\n",
    "\n",
    "# filter if the file is a txt file\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/base/\" + x for x in score_file_list]\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_chunking_1s.txt\n",
      "eer: 36.04904132135414\tthreshold: 0.6246887048085531\n",
      "\n",
      "0.36049041321354136\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_chunking_2s.txt\n",
      "eer: 30.99916819415607\tthreshold: 2.0904321670532227\n",
      "\n",
      "0.3099916819415607\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_chunking_3s.txt\n",
      "eer: 32.89928184508063\tthreshold: -1.0790154933929443\n",
      "\n",
      "0.3289928184508063\n",
      "\n",
      "\n",
      "Best_LA_model_for_DF_add_track2_baseline_aasist_ssl_train_la19_eval_chunking_4s.txt\n",
      "eer: 31.823513971153826\tthreshold: -1.733583927154541\n",
      "\n",
      "0.3182351397115383\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # import sys\n",
    "    # print(submission_scores_grouped_df.head())\n",
    "    # #print(cm_data.head())\n",
    "    # sys.exit()\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    \n",
    "    \n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir('/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/base/chunking')\n",
    "\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/base/chunking/\" + x for x in score_file_list]\n",
    "\n",
    "mode = 'mean'\n",
    "suffix = \"_\"\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\",\n",
    "                            mode=mode,\n",
    "                            suffix=suffix))\n",
    "                           \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_track2_xlsr_aasist_multiview_conf-2_eval_1s_rs.txt\n",
      "eer: 39.41249179608479\tthreshold: -2.74609375\n",
      "\n",
      "0.39412491796084786\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_2s_rs.txt\n",
      "eer: 33.76587829632709\tthreshold: -2.830078125\n",
      "\n",
      "0.3376587829632709\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_3s_rs.txt\n",
      "eer: 32.38252324524158\tthreshold: -2.841796875\n",
      "\n",
      "0.3238252324524158\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_4s_rs.txt\n",
      "eer: 32.3179284202617\tthreshold: -2.80078125\n",
      "\n",
      "0.323179284202617\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_var.txt\n",
      "eer: 26.288874953204953\tthreshold: -1.7490234375\n",
      "\n",
      "0.26288874953204955\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/mdt\")\n",
    "\n",
    "# filter if the file is a txt file\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/mdt/\" + x for x in score_file_list]\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_track2_xlsr_aasist_multiview_conf-2_eval_1s_chunking.txt\n",
      "eer: 38.686561773565394\tthreshold: -2.576171875\n",
      "\n",
      "0.3868656177356539\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_2s_chunking.txt\n",
      "eer: 33.33333333333333\tthreshold: -2.8125\n",
      "\n",
      "0.3333333333333333\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_3s_chunking.txt\n",
      "eer: 35.8060489691753\tthreshold: -3.06640625\n",
      "\n",
      "0.358060489691753\n",
      "\n",
      "\n",
      "add_track2_xlsr_aasist_multiview_conf-2_eval_4s_chunking.txt\n",
      "eer: 35.29441935191648\tthreshold: -2.990234375\n",
      "\n",
      "0.35294419351916484\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file, suffix=\"___\", mode='mean'):\n",
    "    \"\"\"#+\n",
    "    This function evaluates the performance of a speech spoofing detection system using the given score file and #+\n",
    "    corresponding metadata file. It calculates the Equal Error Rate (EER) and the threshold at which the EER is achieved.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    score_file (str): The path to the file containing the scores generated by the system. Each line in the file should #+\n",
    "                      contain a filename and a corresponding score.#+\n",
    "    cm_key_file (str): The path to the file containing the ground truth labels for the evaluation set. Each line in the file #+\n",
    "                       should contain a filename, a subset, and a corresponding label.#+\n",
    "    suffix (str): The suffix to be removed from the filenames in the score file to obtain the base utt_id. Default is \"___\".#+\n",
    "    mode (str): The aggregation mode to be used when calculating the scores for each base utt_id. It can be either 'mean' or 'max'. #+\n",
    "                 Default is 'mean'.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    str: A string containing the EER and the threshold at which the EER is achieved, formatted as \"eer: {eer_value}\\tthreshold: {threshold_value}\".#+\n",
    "    \"\"\"  # +\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "    # Extract the base utt_id by removing the chunk suffix\n",
    "    submission_scores['base_utt_id'] = submission_scores['filename'].apply(\n",
    "        lambda x: '_'.join(x.split(suffix)[:-1]))\n",
    "\n",
    "    # Group by the base utt_id and calculate the average scores\n",
    "    submission_scores_grouped_df = submission_scores.groupby('base_utt_id').agg(\n",
    "        {'score': mode}).reset_index()\n",
    "\n",
    "    # import sys\n",
    "    # print(submission_scores_grouped_df.head())\n",
    "    # #print(cm_data.head())\n",
    "    # sys.exit()\n",
    "    # Rename the columns to match the final format\n",
    "    submission_scores_grouped_df.rename(\n",
    "        columns={'base_utt_id': 'filename'}, inplace=True)\n",
    "\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores_grouped_df.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file_list = os.listdir(\n",
    "    '/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/mdt/chunking')\n",
    "\n",
    "score_file_list = [x for x in score_file_list if x.endswith(\".txt\")]\n",
    "\n",
    "score_file_list = [\n",
    "    \"/nvme1/hungdx/Lightning-hydra/logs/eval/add_track2/mdt/chunking/\" + x for x in score_file_list]\n",
    "\n",
    "mode = 'mean'\n",
    "suffix = \"___\"\n",
    "\n",
    "for score_file in score_file_list:\n",
    "    # print the last part of the path\n",
    "    print(score_file.split(\"/\")[-1])\n",
    "    print(eval_to_score_file(score_file,\n",
    "                             \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\",\n",
    "                             mode=mode,\n",
    "                             suffix=suffix))\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate EER group by duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## Create protocol df file\n",
    "\n",
    "index_col = 'trial'\n",
    "# =====\n",
    "# Configuration to load CM protocol and score file\n",
    "# =====\n",
    "# name of data series for procotol file\n",
    "p_names = ['speaker', index_col, 'compr', 'source', 'attack',\n",
    "                'label', 'trim', 'subset', 'vocoder', \n",
    "                'task', 'team', 'gender-pair', 'language']\n",
    "\n",
    "cm_file = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/logs/asvspoof-challenge-2021/eval-package/keys/DF/CM/trial_metadata.txt\", sep=' ', header=None)\n",
    "\n",
    "cm_file.columns = p_names\n",
    "\n",
    "# keep only following columns: [trial, subset, label]\n",
    "\n",
    "cm_file = cm_file[[index_col, 'subset', 'label']]\n",
    "\n",
    "# save to file\n",
    "cm_file.to_csv(\"ASVspoof2021_DF_protocol.txt\", sep=' ', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "\n",
    "duration_df = pd.read_csv(\"/nvme1/hungdx/Lightning-hydra/notebooks/ASVspoof2021_DF_eval_durations.csv\", sep=',')\n",
    "\n",
    "# colums are [filename, duration]\n",
    "# add a new column call group_duration based on following conditions:\n",
    "# 1. if duration < 1s, group_duration = \"<1s\"\n",
    "# 2. if duration >= 1s and duration < 2s, group_duration = \"1~2s\"\n",
    "# 3. if duration >= 2s and duration < 3s, group_duration = \"2~3s\"\n",
    "# 4. if duration >= 3s and duration < 4s, group_duration = \"3~4s\"\n",
    "# 5. if duration >= 4s and duration < 5s, group_duration = \"4~5s\"\n",
    "# 6. if duration >= 5s, group_duration = \">5s\"\n",
    "\n",
    "\n",
    "duration_df['group_duration'] = duration_df['duration'].apply(lambda x: \"<1s\" if x < 1 else (\"1~2s\" if x < 2 else (\"2~3s\" if x < 3 else (\"3~4s\" if x < 4 else (\"4~5s\" if x < 5 else \">5s\")))))\n",
    "\n",
    "\n",
    "# remove .flac extension in filename\n",
    "duration_df['filename'] = duration_df['filename'].apply(lambda x: x.split(\".\")[0])\n",
    "# save to file\n",
    "\n",
    "cm_file = pd.read_csv('ASVspoof2021_DF_protocol.txt', sep=' ', header=None)\n",
    "\n",
    "cm_file.columns = ['filename', 'subset', 'label']\n",
    "\n",
    "# merge duration_df with cm_file\n",
    "duration_df = duration_df.merge(cm_file, on='filename', how='left') \n",
    "\n",
    "duration_df = duration_df[['filename', 'subset', 'label', 'group_duration']]\n",
    "\n",
    "\n",
    "duration_df.to_csv(\"ASVspoof2021_DF_eval_durations_grouped.csv\", sep=' ', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing evaluations: 100%|| 84/84 [00:18<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Results saved to evaluation_results.json\n",
      "Visualization saved to results_visualization.png\n"
     ]
    }
   ],
   "source": [
    "import eval_metrics_DF as em\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_single_evaluation(args):\n",
    "    \"\"\"Process a single evaluation combination\"\"\"\n",
    "    score_file, subset, duration_group = args\n",
    "    try:\n",
    "        cm_data = pd.read_csv(\n",
    "            \"ASVspoof2021_DF_eval_durations_grouped.csv\", sep=' ', header=None)\n",
    "        cm_data.columns = ['filename', 'subset', 'label', 'duration_group']\n",
    "\n",
    "        submission_scores = pd.read_csv(\n",
    "            score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "        submission_scores.columns = ['filename', 'score']\n",
    "\n",
    "        cm_scores = submission_scores.merge(\n",
    "            cm_data, on='filename', how='inner')\n",
    "        cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "        cm_scores = cm_scores[cm_scores['duration_group'] == duration_group]\n",
    "\n",
    "        if len(cm_scores) == 0:\n",
    "            return {\n",
    "                'file': os.path.basename(score_file).split(\".\")[0],\n",
    "                'subset': subset,\n",
    "                'duration_group': duration_group,\n",
    "                'eer': None,\n",
    "                'threshold': None\n",
    "            }\n",
    "\n",
    "        bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "        spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "        if len(bona_cm) == 0 or len(spoof_cm) == 0:\n",
    "            return {\n",
    "                'file': os.path.basename(score_file).split(\".\")[0],\n",
    "                'subset': subset,\n",
    "                'duration_group': duration_group,\n",
    "                'eer': None,\n",
    "                'threshold': None\n",
    "            }\n",
    "\n",
    "        eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "\n",
    "        return {\n",
    "            'file': os.path.basename(score_file).split(\".\")[0],\n",
    "            'subset': subset,\n",
    "            'duration_group': duration_group,\n",
    "            'eer': float(eer_cm * 100),  # Convert to percentage\n",
    "            'threshold': float(th)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error processing {score_file} - {subset} - {duration_group}: {str(e)}\")\n",
    "        return {\n",
    "            'file': os.path.basename(score_file).split(\".\")[0],\n",
    "            'subset': subset,\n",
    "            'duration_group': duration_group,\n",
    "            'eer': None,\n",
    "            'threshold': None\n",
    "        }\n",
    "\n",
    "\n",
    "def plot_results(results_data, output_path=\"results_visualization.png\"):\n",
    "    \"\"\"Create bar chart visualization for eval subset results\"\"\"\n",
    "    # Filter for eval subset only and remove None values\n",
    "    eval_results = [r for r in results_data if r['subset']\n",
    "                    == 'eval' and r['eer'] is not None]\n",
    "\n",
    "    if not eval_results:\n",
    "        print(\"No valid eval results to plot\")\n",
    "        return\n",
    "\n",
    "    # Group results by file and duration\n",
    "    files = sorted(list(set(r['file'] for r in eval_results)))\n",
    "    durations = DURATION_GROUP\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    data = {}\n",
    "    for file in files:\n",
    "        data[file] = []\n",
    "        for duration in durations:\n",
    "            result = next((r['eer'] for r in eval_results\n",
    "                           if r['file'] == file and r['duration_group'] == duration), None)\n",
    "            data[file].append(result if result is not None else np.nan)\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    x = np.arange(len(durations))\n",
    "    width = 0.8 / len(files)\n",
    "\n",
    "    for i, (file, values) in enumerate(data.items()):\n",
    "        plt.bar(x + i * width, values, width, label=file)\n",
    "\n",
    "    plt.xlabel('Duration Groups')\n",
    "    plt.ylabel('EER (%)')\n",
    "    plt.title('EER by Duration Group for Different Systems (Eval Set)')\n",
    "    plt.xticks(x + width * (len(files) - 1) / 2, durations, rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "DURATION_GROUP = [\"<1s\", \"1~2s\", \"2~3s\", \"3~4s\", \"4~5s\", \">5s\"]\n",
    "SUBSETS = [\"eval\", \"hidden\"]\n",
    "FILE_LIST_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/var_analysis\"\n",
    "\n",
    "# Get score files\n",
    "score_file_list = [os.path.join(FILE_LIST_DIR, x)\n",
    "                    for x in os.listdir(FILE_LIST_DIR)\n",
    "                    if x.endswith(\".txt\")]\n",
    "\n",
    "# Create all combinations of parameters\n",
    "combinations = list(product(score_file_list, SUBSETS, DURATION_GROUP))\n",
    "\n",
    "# Process in parallel\n",
    "num_processes = max(1, cpu_count() - 1)  # Leave one CPU free\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    results = list(tqdm(\n",
    "        pool.imap_unordered(process_single_evaluation, combinations),\n",
    "        total=len(combinations),\n",
    "        desc=\"Processing evaluations\"\n",
    "    ))\n",
    "\n",
    "# Save results to JSON\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Create visualization\n",
    "plot_results(results)\n",
    "\n",
    "print(\"Processing complete!\")\n",
    "print(\"Results saved to evaluation_results.json\")\n",
    "print(\"Visualization saved to results_visualization.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>bonafide</th>\n",
       "      <th>spoof</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration_group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1~2s</th>\n",
       "      <td>2733</td>\n",
       "      <td>140173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2~3s</th>\n",
       "      <td>7031</td>\n",
       "      <td>186116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3~4s</th>\n",
       "      <td>6824</td>\n",
       "      <td>127953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4~5s</th>\n",
       "      <td>3652</td>\n",
       "      <td>95279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;1s</th>\n",
       "      <td>277</td>\n",
       "      <td>7038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;5s</th>\n",
       "      <td>2100</td>\n",
       "      <td>32653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label           bonafide   spoof\n",
       "duration_group                  \n",
       "1~2s                2733  140173\n",
       "2~3s                7031  186116\n",
       "3~4s                6824  127953\n",
       "4~5s                3652   95279\n",
       "<1s                  277    7038\n",
       ">5s                 2100   32653"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Deeper analysis\n",
    "import pandas as pd\n",
    "\n",
    "duration_df = pd.read_csv(\"ASVspoof2021_DF_eval_durations_grouped.csv\", sep=' ', header=None)\n",
    "duration_df.columns = ['filename', 'subset', 'label', 'duration_group']\n",
    "\n",
    "# How many files in each duration group, group by label\n",
    "duration_df.groupby(['duration_group', 'label']).size().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larcorpus eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/largecorpus_wavlm_conformertcm_adapter_mdt_ft12_4s.txt\n",
      "eer: 0.2796785249356729\tthreshold: -0.4054763317108154\n",
      "\n",
      "0.0027967852493567293\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/largecorpus_xlsr_conformertcm_adapter_mdt_ft12_4s.txt\n",
      "eer: 0.18669753092518784\tthreshold: -1.2655521631240845\n",
      "\n",
      "0.0018669753092518783\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/largecorpus_xlsr_conformertcm_adapter_mdt_ft24_4s.txt\n",
      "eer: 0.162156710316127\tthreshold: -0.8763607144355774\n",
      "\n",
      "0.0016215671031612699\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/largecorpus_xlsr_conformertcm_adapter_mdt_fz24_4s.txt\n",
      "eer: 0.6768788644908917\tthreshold: 0.1206281557679176\n",
      "\n",
      "0.0067687886449089175\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/nvme1/hungdx/Lightning-hydra/data/0_large-corpus/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlaad_v5 eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5/mlaad_v5_xlsr_conformertcm_24_lora_all_4s.txt\n",
      "Accuracy: 94.00\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5/mlaadv5_wavlm_conformertcm_adapter_mdt_ft12_4s.txt\n",
      "Accuracy: 91.61\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5/mlaadv5_xlsr_conformertcm_adapter_mdt_ft12_4s.txt\n",
      "Accuracy: 94.56\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5/mlaadv5_xlsr_conformertcm_adapter_mdt_fz24_4s.txt\n",
      "Accuracy: 90.21\n",
      "\n",
      "\n",
      "/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5/mlaadv5_xlsr_conformertcm_mdt_ft24_4s.txt\n",
      "Accuracy: 93.73\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/mlaad_v5_protocol.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "BASE_DIR = \"/nvme1/hungdx/Lightning-hydra/logs/eval/cnsl_noisy/mlaad_v5\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    #score_file = \"/home/hungdx/mlaad_v5_xlsr_conformertcm_train_large_corpus_multiview_conf-1_lts_4s.txt\"\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    # pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt\n",
    "    res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "    print(\"Accuracy: {:.2f}\".format(\n",
    "        accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
