{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InTheWild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AASISST-SSL Multiview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild/itw_xlsr_aasist_multiview_conf-2_epoch15_1s.txt\n",
      "eer: 15.056856750662389\tthreshold: -2.7421875\n",
      "\n",
      "0.1505685675066239\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild/itw_xlsr_aasist_multiview_conf-2_epoch15_2s.txt\n",
      "eer: 9.968987226911425\tthreshold: -3.037109375\n",
      "\n",
      "0.09968987226911424\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild/itw_xlsr_aasist_multiview_conf-2_epoch15_3s.txt\n",
      "eer: 9.326796296235376\tthreshold: -2.986328125\n",
      "\n",
      "0.09326796296235375\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild/itw_xlsr_aasist_multiview_conf-2_epoch15_4s.txt\n",
      "eer: 9.480607844892198\tthreshold: -2.91796875\n",
      "\n",
      "0.09480607844892197\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/inthewild\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/datad/Datasets/in_the_wild.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAISST-SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/inthewild/eval_CM_scores_file_SSL_DF_1s.txt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m score_file \u001b[38;5;129;01min\u001b[39;00m list_dir:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(score_file)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43meval_to_score_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/datad/Datasets/in_the_wild.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36meval_to_score_file\u001b[0;34m(score_file, cm_key_file)\u001b[0m\n\u001b[1;32m     22\u001b[0m bona_cm \u001b[38;5;241m=\u001b[39m cm_scores[cm_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1_y\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbonafide\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1_x\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     23\u001b[0m spoof_cm \u001b[38;5;241m=\u001b[39m cm_scores[cm_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1_y\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspoof\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1_x\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 25\u001b[0m eer_cm, th \u001b[38;5;241m=\u001b[39m \u001b[43mem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_eer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbona_cm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspoof_cm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m out_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meer: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mthreshold: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39meer_cm, th)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(out_data)\n",
      "File \u001b[0;32m/data/hungdx/Lightning-hydra/notebooks/eval_metrics_DF.py:44\u001b[0m, in \u001b[0;36mcompute_eer\u001b[0;34m(target_scores, nontarget_scores)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_eer\u001b[39m(target_scores, nontarget_scores):\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Returns equal error rate (EER) and the corresponding threshold. \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     frr, far, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_det_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnontarget_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     abs_diffs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(frr \u001b[38;5;241m-\u001b[39m far)\n\u001b[1;32m     46\u001b[0m     min_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(abs_diffs)\n",
      "File \u001b[0;32m/data/hungdx/Lightning-hydra/notebooks/eval_metrics_DF.py:37\u001b[0m, in \u001b[0;36mcompute_det_curve\u001b[0;34m(target_scores, nontarget_scores)\u001b[0m\n\u001b[1;32m     35\u001b[0m frr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;241m0\u001b[39m), tar_trial_sums \u001b[38;5;241m/\u001b[39m target_scores\u001b[38;5;241m.\u001b[39msize))  \u001b[38;5;66;03m# false rejection rates\u001b[39;00m\n\u001b[1;32m     36\u001b[0m far \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;241m1\u001b[39m), nontarget_trial_sums \u001b[38;5;241m/\u001b[39m nontarget_scores\u001b[38;5;241m.\u001b[39msize))  \u001b[38;5;66;03m# false acceptance rates\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m thresholds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39matleast_1d(all_scores[\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.001\u001b[39m), all_scores[indices]))  \u001b[38;5;66;03m# Thresholds are the sorted scores\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frr, far, thresholds\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/inthewild\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/datad/Datasets/in_the_wild.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 8.234916170745263\tthreshold: -3.04897689819336\n",
      "\n",
      "0.08234916170745263\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/tcm_add/Scores/avg_5_best_baseline_itw_var.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 8.143285711606419\tthreshold: -3.943359375\n",
      "\n",
      "0.08143285711606418\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/Lightning-hydra/logs/itw_eval_aasistssl_multiview_conf-2-var_evalvar.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 12.281341207367685\tthreshold: -2.8203125\n",
      "\n",
      "0.12281341207367685\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/Lightning-hydra/logs/eval/in_the_wild_xlsr_aasist_multiview_conf-2_avg_var.txt\",\n",
    "      \"/datad/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 16.20590869852626\tthreshold: -3.991901397705078\n",
      "\n",
      "0.1620590869852626\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "print(eval_to_score_file(\"/data/hungdx/tcm_add/Scores/avg_5_best_4_varmultiview_itw_var.txt\", \"/dataa/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSD_Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 25.30815091379387\tthreshold: -1.560262680053711\n",
      "\n",
      "0.2530815091379387\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_4_multiview_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 18.64048962177763\tthreshold: -0.8904756307601929\n",
      "\n",
      "0.18640489621777628\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_baseline_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.8659293089861955\tthreshold: -0.1556183844804763\n",
      "\n",
      "0.008659293089861955\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.6156233094713074\tthreshold: -0.1305226236581802\n",
      "\n",
      "0.006156233094713074\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 10.001487928865409\tthreshold: 0.2573180198669433\n",
      "\n",
      "0.10001487928865409\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_1s.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 1.1838866621967168\tthreshold: 0.1702563166618347\n",
      "\n",
      "0.011838866621967169\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/Lightning-hydra/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_var.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\n",
      "eer: 0.8659293089861955\tthreshold: -0.1556183844804763\n",
      "\n",
      "0.008659293089861955\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_1s.txt\n",
      "eer: 10.001487928865409\tthreshold: 0.2573180198669433\n",
      "\n",
      "0.10001487928865409\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_2s.txt\n",
      "eer: 2.3501879815915236\tthreshold: 0.3509694933891296\n",
      "\n",
      "0.023501879815915238\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_3s.txt\n",
      "eer: 1.0986499656991737\tthreshold: 0.608413577079773\n",
      "\n",
      "0.010986499656991737\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_var.txt\n",
      "eer: 1.1838866621967168\tthreshold: 0.1702563166618347\n",
      "\n",
      "0.011838866621967169\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "import os\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_baseline_scores\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.5750324972539275\tthreshold: 0.6296007037162781\n",
      "\n",
      "0.005750324972539275\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/data/hungdx/tcm_add/Scores/avg_5_best_4_train_multiview_dsd_corpus.txt\"\n",
    "cm_key_file = \"/data/hungdx/Datasets/protocol.txt\"\n",
    "\n",
    "print(eval_to_score_file(score_file, cm_key_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL Multiview for DSD Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/multiview_dsdcoprus.txt\n",
      "eer: 0.5777385514017528\tthreshold: -0.1685791015625\n",
      "\n",
      "0.005777385514017528\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/multiview_dsdcoprus_1s.txt\n",
      "eer: 2.063346241922039\tthreshold: 0.35107421875\n",
      "\n",
      "0.02063346241922039\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/multiview_dsdcoprus_2s.txt\n",
      "eer: 0.8172203343253397\tthreshold: 0.11444091796875\n",
      "\n",
      "0.008172203343253397\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/multiview_dsdcoprus_3s.txt\n",
      "eer: 0.6372717426539101\tthreshold: 0.1741943359375\n",
      "\n",
      "0.006372717426539101\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/multiview_dsdcoprus_var.txt\n",
      "eer: 0.488438764523517\tthreshold: -0.1036376953125\n",
      "\n",
      "0.0048843876452351696\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformertcm Multiview for DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/conformertcm_multiview/avg_5_best_4_train_multiview_dsd_corpus.txt\n",
      "eer: 0.5750324972539275\tthreshold: 0.6296007037162781\n",
      "\n",
      "0.005750324972539275\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_multiview/avg_5_best_4_train_multiview_dsd_corpus_1s.txt\n",
      "eer: 2.4557240933567117\tthreshold: -0.0493079088628292\n",
      "\n",
      "0.024557240933567117\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_multiview/avg_5_best_4_train_multiview_dsd_corpus_2s.txt\n",
      "eer: 0.9200503919427021\tthreshold: 0.2275602966547012\n",
      "\n",
      "0.009200503919427021\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_multiview/avg_5_best_4_train_multiview_dsd_corpus_3s.txt\n",
      "eer: 0.645389905097386\tthreshold: 0.5721814036369324\n",
      "\n",
      "0.0064538990509738605\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_multiview/avg_5_best_4_train_multiview_dsd_corpus_var.txt\n",
      "eer: 0.44514189815831173\tthreshold: 1.5501747131347656\n",
      "\n",
      "0.004451418981583117\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/conformertcm_multiview\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformertcm base for DSD Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus.txt\n",
      "eer: 0.6156233094713074\tthreshold: -0.1305226236581802\n",
      "\n",
      "0.006156233094713074\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_var.txt\n",
      "eer: 0.9471109334209554\tthreshold: 1.0857112407684326\n",
      "\n",
      "0.009471109334209554\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores/avg_5_best_4_baseline_train_dsd_corpus_1s.txt\n",
      "eer: 9.341218735113937\tthreshold: -3.316864252090454\n",
      "\n",
      "0.09341218735113938\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores/avg_5_best_4_baseline_train_dsd_corpus_2s.txt\n",
      "eer: 2.6897937679846473\tthreshold: -3.237324476242065\n",
      "\n",
      "0.026897937679846474\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores/avg_5_best_4_baseline_train_dsd_corpus_3s.txt\n",
      "eer: 0.8442808758035929\tthreshold: -1.81674599647522\n",
      "\n",
      "0.008442808758035929\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/conformertcm_baseline_scores\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train/1s_eval.txt\n",
      "eer: 3.335183673082034\tthreshold: 0.0565097704529762\n",
      "\n",
      "0.03335183673082034\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train/ConformerTCM_train_dsd_corpus_1s_avg_best_conformertcm_train_dsd_corpus_1s_eval_2s.txt\n",
      "eer: 7.57282437698695\tthreshold: 0.5469886660575867\n",
      "\n",
      "0.0757282437698695\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train/ConformerTCM_train_dsd_corpus_1s_avg_best_conformertcm_train_dsd_corpus_1s_eval_3s.txt\n",
      "eer: 12.192018715735237\tthreshold: 0.3394620418548584\n",
      "\n",
      "0.12192018715735238\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train/ConformerTCM_train_dsd_corpus_1s_avg_best_conformertcm_train_dsd_corpus_1s_eval_4s.txt\n",
      "eer: 14.831754491144073\tthreshold: 0.3808308243751526\n",
      "\n",
      "0.14831754491144072\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/1s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train/ConformerTCM_train_dsd_corpus_2s_avg_best_conformertcm_train_dsd_corpus_2s_eval_1s.txt\n",
      "eer: 5.682665600525732\tthreshold: 0.5776277184486389\n",
      "\n",
      "0.056826656005257326\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train/ConformerTCM_train_dsd_corpus_2s_avg_best_conformertcm_train_dsd_corpus_2s_eval_2s.txt\n",
      "eer: 1.1568261207184636\tthreshold: -0.4768339991569519\n",
      "\n",
      "0.011568261207184636\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train/ConformerTCM_train_dsd_corpus_2s_avg_best_conformertcm_train_dsd_corpus_2s_eval_3s.txt\n",
      "eer: 1.0553530993339686\tthreshold: 0.041158989071846\n",
      "\n",
      "0.010553530993339685\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train/ConformerTCM_train_dsd_corpus_2s_avg_best_conformertcm_train_dsd_corpus_2s_eval_4s.txt\n",
      "eer: 1.524849484822708\tthreshold: 0.2138361930847168\n",
      "\n",
      "0.015248494848227082\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/2s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConformerTCM for DSD Corpus train with 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/3s_train/ConformerTCM_train_dsd_corpus_3s_avg_best_conformertcm_train_dsd_corpus_3s_eval_1s.txt\n",
      "eer: 8.242568769414763\tthreshold: -2.873412847518921\n",
      "\n",
      "0.08242568769414764\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/notebooks/xlsr_conformertcm/3s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/1s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_1s_aasist_ssl_train_1s_eval_1s.txt\n",
      "eer: 3.118699341256008\tthreshold: 0.2435822337865829\n",
      "\n",
      "0.031186993412560076\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/1s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_1s_aasist_ssl_train_1s_eval_4s.txt\n",
      "eer: 12.60198190997182\tthreshold: -1.363247632980347\n",
      "\n",
      "0.1260198190997182\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/1s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/2s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_2s_aasist_ssl_train_2s_eval_1s.txt\n",
      "eer: 6.462001176781518\tthreshold: 2.4476194381713867\n",
      "\n",
      "0.06462001176781518\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/2s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_2s_aasist_ssl_train_2s_eval_2s.txt\n",
      "eer: 1.4923768350488043\tthreshold: -0.489857941865921\n",
      "\n",
      "0.014923768350488042\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/2s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_2s_aasist_ssl_train_2s_eval_4s.txt\n",
      "eer: 1.7697433760419454\tthreshold: -1.6990463733673096\n",
      "\n",
      "0.017697433760419455\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/2s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AASIST-SSL for DSD Corpus train with 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/3s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_3s_aasist_ssl_train_3s_eval_1s.txt\n",
      "eer: 8.104560007875673\tthreshold: 0.8643546104431152\n",
      "\n",
      "0.08104560007875672\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/3s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_3s_aasist_ssl_train_3s_eval_3s.txt\n",
      "eer: 1.2542440700401754\tthreshold: -0.9507216215133668\n",
      "\n",
      "0.012542440700401752\n",
      "\n",
      "\n",
      "/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/3s_train/AASIST_SSL_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_dsd_corpus_train_3s_aasist_ssl_train_3s_eval_4s.txt\n",
      "eer: 1.3327196403271098\tthreshold: -1.521807074546814\n",
      "\n",
      "0.013327196403271098\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/logs/aasist_ssl_multiview/dsd_corpus/3s_train\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file, \"/data/hungdx/Datasets/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-corpus ConfomerTCM + Multiview + trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.20603931906037296\tthreshold: 0.2680955827236175\n",
      "\n",
      "0.0020603931906037296\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    #cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_4s.txt\"\n",
    "print(eval_to_score_file(score_file, \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_1s_rs.txt\n",
      "eer: 1.5768987193684538\tthreshold: 0.1454499810934066\n",
      "\n",
      "0.01576898719368454\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_2s.txt\n",
      "eer: 0.41951778740350937\tthreshold: 0.2718126773834228\n",
      "\n",
      "0.004195177874035094\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_3s.txt\n",
      "eer: 0.21794195791279453\tthreshold: 0.1673624962568283\n",
      "\n",
      "0.0021794195791279454\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_4s.txt\n",
      "eer: 0.20603931906037296\tthreshold: 0.2680955827236175\n",
      "\n",
      "0.0020603931906037296\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    print(score_file)\n",
    "    print(eval_to_score_file(score_file,\n",
    "          \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.3057071754012865\tthreshold: 0.899989664554596\n",
      "\n",
      "0.003057071754012865\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_large_corpus_ori_random_start.txt\"\n",
    "print(eval_to_score_file(score_file, \"/nvme2/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.40598974046468017\tthreshold: 0.3207651078701019\n",
      "\n",
      "0.004059897404646802\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on=0, right_on=0, how='inner')\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
    "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "print(eval_to_score_file(\"/nvme2/hungdx/tcm_add/Scores/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_var_itw.txt\",\n",
    "      \"/datad/Datasets/in_the_wild.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConformerTCM + train largr corpus + multiview + trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_1s_rs_mlaad.txt\n",
      "Accuracy: 80.05\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_2s_rs_mlaad.txt\n",
      "Accuracy: 85.37\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_3s_rs_mlaad.txt\n",
      "Accuracy: 86.98\n",
      "\n",
      "\n",
      "/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD/Conformer_w_TCM_LA_WCE_1e-06_ES144_H4_NE4_KS31_AUG3_w_sin_pos_multiview_large_corpus_random_start_avg_5_best_4_multiview_train_random_start_large_corpus_4s_rs_mlaad.txt\n",
      "Accuracy: 87.62\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/nvme2/Datasets/MLAAD/protocol_eval.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "list_dir = os.listdir(\n",
    "    BASE_DIR)\n",
    "\n",
    "list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "list_dir = sorted(list_dir)\n",
    "\n",
    "for score_file in list_dir:\n",
    "    pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "    print(score_file)\n",
    "    pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "    pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "    # pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # if spoof < score, then bonafide, else spoof\n",
    "    pred_df['pred'] = pred_df.apply(\n",
    "        lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "    # merge eval_df and pred_df on utt\n",
    "    res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "    print(\"Accuracy: {:.2f}\".format(accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hungdx/mlaad_xlsr_conformertcm_train_large_corpus_multiview_conf-1.txt\n",
      "Accuracy: 93.54\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, det_curve\n",
    "full_df = pd.read_csv(\n",
    "    \"/home/hungdx/MLAAD/protocol_lts.txt\", sep=\" \", header=None)\n",
    "\n",
    "full_df.columns = [\"utt\", \"subset\", \"label\"]\n",
    "\n",
    "# BASE_DIR = \"/nvme2/hungdx/tcm_add/Scores/CNSL_Multiview/ConformerTCM_LargeCorpus_eval_MLAAD\"\n",
    "# list_dir = os.listdir(\n",
    "#     BASE_DIR)\n",
    "\n",
    "# list_dir = [os.path.join(BASE_DIR, x) for x in list_dir]\n",
    "# list_dir = sorted(list_dir)\n",
    "\n",
    "#for score_file in list_dir:\n",
    "score_file = \"/home/hungdx/mlaad_xlsr_conformertcm_train_large_corpus_multiview_conf-1.txt\"\n",
    "pred_df = pd.read_csv(score_file, sep=\" \", header=None)\n",
    "print(score_file)\n",
    "pred_df.columns = [\"utt\", \"spoof\", \"score\"]\n",
    "pred_df = pred_df.drop_duplicates(subset=['utt'])\n",
    "\n",
    "# pred_df['utt'] = pred_df['utt'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# if spoof < score, then bonafide, else spoof\n",
    "pred_df['pred'] = pred_df.apply(\n",
    "    lambda x: 'bonafide' if x['spoof'] < x['score'] else 'spoof', axis=1)\n",
    "\n",
    "# merge eval_df and pred_df on utt\n",
    "res_df = pd.merge(full_df, pred_df, on='utt')\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(res_df[\"label\"], res_df[\"pred\"])*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer: 0.22422491216022286\tthreshold: -0.7054389119148254\n",
      "\n",
      "0.0022422491216022287\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas\n",
    "import eval_metrics_DF as em\n",
    "\n",
    "\n",
    "def eval_to_score_file(score_file, cm_key_file):\n",
    "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
    "    # score file is the output of the system that contains the scores for the eval set\n",
    "    # phase is the phase of the eval set (dev or eval)\n",
    "\n",
    "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
    "    cm_data.columns = ['filename', 'subset', 'label']\n",
    "    submission_scores = pandas.read_csv(\n",
    "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
    "    submission_scores.columns = ['filename', 'score']\n",
    "    # check here for progress vs eval set\n",
    "    cm_scores = submission_scores.merge(\n",
    "        cm_data, left_on='filename', right_on='filename', how='inner')\n",
    "\n",
    "    # cm_scores = cm_scores[cm_scores['subset'] == subset]\n",
    "    # cm_scores.head()\n",
    "    #  0       1_x   1_y      2      3\n",
    "    #  a.wav  1.234   eval   Music   spoof\n",
    "    bona_cm = cm_scores[cm_scores['label'] == 'bonafide']['score'].values\n",
    "    spoof_cm = cm_scores[cm_scores['label'] == 'spoof']['score'].values\n",
    "\n",
    "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
    "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
    "    print(out_data)\n",
    "    return eer_cm\n",
    "\n",
    "\n",
    "score_file = \"/home/hungdx/large_corpus_eval_xlsr_conformertcm_train_large_corpus_multiview_conf-1.txt\"\n",
    "print(eval_to_score_file(score_file, \"/data/Datasets/0_large-corpus/protocol.txt\"))\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
