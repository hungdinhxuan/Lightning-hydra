{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading protocol file...\n",
      "Processing 248159 files using 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungdx/miniconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Calculating durations: 100%|██████████| 8/8 [00:02<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results...\n",
      "\n",
      "Processing Statistics:\n",
      "Total files processed: 248159\n",
      "Failed files: 0\n",
      "Total duration: 433.71 hours\n",
      "Mean duration: 6.29 seconds\n",
      "Min duration: 0.08 seconds\n",
      "Max duration: 26.20 seconds\n",
      "\n",
      "First few rows of results:\n",
      "                          utt_id subset     label  duration\n",
      "0     SNS/2024/Zuckerberg_38.wav  train  bonafide       7.0\n",
      "1     SNS/2024/Matt_damon_58.wav  train  bonafide       6.5\n",
      "2  SNS/2024/Anne_Hathaway_83.wav  train  bonafide       6.5\n",
      "3    SNS/2024/Zuckerberg_127.wav  train  bonafide       7.0\n",
      "4        SNS/2024/Macron_193.wav  train  bonafide       7.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def get_audio_duration(row, base_dir):\n",
    "    \"\"\"Calculate duration for a single audio file\"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(base_dir, row['utt_id'])\n",
    "        duration = librosa.get_duration(path=file_path)\n",
    "        return {\n",
    "            'utt_id': row['utt_id'],\n",
    "            'subset': row['subset'],\n",
    "            'label': row['label'],\n",
    "            'duration': duration\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['utt_id']}: {str(e)}\")\n",
    "        return {\n",
    "            'utt_id': row['utt_id'],\n",
    "            'subset': row['subset'],\n",
    "            'label': row['label'],\n",
    "            'duration': -1  # Mark failed files with -1\n",
    "        }\n",
    "\n",
    "\n",
    "def process_chunk(chunk, base_dir):\n",
    "    \"\"\"Process a chunk of the dataframe\"\"\"\n",
    "    return [get_audio_duration(row, base_dir) for row in chunk.to_dict('records')]\n",
    "\n",
    "\n",
    "def calculate_durations(protocol_file, base_dir, output_file, n_workers=None):\n",
    "    \"\"\"\n",
    "    Calculate durations for all audio files in parallel\n",
    "    \n",
    "    Args:\n",
    "        protocol_file: Path to protocol file\n",
    "        base_dir: Base directory containing audio files\n",
    "        output_file: Path to output CSV file\n",
    "        n_workers: Number of worker processes (default: CPU count - 1)\n",
    "    \"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = cpu_count() - 1\n",
    "\n",
    "    print(\"Reading protocol file...\")\n",
    "    protocol = pd.read_csv(protocol_file, sep=\" \", header=None)\n",
    "    protocol.columns = [\"utt_id\", \"subset\", \"label\"]\n",
    "\n",
    "    # Split dataframe into chunks for parallel processing\n",
    "    chunk_size = len(protocol) // n_workers + 1\n",
    "    chunks = np.array_split(protocol, n_workers)\n",
    "\n",
    "    print(f\"Processing {len(protocol)} files using {n_workers} workers...\")\n",
    "\n",
    "    # Process chunks in parallel\n",
    "    with Pool(n_workers) as pool:\n",
    "        partial_process = partial(process_chunk, base_dir=base_dir)\n",
    "        results = list(tqdm(\n",
    "            pool.imap(partial_process, chunks),\n",
    "            total=len(chunks),\n",
    "            desc=\"Calculating durations\"\n",
    "        ))\n",
    "\n",
    "    # Flatten results and convert to dataframe\n",
    "    all_results = [item for sublist in results for item in sublist]\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "\n",
    "    # Calculate statistics\n",
    "    valid_durations = df_results[df_results['duration'] != -1]['duration']\n",
    "    stats = {\n",
    "        'total_files': len(df_results),\n",
    "        'failed_files': len(df_results[df_results['duration'] == -1]),\n",
    "        'total_duration_hours': valid_durations.sum() / 3600,\n",
    "        'mean_duration': valid_durations.mean(),\n",
    "        'min_duration': valid_durations.min(),\n",
    "        'max_duration': valid_durations.max()\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    print(\"\\nSaving results...\")\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Total files processed: {stats['total_files']}\")\n",
    "    print(f\"Failed files: {stats['failed_files']}\")\n",
    "    print(f\"Total duration: {stats['total_duration_hours']:.2f} hours\")\n",
    "    print(f\"Mean duration: {stats['mean_duration']:.2f} seconds\")\n",
    "    print(f\"Min duration: {stats['min_duration']:.2f} seconds\")\n",
    "    print(f\"Max duration: {stats['max_duration']:.2f} seconds\")\n",
    "\n",
    "    return df_results, stats\n",
    "\n",
    "\n",
    "# Usage\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/data/0_large-corpus\"\n",
    "protocol_file = \"new_protocol_trim_vocoded_v2.txt\"\n",
    "output_file = \"audio_durations_v2.csv\"\n",
    "\n",
    "# Run the processing\n",
    "df_results, stats = calculate_durations(\n",
    "    protocol_file=protocol_file,\n",
    "    base_dir=BASE_DIR,\n",
    "    output_file=output_file,\n",
    "    n_workers=8  # Adjust based on your system\n",
    ")\n",
    "\n",
    "# Display first few rows of results\n",
    "print(\"\\nFirst few rows of results:\")\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short audio files (duration < 1 second):\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "# Filter audio files with duration less than 1 second with subset == train\n",
    "short_files = df_results[df_results['duration'] < 1 & (df_results['subset'] == 'train')]\n",
    "print(\"\\nShort audio files (duration < 1 second):\")\n",
    "print(len(short_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all files with duration < 1 second and subset == train\n",
    "df_results = df_results[~((df_results['duration'] < 1) & (df_results['subset'] == 'train'))]\n",
    "\n",
    "# drop duration column and save\n",
    "df_results.drop(columns=['duration'], inplace=True)\n",
    "df_results.to_csv(\"new_protocol_trim_vocoded_cleaned_v2.txt\", index=False, header=False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing vocoded files: 405579\n",
      "After removing vocoded files: 390512\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"new_protocol_trim_vocoded_cleaned.txt\", sep=\" \", header=None)\n",
    "df.columns = [\"utt_id\", \"subset\", \"label\"]\n",
    "\n",
    "print(\"Before removing vocoded files:\", len(df))\n",
    "\n",
    "df = df[~((df[\"utt_id\"].str.startswith(\"vocoded\")) & (df[\"subset\"] == \"dev\"))]\n",
    "\n",
    "print(\"After removing vocoded files:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"~/MLAAD/protocol.txt\", sep=\" \", header=None)\n",
    "df.columns = [\"utt_id\", \"subset\", \"unk\", \"label\"]\n",
    "\n",
    "# drop unk column\n",
    "df.drop(columns=['unk'], inplace=True)\n",
    "\n",
    "df.to_csv(\"~/MLAAD/protocol_lts.txt\", index=False, header=False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing vocoded files: 405579\n",
      "After removing vocoded files: 390512\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"new_protocol_trim_vocoded_cleaned.txt\", sep=\" \", header=None)\n",
    "df.columns = [\"utt_id\", \"subset\", \"label\"]\n",
    "\n",
    "print(\"Before removing vocoded files:\", len(df))\n",
    "\n",
    "df = df[~((df[\"utt_id\"].str.startswith(\"vocoded\")) & (df[\"subset\"] == \"dev\"))]\n",
    "\n",
    "print(\"After removing vocoded files:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created protocol.txt with 152388 entries\n",
      "Created combined_meta.csv with 152388 entries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "\n",
    "def read_csv_safely(file_path):\n",
    "    try:\n",
    "        # First attempt with standard reading\n",
    "        return pd.read_csv(file_path, sep='|')\n",
    "    except:\n",
    "        try:\n",
    "            # Second attempt with quote character handling\n",
    "            return pd.read_csv(file_path, sep='|', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "        except:\n",
    "            try:\n",
    "                # Third attempt with error handling\n",
    "                return pd.read_csv(file_path, sep='|', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "def create_protocol_and_merge_meta(root_dir):\n",
    "    # Initialize empty list to store all metadata\n",
    "    all_meta_data = []\n",
    "\n",
    "    # Initialize protocol file\n",
    "    protocol_lines = []\n",
    "\n",
    "    # Walk through the directory structure\n",
    "    for language_dir in os.listdir(root_dir):\n",
    "        language_path = os.path.join(root_dir, language_dir)\n",
    "        if not os.path.isdir(language_path):\n",
    "            continue\n",
    "\n",
    "        # For each model directory in the language directory\n",
    "        for model_dir in os.listdir(language_path):\n",
    "            model_path = os.path.join(language_path, model_dir)\n",
    "            if not os.path.isdir(model_path):\n",
    "                continue\n",
    "\n",
    "            # Look for meta.csv file\n",
    "            meta_file = os.path.join(model_path, 'meta.csv')\n",
    "            if os.path.exists(meta_file):\n",
    "                # Read meta.csv with safe reading function\n",
    "                df = read_csv_safely(meta_file)\n",
    "\n",
    "                if df is not None:\n",
    "                    # Add to combined metadata\n",
    "                    all_meta_data.append(df)\n",
    "\n",
    "                    # Create protocol lines for all audio files in this directory\n",
    "                    for audio_path in df['path']:\n",
    "                        protocol_lines.append(f\"{audio_path} eval spoof\")\n",
    "\n",
    "    # Combine all metadata\n",
    "    if all_meta_data:\n",
    "        combined_meta = pd.concat(all_meta_data, ignore_index=True)\n",
    "\n",
    "        # Save combined metadata\n",
    "        combined_meta.to_csv('mlaad_v5_combined_meta.csv', sep='|',\n",
    "                             index=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "\n",
    "        # Save protocol file\n",
    "        with open('mlaad_v5_protocol.txt', 'w') as f:\n",
    "            f.write('\\n'.join(protocol_lines))\n",
    "\n",
    "        print(f\"Created protocol.txt with {len(protocol_lines)} entries\")\n",
    "        print(f\"Created combined_meta.csv with {len(combined_meta)} entries\")\n",
    "    else:\n",
    "        print(\"No meta.csv files found or all files had errors\")\n",
    "\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    create_protocol_and_merge_meta(\n",
    "        '/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# partialSpoof protocol generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting protocol file creation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subsets:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created protocol file for train: /nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol_train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subsets:  67%|██████▋   | 2/3 [00:00<00:00, 14.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created protocol file for dev: /nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol_dev.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subsets: 100%|██████████| 3/3 [00:00<00:00, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: No labels found for 2 files in eval\n",
      "First few missing files: ['CON_E_0034982', 'CON_E_0058039']\n",
      "✓ Created protocol file for eval: /nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database/protocol_eval.txt\n",
      "\n",
      "Protocol file creation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def read_protocol_metadata(protocol_file):\n",
    "    \"\"\"Read protocol metadata and create mapping dictionary.\"\"\"\n",
    "    metadata = {}\n",
    "    with open(protocol_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in tqdm(lines, desc=f\"Reading {protocol_file.name}\", leave=False):\n",
    "            parts = line.strip().split()\n",
    "            # print(parts)\n",
    "            # import sys\n",
    "            # sys.exit()\n",
    "            if len(parts) >= 4:\n",
    "                # Extract the file ID (e.g., LA_0079) from the first column\n",
    "                file_id = parts[1]\n",
    "                label = parts[4]    # spoof or bonafide\n",
    "                metadata[file_id] = label\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def create_new_protocol(lst_file, wav_folder, protocol_metadata, subset, output_file):\n",
    "    \"\"\"Create new protocol file with wav paths and labels.\"\"\"\n",
    "    with open(lst_file, 'r') as f:\n",
    "        file_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    missing_labels = []\n",
    "    with open(output_file, 'w') as out_f:\n",
    "        for file_id in tqdm(file_ids, desc=f\"Processing {subset}\", leave=False):\n",
    "            wav_path = f\"{subset}/con_wav/{file_id}.wav\"\n",
    "            # Direct dictionary lookup instead of loop\n",
    "            label = protocol_metadata.get(file_id)\n",
    "\n",
    "            if label:\n",
    "                out_f.write(f\"{wav_path} {subset} {label}\\n\")\n",
    "            else:\n",
    "                missing_labels.append(file_id)\n",
    "\n",
    "    if missing_labels:\n",
    "        print(\n",
    "            f\"\\nWarning: No labels found for {len(missing_labels)} files in {subset}\")\n",
    "        print(\"First few missing files:\", missing_labels[:5])\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define paths\n",
    "    base_dir = Path(\"/nvme1/hungdx/Lightning-hydra/data/PartialSpoof/database\")\n",
    "    protocol_dir = base_dir / \"protocols/PartialSpoof_LA_cm_protocols\"\n",
    "\n",
    "    # Process each subset (train, dev, eval)\n",
    "    subsets = ['train', 'dev', 'eval']\n",
    "\n",
    "    print(\"Starting protocol file creation...\")\n",
    "    for subset in tqdm(subsets, desc=\"Processing subsets\"):\n",
    "        # Input files\n",
    "        protocol_file = protocol_dir / f\"PartialSpoof.LA.cm.{subset}.trl.txt\"\n",
    "        lst_file = base_dir / subset / f\"{subset}.lst\"\n",
    "        output_file = base_dir / f\"protocol_{subset}.txt\"\n",
    "\n",
    "        # Read protocol metadata\n",
    "        metadata = read_protocol_metadata(protocol_file)\n",
    "\n",
    "        # Create new protocol file\n",
    "        create_new_protocol(\n",
    "            lst_file=lst_file,\n",
    "            wav_folder=subset,\n",
    "            protocol_metadata=metadata,\n",
    "            subset=subset,\n",
    "            output_file=output_file\n",
    "        )\n",
    "\n",
    "        print(f\"✓ Created protocol file for {subset}: {output_file}\")\n",
    "\n",
    "    print(\"\\nProtocol file creation completed!\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend ASVspoof generator \n",
    "\n",
    "This dataset includes: Orginal ASVspoof dataset and my trimmed version of ASVspoof dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Prepare original train set\n",
    "df_ori_train = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Datasets/protocols/database/ASVspoof_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\", sep=\" \", header=None)\n",
    "df_ori_train.columns = [\"unk1\", \"utt\", \"unk2\", \"attack\", \"label\"]\n",
    "\n",
    "df_ori_train[\"subset\"] = \"train\"\n",
    "\n",
    "# prepare trimmed train set\n",
    "\n",
    "df_trimmed_train = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Datasets/prototcols/LA19.cm.train.trn_1s.txt\", sep=\" \", header=None)\n",
    "df_trimmed_train.columns = [\"unk1\", \"utt\", \"unk2\", \"attack\", \"label\"]\n",
    "\n",
    "# add subset column\n",
    "df_trimmed_train[\"subset\"] = \"train\"\n",
    "\n",
    "\n",
    "# Filter utt  without contain no_speech\n",
    "df_trimmed_train = df_trimmed_train[~df_trimmed_train[\"utt\"].str.contains(\"no_speech\")]\n",
    "\n",
    "# Prepare original dev set\n",
    "df_ori_dev = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Datasets/protocols/database/ASVspoof_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\", sep=\" \", header=None)\n",
    "df_ori_dev.columns = [\"unk1\", \"utt\", \"unk2\", \"attack\", \"label\"]\n",
    "\n",
    "df_ori_dev[\"subset\"] = \"dev\"\n",
    "\n",
    "index_col = 'trial'\n",
    "# =====\n",
    "# Configuration to load CM protocol and score file\n",
    "# =====\n",
    "# name of data series for procotol file\n",
    "p_names = ['speaker', index_col, 'compr', 'source', 'attack',\n",
    "                'label', 'trim', 'subset', 'vocoder', \n",
    "                'task', 'team', 'gender-pair', 'language']\n",
    "\n",
    "\n",
    "df_eval_trial = pd.read_csv(\n",
    "    '/nvme1/hungdx/Datasets/protocols/database/ASVspoof_DF_cm_protocols/ASVspoof2021.DF.cm.eval.trl.txt', sep=\" \", header=None)\n",
    "df_eval_trial.columns = ['utt']\n",
    "\n",
    "df_eval_meta = pd.read_csv(\n",
    "    '/nvme1/hungdx/Lightning-hydra/logs/asvspoof-challenge-2021/eval-package/keys/DF/CM/trial_metadata.txt', sep=\" \", header=None)\n",
    "\n",
    "df_eval_meta.columns = p_names\n",
    "\n",
    "# Merge trial and metadata\n",
    "\n",
    "df_eval = pd.merge(df_eval_trial, df_eval_meta, left_on='utt', right_on=index_col)\n",
    "\n",
    "# keep only utt and label\n",
    "df_eval = df_eval[['utt', 'label']]\n",
    "\n",
    "df_eval[\"subset\"] = \"eval\"\n",
    "\n",
    "TRAIN_PATH = \"ASVspoof2019_LA_train/\"\n",
    "TRIMMED_TRAIN_PATH = \"LA19_train_1s/\"\n",
    "DEV_PATH = \"ASVspoof2019_LA_dev/\"\n",
    "EVAL_PATH = \"ASVspoof2021_DF_eval/\"\n",
    "\n",
    "\n",
    "# Merge df_ori_train, df_trimmed_train, df_ori_dev, df_eval into a single dataframe\n",
    "# Keep only utt, subset and label columns\n",
    "# new utt should be the full path to the audio file\n",
    "# For example, with subset = \"train\" and utt = \"LA_T_1000001\", the new utt should be f\"{TRAIN_PATH}/LA_T_1000001.flac\"\n",
    "\n",
    "df_ori_train[\"utt\"] = df_ori_train[\"utt\"].apply(lambda x: f\"{TRAIN_PATH}/{x}.flac\")\n",
    "df_trimmed_train[\"utt\"] = df_trimmed_train[\"utt\"].apply(lambda x: f\"{TRIMMED_TRAIN_PATH}/{x}.flac\")\n",
    "df_ori_dev[\"utt\"] = df_ori_dev[\"utt\"].apply(lambda x: f\"{DEV_PATH}/{x}.flac\")\n",
    "df_eval[\"utt\"] = df_eval[\"utt\"].apply(lambda x: f\"{EVAL_PATH}/{x}.flac\")\n",
    "\n",
    "\n",
    "df = pd.concat([df_ori_train[[\"utt\", \"subset\", \"label\"]], df_trimmed_train[[\"utt\", \"subset\", \"label\"]], df_ori_dev[[\"utt\", \"subset\", \"label\"]], df_eval[[\"utt\", \"subset\", \"label\"]]], ignore_index=True)\n",
    "\n",
    "df.to_csv(\"/nvme1/hungdx/Lightning-hydra/data/ExtendedASVspoof/protocol.txt\",\n",
    "          index=False, header=False, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD track 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "label = pd.read_csv(\n",
    "    \"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/track2_label.txt\", sep=\" \", header=None)\n",
    "label.columns = [\"utt\", \"label\"]\n",
    "\n",
    "# add subset column\n",
    "label[\"subset\"] = \"eval\"\n",
    "\n",
    "# move subset column to second column\n",
    "label = label[[\"utt\", \"subset\", \"label\"]]\n",
    "\n",
    "# Change value of label column (fake -> spoof, genuine -> bonafide)\n",
    "label[\"label\"] = label[\"label\"].apply(lambda x: \"spoof\" if x == \"fake\" else \"bonafide\")\n",
    "\n",
    "# save to file\n",
    "label.to_csv(\"/nvme1/hungdx/Lightning-hydra/data/ADD_eval/protocol.txt\",\n",
    "             index=False, header=False, sep=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy dataset (ASVSpoof_Laundered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Filtering_df = pd.read_csv(\"/nvme1/Datasets/ASVSpoof_Laundered_protocols/ASVspoofLauneredDatabase_Filtering.txt\", sep=\" \", header=None)\n",
    "Filtering_df.columns = [\"LA_group\", \"utt\", \"attack\", \"label\", \"group\", \"subgroup\"]\n",
    "\n",
    "Noise_Addition_df = pd.read_csv(\"/nvme1/Datasets/ASVSpoof_Laundered_protocols/ASVspoofLauneredDatabase_Noise_Addition.txt\", sep=\" \", header=None)\n",
    "Noise_Addition_df.columns = [\"LA_group\", \"utt\", \"attack\", \"label\", \"group\", \"subgroup\"]\n",
    "\n",
    "Recompression_df = pd.read_csv(\"/nvme1/Datasets/ASVSpoof_Laundered_protocols/ASVspoofLauneredDatabase_Recompression.txt\", sep=\" \", header=None)\n",
    "Recompression_df.columns = [\"LA_group\", \"utt\", \"attack\", \"label\", \"group\", \"subgroup\"]\n",
    "\n",
    "\n",
    "Resampling_df = pd.read_csv(\"/nvme1/Datasets/ASVSpoof_Laundered_protocols/ASVspoofLauneredDatabase_Resampling.txt\", sep=\" \", header=None)\n",
    "Resampling_df.columns = [\"LA_group\", \"utt\", \"attack\", \"label\", \"group\", \"subgroup\"]\n",
    "\n",
    "\n",
    "Reverberation_df = pd.read_csv(\"/nvme1/Datasets/ASVSpoof_Laundered_protocols/ASVspoofLauneredDatabase_Reverberation.txt\", sep=\" \", header=None)\n",
    "Reverberation_df.columns = [\"LA_group\", \"utt\", \"attack\", \"label\", \"group\", \"subgroup\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes\n",
    "\n",
    "df = pd.concat([Filtering_df, Noise_Addition_df, Recompression_df, Resampling_df, Reverberation_df], ignore_index=True)\n",
    "#df\n",
    "# Create a protocol.txt file with utt, subset and label columns\n",
    "# the subset column has two values: train and dev \n",
    "# The ratio of train and dev is 50:50\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protocol.txt file has been created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Select only required columns: utt, label\n",
    "df = df[['utt', 'label']].copy()\n",
    "\n",
    "# Shuffle the dataset to ensure randomness\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into 50% train, 50% dev\n",
    "split_idx = len(df) // 2\n",
    "df.loc[:split_idx, 'subset'] = 'train'\n",
    "df.loc[split_idx:, 'subset'] = 'dev'\n",
    "\n",
    "# Reorder columns as required\n",
    "df = df[['utt', 'subset', 'label']]\n",
    "df['utt'] = df['utt'].apply(lambda x: f\"{x}.flac\")\n",
    "# Save to protocol.txt (tab-separated)\n",
    "df.to_csv(\"protocol.txt\", sep=' ', index=False, header=False)\n",
    "\n",
    "print(\"protocol.txt file has been created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset\n",
      "dev      1032937\n",
      "train    1032936\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['subset'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ balanced_protocol.txt file has been created successfully with balanced classes!\n"
     ]
    }
   ],
   "source": [
    "# Select only required columns: utt, label\n",
    "df = df[['utt', 'label']].copy()\n",
    "\n",
    "# Separate bonafide and spoof samples\n",
    "df_bonafide = df[df['label'] == 'bonafide']\n",
    "df_spoof = df[df['label'] == 'spoof']\n",
    "\n",
    "# Ensure equal number of spoof and bonafide samples\n",
    "num_bonafide = len(df_bonafide)\n",
    "df_spoof_balanced = df_spoof.sample(n=num_bonafide, random_state=42)\n",
    "\n",
    "# Combine balanced dataset\n",
    "df_balanced = pd.concat([df_bonafide, df_spoof_balanced])\n",
    "\n",
    "# Shuffle the dataset for randomness\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into 50% train and 50% dev\n",
    "split_idx = len(df_balanced) // 2\n",
    "df_balanced.loc[:split_idx, 'subset'] = 'train'\n",
    "df_balanced.loc[split_idx:, 'subset'] = 'dev'\n",
    "\n",
    "# Reorder columns as required\n",
    "df_balanced = df_balanced[['utt', 'subset', 'label']]\n",
    "\n",
    "# Append \".flac\" to utt column\n",
    "df_balanced['utt'] = df_balanced['utt'].apply(lambda x: f\"{x}.flac\")\n",
    "\n",
    "# Save to protocol.txt (space-separated, no header)\n",
    "df_balanced.to_csv(\"balanced_protocol.txt\", sep=' ', index=False, header=False)\n",
    "\n",
    "print(\"✅ balanced_protocol.txt file has been created successfully with balanced classes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only required columns: utt, label\n",
    "df = df[['utt', 'label']].copy()\n",
    "\n",
    "# Separate bonafide and spoof samples\n",
    "df_bonafide = df[df['label'] == 'bonafide']\n",
    "df_spoof = df[df['label'] == 'spoof']\n",
    "\n",
    "# Ensure equal number of spoof and bonafide samples\n",
    "num_bonafide = len(df_bonafide)\n",
    "df_spoof_balanced = df_spoof.sample(n=num_bonafide, random_state=42)\n",
    "\n",
    "# Combine balanced dataset\n",
    "df_balanced = pd.concat([df_bonafide, df_spoof_balanced])\n",
    "\n",
    "# Shuffle the dataset for randomness\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into 50% train and 50% dev\n",
    "split_idx = len(df_balanced) // 2\n",
    "df_balanced.loc[:split_idx, 'subset'] = 'train'\n",
    "df_balanced.loc[split_idx:, 'subset'] = 'dev'\n",
    "\n",
    "# Reorder columns as required\n",
    "df_balanced = df_balanced[['utt', 'subset', 'label']]\n",
    "\n",
    "# Append \".flac\" to utt column\n",
    "df_balanced['utt'] = df_balanced['utt'].apply(lambda x: f\"ASVSpoof_Laundered_flac/{x}.flac\")\n",
    "\n",
    "# Save to protocol.txt (space-separated, no header)\n",
    "# df_balanced.to_csv(\"balanced_protocol.txt\", sep=' ', index=False, header=False)\n",
    "\n",
    "# print(\"✅ balanced_protocol.txt file has been created successfully with balanced classes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.to_csv(\"new_balanced_protocol.txt\", sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
