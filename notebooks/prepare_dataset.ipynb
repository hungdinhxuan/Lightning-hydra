{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading protocol file...\n",
      "Processing 248159 files using 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungdx/miniconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Calculating durations: 100%|██████████| 8/8 [00:02<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results...\n",
      "\n",
      "Processing Statistics:\n",
      "Total files processed: 248159\n",
      "Failed files: 0\n",
      "Total duration: 433.71 hours\n",
      "Mean duration: 6.29 seconds\n",
      "Min duration: 0.08 seconds\n",
      "Max duration: 26.20 seconds\n",
      "\n",
      "First few rows of results:\n",
      "                          utt_id subset     label  duration\n",
      "0     SNS/2024/Zuckerberg_38.wav  train  bonafide       7.0\n",
      "1     SNS/2024/Matt_damon_58.wav  train  bonafide       6.5\n",
      "2  SNS/2024/Anne_Hathaway_83.wav  train  bonafide       6.5\n",
      "3    SNS/2024/Zuckerberg_127.wav  train  bonafide       7.0\n",
      "4        SNS/2024/Macron_193.wav  train  bonafide       7.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def get_audio_duration(row, base_dir):\n",
    "    \"\"\"Calculate duration for a single audio file\"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(base_dir, row['utt_id'])\n",
    "        duration = librosa.get_duration(path=file_path)\n",
    "        return {\n",
    "            'utt_id': row['utt_id'],\n",
    "            'subset': row['subset'],\n",
    "            'label': row['label'],\n",
    "            'duration': duration\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['utt_id']}: {str(e)}\")\n",
    "        return {\n",
    "            'utt_id': row['utt_id'],\n",
    "            'subset': row['subset'],\n",
    "            'label': row['label'],\n",
    "            'duration': -1  # Mark failed files with -1\n",
    "        }\n",
    "\n",
    "\n",
    "def process_chunk(chunk, base_dir):\n",
    "    \"\"\"Process a chunk of the dataframe\"\"\"\n",
    "    return [get_audio_duration(row, base_dir) for row in chunk.to_dict('records')]\n",
    "\n",
    "\n",
    "def calculate_durations(protocol_file, base_dir, output_file, n_workers=None):\n",
    "    \"\"\"\n",
    "    Calculate durations for all audio files in parallel\n",
    "    \n",
    "    Args:\n",
    "        protocol_file: Path to protocol file\n",
    "        base_dir: Base directory containing audio files\n",
    "        output_file: Path to output CSV file\n",
    "        n_workers: Number of worker processes (default: CPU count - 1)\n",
    "    \"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = cpu_count() - 1\n",
    "\n",
    "    print(\"Reading protocol file...\")\n",
    "    protocol = pd.read_csv(protocol_file, sep=\" \", header=None)\n",
    "    protocol.columns = [\"utt_id\", \"subset\", \"label\"]\n",
    "\n",
    "    # Split dataframe into chunks for parallel processing\n",
    "    chunk_size = len(protocol) // n_workers + 1\n",
    "    chunks = np.array_split(protocol, n_workers)\n",
    "\n",
    "    print(f\"Processing {len(protocol)} files using {n_workers} workers...\")\n",
    "\n",
    "    # Process chunks in parallel\n",
    "    with Pool(n_workers) as pool:\n",
    "        partial_process = partial(process_chunk, base_dir=base_dir)\n",
    "        results = list(tqdm(\n",
    "            pool.imap(partial_process, chunks),\n",
    "            total=len(chunks),\n",
    "            desc=\"Calculating durations\"\n",
    "        ))\n",
    "\n",
    "    # Flatten results and convert to dataframe\n",
    "    all_results = [item for sublist in results for item in sublist]\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "\n",
    "    # Calculate statistics\n",
    "    valid_durations = df_results[df_results['duration'] != -1]['duration']\n",
    "    stats = {\n",
    "        'total_files': len(df_results),\n",
    "        'failed_files': len(df_results[df_results['duration'] == -1]),\n",
    "        'total_duration_hours': valid_durations.sum() / 3600,\n",
    "        'mean_duration': valid_durations.mean(),\n",
    "        'min_duration': valid_durations.min(),\n",
    "        'max_duration': valid_durations.max()\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    print(\"\\nSaving results...\")\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\nProcessing Statistics:\")\n",
    "    print(f\"Total files processed: {stats['total_files']}\")\n",
    "    print(f\"Failed files: {stats['failed_files']}\")\n",
    "    print(f\"Total duration: {stats['total_duration_hours']:.2f} hours\")\n",
    "    print(f\"Mean duration: {stats['mean_duration']:.2f} seconds\")\n",
    "    print(f\"Min duration: {stats['min_duration']:.2f} seconds\")\n",
    "    print(f\"Max duration: {stats['max_duration']:.2f} seconds\")\n",
    "\n",
    "    return df_results, stats\n",
    "\n",
    "\n",
    "# Usage\n",
    "BASE_DIR = \"/data/hungdx/Lightning-hydra/data/0_large-corpus\"\n",
    "protocol_file = \"new_protocol_trim_vocoded_v2.txt\"\n",
    "output_file = \"audio_durations_v2.csv\"\n",
    "\n",
    "# Run the processing\n",
    "df_results, stats = calculate_durations(\n",
    "    protocol_file=protocol_file,\n",
    "    base_dir=BASE_DIR,\n",
    "    output_file=output_file,\n",
    "    n_workers=8  # Adjust based on your system\n",
    ")\n",
    "\n",
    "# Display first few rows of results\n",
    "print(\"\\nFirst few rows of results:\")\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short audio files (duration < 1 second):\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "# Filter audio files with duration less than 1 second with subset == train\n",
    "short_files = df_results[df_results['duration'] < 1 & (df_results['subset'] == 'train')]\n",
    "print(\"\\nShort audio files (duration < 1 second):\")\n",
    "print(len(short_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all files with duration < 1 second and subset == train\n",
    "df_results = df_results[~((df_results['duration'] < 1) & (df_results['subset'] == 'train'))]\n",
    "\n",
    "# drop duration column and save\n",
    "df_results.drop(columns=['duration'], inplace=True)\n",
    "df_results.to_csv(\"new_protocol_trim_vocoded_cleaned_v2.txt\", index=False, header=False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing vocoded files: 405579\n",
      "After removing vocoded files: 390512\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"new_protocol_trim_vocoded_cleaned.txt\", sep=\" \", header=None)\n",
    "df.columns = [\"utt_id\", \"subset\", \"label\"]\n",
    "\n",
    "print(\"Before removing vocoded files:\", len(df))\n",
    "\n",
    "df = df[~((df[\"utt_id\"].str.startswith(\"vocoded\")) & (df[\"subset\"] == \"dev\"))]\n",
    "\n",
    "print(\"After removing vocoded files:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"~/MLAAD/protocol.txt\", sep=\" \", header=None)\n",
    "df.columns = [\"utt_id\", \"subset\", \"unk\", \"label\"]\n",
    "\n",
    "# drop unk column\n",
    "df.drop(columns=['unk'], inplace=True)\n",
    "\n",
    "df.to_csv(\"~/MLAAD/protocol_lts.txt\", index=False, header=False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing vocoded files: 405579\n",
      "After removing vocoded files: 390512\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"new_protocol_trim_vocoded_cleaned.txt\", sep=\" \", header=None)\n",
    "df.columns = [\"utt_id\", \"subset\", \"label\"]\n",
    "\n",
    "print(\"Before removing vocoded files:\", len(df))\n",
    "\n",
    "df = df[~((df[\"utt_id\"].str.startswith(\"vocoded\")) & (df[\"subset\"] == \"dev\"))]\n",
    "\n",
    "print(\"After removing vocoded files:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLAAD V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created protocol.txt with 152388 entries\n",
      "Created combined_meta.csv with 152388 entries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "\n",
    "def read_csv_safely(file_path):\n",
    "    try:\n",
    "        # First attempt with standard reading\n",
    "        return pd.read_csv(file_path, sep='|')\n",
    "    except:\n",
    "        try:\n",
    "            # Second attempt with quote character handling\n",
    "            return pd.read_csv(file_path, sep='|', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "        except:\n",
    "            try:\n",
    "                # Third attempt with error handling\n",
    "                return pd.read_csv(file_path, sep='|', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "def create_protocol_and_merge_meta(root_dir):\n",
    "    # Initialize empty list to store all metadata\n",
    "    all_meta_data = []\n",
    "\n",
    "    # Initialize protocol file\n",
    "    protocol_lines = []\n",
    "\n",
    "    # Walk through the directory structure\n",
    "    for language_dir in os.listdir(root_dir):\n",
    "        language_path = os.path.join(root_dir, language_dir)\n",
    "        if not os.path.isdir(language_path):\n",
    "            continue\n",
    "\n",
    "        # For each model directory in the language directory\n",
    "        for model_dir in os.listdir(language_path):\n",
    "            model_path = os.path.join(language_path, model_dir)\n",
    "            if not os.path.isdir(model_path):\n",
    "                continue\n",
    "\n",
    "            # Look for meta.csv file\n",
    "            meta_file = os.path.join(model_path, 'meta.csv')\n",
    "            if os.path.exists(meta_file):\n",
    "                # Read meta.csv with safe reading function\n",
    "                df = read_csv_safely(meta_file)\n",
    "\n",
    "                if df is not None:\n",
    "                    # Add to combined metadata\n",
    "                    all_meta_data.append(df)\n",
    "\n",
    "                    # Create protocol lines for all audio files in this directory\n",
    "                    for audio_path in df['path']:\n",
    "                        protocol_lines.append(f\"{audio_path} eval spoof\")\n",
    "\n",
    "    # Combine all metadata\n",
    "    if all_meta_data:\n",
    "        combined_meta = pd.concat(all_meta_data, ignore_index=True)\n",
    "\n",
    "        # Save combined metadata\n",
    "        combined_meta.to_csv('mlaad_v5_combined_meta.csv', sep='|',\n",
    "                             index=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "\n",
    "        # Save protocol file\n",
    "        with open('mlaad_v5_protocol.txt', 'w') as f:\n",
    "            f.write('\\n'.join(protocol_lines))\n",
    "\n",
    "        print(f\"Created protocol.txt with {len(protocol_lines)} entries\")\n",
    "        print(f\"Created combined_meta.csv with {len(combined_meta)} entries\")\n",
    "    else:\n",
    "        print(\"No meta.csv files found or all files had errors\")\n",
    "\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    create_protocol_and_merge_meta(\n",
    "        '/nvme1/hungdx/Lightning-hydra/data/mlaad_v5/fake')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
