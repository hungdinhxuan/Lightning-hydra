# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: normal_largecorpus_multiview_cnsl
  - override /model: v2/xlsr_conformertcm_mdt_paper
  - override /callbacks: default_lora_loss_earlystop_logger
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["normal_largecorpus_multiview_cnsl", "xlsr_conformertcm_mdt_paper"]

seed: 1234

trainer:
  max_epochs: 100
  gradient_clip_val: 0.0
  accelerator: cuda

model:
  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 0.00005 # 5e-5
    weight_decay: 0.0001
  net: null
  scheduler: null
  cross_entropy_weight: [0.7, 0.3] # amplify bonafide loss
  is_base_model_path_ln: false
  
  adapter_type: null
  weighted_views:
    '1': 1
    '2': 1
    '3': 1
    '4': 1
  
data:
  batch_size: 16
  num_workers: 8
  pin_memory: true
  data_dir: ${oc.env:NOISE_DATASET_PATH}

  args:
    protocol_path: ${oc.env:NOISE_DATASET_PROTOCOL}
    augmentation_methods: ["RawBoost12", "pitch_1", "volume_10", "speed_01", "none"] # "none" is the original data
    padding_type: repeat
    random_start: False
    trim_length: 66800
    is_dev_aug: False
    view_padding_configs:
      '1':
        padding_type: repeat
        random_start: True
      '2':
        padding_type: repeat
        random_start: True
      '3':
        padding_type: repeat
        random_start: True
      '4':
        padding_type: repeat
        random_start: True

logger:
  wandb:
    tags: ${tags}
    group: "normal_largecorpus_multiview_cnsl"
  aim:
    experiment: "normal_largecorpus_multiview_cnsl"
