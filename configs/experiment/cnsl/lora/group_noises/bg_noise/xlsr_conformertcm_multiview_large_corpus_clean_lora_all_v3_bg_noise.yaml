# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: normal_largecorpus_multiview_cnsl
  - override /model: xlsr_conformertcm_baseline_multiview_lora
  - override /callbacks: default_lora_loss_earlystop
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["normal_largecorpus_multiview_cnsl", "xlsr_conformertcm_baseline_multiview_lora"]

seed: 1234

trainer:
  max_epochs: 100
  gradient_clip_val: 0.0
  accelerator: cuda

model:
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.000001
    weight_decay: 0.0001
  net: null
  scheduler:
    _target_: torch.optim.lr_scheduler.CyclicLR
    _partial_: true
    mode: triangular
    base_lr: 0.0000001 # min_lr 1e-7
    max_lr: 0.00001 # max_lr 1e-5
    step_size_up: 200

  cross_entropy_weight: [0.7, 0.3] # Balanced weights for cross entropy loss
  args:
    lora:
      r: 4
      target_modules: ["q_proj", "v_proj", "k_proj", "out_proj"]
      modules_to_save:  [] # conformer qkv
      lora_dropout: 0.0
      lora_alpha: 2
  
data:
  batch_size: 16
  num_workers: 8
  pin_memory: true
  data_dir: ${oc.env:NOISE_DATASET_PATH}

  args:
    protocol_path: ${oc.env:NOISE_DATASET_PROTOCOL}
    augmentation_methods: ["RawBoostdf", "background_noise_v1", "none"]
    padding_type: repeat
    random_start: True
    view_padding_configs:
      '1':
        padding_type: repeat
        random_start: True
      '2':
        padding_type: repeat
        random_start: True
      '3':
        padding_type: repeat
        random_start: True
      '4':
        padding_type: repeat
        random_start: True


logger:
  wandb:
    tags: ${tags}
    group: "normal_largecorpus_multiview_cnsl"
  aim:
    experiment: "normal_largecorpus_multiview_cnsl"
